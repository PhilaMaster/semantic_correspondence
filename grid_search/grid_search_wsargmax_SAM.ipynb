{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RZX8dGTK_xIO",
        "outputId": "c59f1aab-e61b-4753-cc88-f8b2c92487ea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Monta Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "wdRMIwKP_4VQ",
        "outputId": "80961ecc-6191-4f55-d297-2dc38ea35aa7"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/semantic_correspondence.zip'"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Percorsi\n",
        "zip_su_drive = '/content/drive/MyDrive/semantic_correspondence.zip'\n",
        "zip_locale = '/content/semantic_correspondence.zip'\n",
        "cartella_destinazione = '/content/'\n",
        "\n",
        "# Copia lo zip in locale\n",
        "import shutil\n",
        "shutil.copy(zip_su_drive, zip_locale)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "4spGp1KM_75c"
      },
      "outputs": [],
      "source": [
        "# Estrai lo zip\n",
        "import zipfile, os\n",
        "os.makedirs(cartella_destinazione, exist_ok=True)\n",
        "with zipfile.ZipFile(zip_locale, 'r') as z:\n",
        "    z.extractall(cartella_destinazione)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "emjurSt8AE5P",
        "outputId": "3602f334-7c3e-4554-c077-1a6718366938"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "✓ GPU: Tesla T4\n"
          ]
        }
      ],
      "source": [
        "# 5. Verify GPU\n",
        "import torch\n",
        "print(f\"\\n✓ GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'No GPU'}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "hcZlHAWdVsBY"
      },
      "outputs": [],
      "source": [
        "\n",
        "base = '/content/semantic_correspondence/SPair71k'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 772
        },
        "id": "KBO4_UOXHnXk",
        "outputId": "17c88022-8c69-43dc-8334-a350a9665abe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "GRID SEARCH FOR WINDOWED SOFTARGMAX HYPERPARAMETERS\n",
            "================================================================================\n",
            "Temperature values: [0.05, 0.1, 0.2, 0.5, 1.0, 2.0]\n",
            "Total combinations: 30\n",
            "Validation set size: 5384\n",
            "================================================================================\n",
            "\n",
            "[1/30] Testing K=3, temperature=0.05\n",
            "  Processed 1/5384 images\n",
            "  Processed 101/5384 images\n",
            "  Processed 1001/5384 images\n",
            "  Processed 2001/5384 images\n",
            "  Processed 3001/5384 images\n",
            "  Processed 4001/5384 images\n",
            "  Processed 5001/5384 images\n",
            "  PCK@0.05: mean=56.75%, median=60.00%\n",
            "  PCK@0.10: mean=68.98%, median=75.00%\n",
            "  PCK@0.20: mean=78.86%, median=87.50%\n",
            "  Time: 927.29s\n",
            "\n",
            "[2/30] Testing K=3, temperature=0.1\n",
            "  Processed 1/5384 images\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-224652174.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    252\u001b[0m     \u001b[0mval_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSPairDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpair_ann_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayout_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpck_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatatype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'val'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 254\u001b[0;31m     \u001b[0mdf_results\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_grid_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'grid_search_results/dinov3/dinov3_finetuned'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-224652174.py\u001b[0m in \u001b[0;36mrun_grid_search\u001b[0;34m(model, val_dataset, device, results_dir)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m             \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m             per_image_metrics = evaluate_with_params(\n\u001b[0m\u001b[1;32m    135\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthresholds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m             )\n",
            "\u001b[0;32m/tmp/ipython-input-224652174.py\u001b[0m in \u001b[0;36mevaluate_with_params\u001b[0;34m(model, dataset, device, K, temperature, img_size, patch_size, thresholds)\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0;31m# extract dense features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0msrc_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_dense_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m             \u001b[0mtgt_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_dense_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0;31m# reshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/semantic_correspondence/helper_functions.py\u001b[0m in \u001b[0;36mextract_dense_features\u001b[0;34m(model, img_tensor, training)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;31m#get tokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mfeatures_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0mpatch_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeatures_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'x_norm_patchtokens'\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# [B, N_patches, D]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/semantic_correspondence/models/dinov3/dinov3/models/vision_transformer.py\u001b[0m in \u001b[0;36mforward_features\u001b[0;34m(self, x, masks)\u001b[0m\n\u001b[1;32m    263\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasks\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_features_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mmasks\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_features_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/semantic_correspondence/models/dinov3/dinov3/models/vision_transformer.py\u001b[0m in \u001b[0;36mforward_features_list\u001b[0;34m(self, x_list, masks_list)\u001b[0m\n\u001b[1;32m    232\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m                 \u001b[0mrope_sincos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrope\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mblk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrope_sincos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m         \u001b[0mall_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/semantic_correspondence/models/dinov3/dinov3/layers/block.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x_or_x_list, rope_or_rope_list)\u001b[0m\n\u001b[1;32m    208\u001b[0m                 \u001b[0mrope_or_rope_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx_or_x_list\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m             \u001b[0;31m# return [self._forward(x, rope=rope) for x, rope in zip(x_or_x_list, rope_or_rope_list)]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_or_x_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrope_list\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrope_or_rope_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mAssertionError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/semantic_correspondence/models/dinov3/dinov3/layers/block.py\u001b[0m in \u001b[0;36m_forward_list\u001b[0;34m(self, x_list, rope_list)\u001b[0m\n\u001b[1;32m    191\u001b[0m             \u001b[0mx_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrope\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrope_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m                 \u001b[0mx_attn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mls1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrope\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m                 \u001b[0mx_ffn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_attn\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mls2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_attn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m                 \u001b[0mx_out\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_ffn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/semantic_correspondence/models/dinov3/dinov3/layers/attention.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, attn_bias, rope)\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_bias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrope\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0mqkv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqkv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         \u001b[0mattn_v\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_attention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqkv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mqkv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_bias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattn_bias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrope\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattn_v\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproj_drop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/semantic_correspondence/models/dinov3/dinov3/layers/attention.py\u001b[0m in \u001b[0;36mcompute_attention\u001b[0;34m(self, qkv, attn_bias, rope)\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrope\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             \u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_rope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscaled_dot_product_attention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/semantic_correspondence/models/dinov3/dinov3/layers/attention.py\u001b[0m in \u001b[0;36mapply_rope\u001b[0;34m(self, q, k, rope)\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0mq_prefix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mprefix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0mq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrope_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprefix\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcos\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# [B, head, hw, D//head]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m         \u001b[0mq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_prefix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# [B, head, N, D//head]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m         \u001b[0mk_prefix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mprefix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrope_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprefix\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcos\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# [B, head, hw, D//head]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import json\n",
        "import time\n",
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "# Add the extracted directory to the Python path\n",
        "sys.path.insert(0, '/content/semantic_correspondence')\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "research_path = \"/content/semantic_correspondence/models/segment_anything\"\n",
        "if research_path not in sys.path:\n",
        "    sys.path.insert(0, research_path)\n",
        "\n",
        "from SPair71k.devkit.SPairDataset import SPairDataset\n",
        "from helper_functions import extract_dense_features_SAM, pixel_to_patch_coord, patch_to_pixel_coord\n",
        "from matching_strategies import find_best_match_window_softargmax\n",
        "# from models.dinov2.dinov2.models.vision_transformer import vit_base\n",
        "# from models.dinov3.dinov3.models.vision_transformer import vit_base\n",
        "from models.segment_anything.segment_anything import SamPredictor, sam_model_registry\n",
        "from pck import compute_pck_spair71k\n",
        "\n",
        "\n",
        "def evaluate_with_params(model, dataset, device, K, temperature, img_size, patch_size, thresholds=[0.05, 0.1, 0.2]):\n",
        "    \"\"\"Evaluate model with specific K and temperature parameters.\"\"\"\n",
        "    per_image_metrics = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for idx, sample in enumerate(dataset):\n",
        "            src_tensor = sample['src_img'].unsqueeze(0).to(device)\n",
        "            tgt_tensor = sample['trg_img'].unsqueeze(0).to(device)\n",
        "\n",
        "            # resize to 518x518\n",
        "            src_tensor = F.interpolate(src_tensor, size=(img_size, img_size), mode='bilinear', align_corners=False)\n",
        "            tgt_tensor = F.interpolate(tgt_tensor, size=(img_size, img_size), mode='bilinear', align_corners=False)\n",
        "\n",
        "            # save original sizes\n",
        "            src_original_size = (sample['src_imsize'][2], sample['src_imsize'][1])\n",
        "            tgt_original_size = (sample['trg_imsize'][2], sample['trg_imsize'][1])\n",
        "\n",
        "            # extract dense features\n",
        "            src_features = extract_dense_features_SAM(model, src_tensor, image_size=img_size)\n",
        "            tgt_features = extract_dense_features_SAM(model, tgt_tensor, image_size=img_size)\n",
        "\n",
        "            # reshape\n",
        "            _, H, W, D = tgt_features.shape\n",
        "            tgt_flat = tgt_features.reshape(H * W, D)\n",
        "\n",
        "            # extract keypoints\n",
        "            src_kps = sample['src_kps'].numpy()\n",
        "            trg_kps = sample['trg_kps'].numpy()\n",
        "            trg_bbox = sample['trg_bbox']\n",
        "\n",
        "            pred_matches = []\n",
        "\n",
        "            # iterate over keypoints\n",
        "            for i in range(src_kps.shape[0]):\n",
        "                src_x, src_y = src_kps[i]\n",
        "                patch_x, patch_y = pixel_to_patch_coord(src_x, src_y, src_original_size, patch_size=patch_size, resized_size=img_size)\n",
        "\n",
        "                # extract source feature\n",
        "                src_feature = src_features[0, patch_y, patch_x, :]\n",
        "\n",
        "                # compute cosine similarities\n",
        "                similarities = F.cosine_similarity(\n",
        "                    src_feature.unsqueeze(0),\n",
        "                    tgt_flat,\n",
        "                    dim=1\n",
        "                )\n",
        "\n",
        "                # find best match with windowed softargmax\n",
        "                match_patch_x, match_patch_y = find_best_match_window_softargmax(\n",
        "                    similarities, W, H, K=K, temperature=temperature\n",
        "                )\n",
        "                match_x, match_y = patch_to_pixel_coord(\n",
        "                    match_patch_x, match_patch_y, tgt_original_size,\n",
        "                    patch_size=patch_size, resized_size=img_size\n",
        "                )\n",
        "\n",
        "                pred_matches.append([match_x, match_y])\n",
        "\n",
        "            # compute PCK for each threshold\n",
        "            image_pcks = {}\n",
        "            for threshold in thresholds:\n",
        "                pck, _, _ = compute_pck_spair71k(\n",
        "                    pred_matches,\n",
        "                    trg_kps.tolist(),\n",
        "                    trg_bbox,\n",
        "                    threshold\n",
        "                )\n",
        "                image_pcks[threshold] = pck\n",
        "\n",
        "            per_image_metrics.append({\n",
        "                'category': sample['category'],\n",
        "                'num_keypoints': src_kps.shape[0],\n",
        "                'pck_scores': image_pcks,\n",
        "            })\n",
        "            if idx==100 or idx%1000==0:\n",
        "                print(f\"  Processed {idx+1}/{len(dataset)} images\", flush=True)\n",
        "            # if idx==10:\n",
        "            #   break  # debug test on 50 images only\n",
        "\n",
        "    return per_image_metrics\n",
        "\n",
        "def run_grid_search(model, val_dataset, device, results_dir):\n",
        "    \"\"\"Run grid search over K and temperature parameters.\"\"\"\n",
        "\n",
        "    #hyperparameter ranges\n",
        "    # K_values = [3, 5, 7, 9]\n",
        "    K_values = [5]\n",
        "    # K = 5\n",
        "    temperature_values = [0.01, 0.05, 0.1, 0.2, 0.5, 1.0, 2.0]\n",
        "    thresholds = [0.05, 0.1, 0.2]\n",
        "\n",
        "    Path(results_dir).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    print(\"=\" * 80)\n",
        "    print(\"GRID SEARCH FOR WINDOWED SOFTARGMAX HYPERPARAMETERS\")\n",
        "    print(\"=\" * 80)\n",
        "    print(f\"K values: {K_values}\")\n",
        "    print(f\"Temperature values: {temperature_values}\")\n",
        "    print(f\"Total combinations: {len(K_values) * len(temperature_values)}\")\n",
        "    print(f\"Validation set size: {len(val_dataset)}\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    all_results = []\n",
        "    total_combinations = len(K_values) * len(temperature_values)\n",
        "    current_combo = 0\n",
        "\n",
        "    for K in K_values:\n",
        "        for temp in temperature_values:\n",
        "            current_combo += 1\n",
        "            print(f\"\\n[{current_combo}/{total_combinations}] Testing K={K}, temperature={temp}\")\n",
        "\n",
        "            start_time = time.time()\n",
        "            per_image_metrics = evaluate_with_params(\n",
        "                model, val_dataset, device, K, temp, img_size, patch_size, thresholds\n",
        "            )\n",
        "            inference_time = time.time() - start_time\n",
        "\n",
        "            result = {\n",
        "                'K': K,\n",
        "                'temperature': temp,\n",
        "                'inference_time_sec': inference_time,\n",
        "            }\n",
        "\n",
        "            for threshold in thresholds:\n",
        "                all_pcks = np.array([img['pck_scores'][threshold] for img in per_image_metrics])\n",
        "\n",
        "                result[f'pck@{threshold:.2f}_mean'] = float(np.mean(all_pcks))\n",
        "                result[f'pck@{threshold:.2f}_std'] = float(np.std(all_pcks))\n",
        "                result[f'pck@{threshold:.2f}_median'] = float(np.median(all_pcks))\n",
        "                result[f'pck@{threshold:.2f}_p25'] = float(np.percentile(all_pcks, 25))\n",
        "                result[f'pck@{threshold:.2f}_p75'] = float(np.percentile(all_pcks, 75))\n",
        "\n",
        "                print(f\"  PCK@{threshold:.2f}: mean={result[f'pck@{threshold:.2f}_mean']:.2f}%, \"\n",
        "                      f\"median={result[f'pck@{threshold:.2f}_median']:.2f}%\")\n",
        "\n",
        "            all_results.append(result)\n",
        "            print(f\"  Time: {inference_time:.2f}s\")\n",
        "\n",
        "    #save all results_SPair71K to CSV\n",
        "    df_results = pd.DataFrame(all_results)\n",
        "    csv_path = f'{results_dir}/grid_search_results.csv'\n",
        "    df_results.to_csv(csv_path, index=False)\n",
        "    print(f\"\\n{'=' * 80}\")\n",
        "    print(f\"Saved grid search results to '{csv_path}'\")\n",
        "\n",
        "    #find best parameters for each threshold\n",
        "    print(f\"\\n{'=' * 80}\")\n",
        "    print(\"BEST PARAMETERS FOR EACH THRESHOLD\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    best_params_summary = []\n",
        "    for threshold in thresholds:\n",
        "        metric_col = f'pck@{threshold:.2f}_mean'\n",
        "        best_idx = df_results[metric_col].idxmax()\n",
        "        best_row = df_results.loc[best_idx]\n",
        "\n",
        "        best_params = {\n",
        "            'threshold': threshold,\n",
        "            'best_K': int(best_row['K']),\n",
        "            'best_temperature': float(best_row['temperature']),\n",
        "            'best_pck_mean': float(best_row[metric_col]),\n",
        "            'best_pck_median': float(best_row[f'pck@{threshold:.2f}_median']),\n",
        "            'best_pck_std': float(best_row[f'pck@{threshold:.2f}_std']),\n",
        "        }\n",
        "        best_params_summary.append(best_params)\n",
        "\n",
        "        print(f\"\\nPCK@{threshold:.2f}:\")\n",
        "        print(f\"  Best K: {best_params['best_K']}\")\n",
        "        print(f\"  Best temperature: {best_params['best_temperature']}\")\n",
        "        print(f\"  Mean PCK: {best_params['best_pck_mean']:.2f}%\")\n",
        "        print(f\"  Median PCK: {best_params['best_pck_median']:.2f}%\")\n",
        "        print(f\"  Std PCK: {best_params['best_pck_std']:.2f}%\")\n",
        "\n",
        "\n",
        "    df_best = pd.DataFrame(best_params_summary)\n",
        "    best_csv_path = f'{results_dir}/best_parameters.csv'\n",
        "    df_best.to_csv(best_csv_path, index=False)\n",
        "    print(f\"\\nSaved best parameters to '{best_csv_path}'\")\n",
        "\n",
        "\n",
        "    best_json_path = f'{results_dir}/best_parameters.json'\n",
        "    with open(best_json_path, 'w') as f:\n",
        "        json.dump(best_params_summary, f, indent=2)\n",
        "    print(f\"Saved best parameters to '{best_json_path}'\")\n",
        "\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    drive_results_base_path = '/content/drive/MyDrive/Colab_SAM_finetuned_grid_search_results/'\n",
        "    drive_destination_path = os.path.join(drive_results_base_path, os.path.basename(results_dir))\n",
        "\n",
        "    try:\n",
        "        if not os.path.exists(drive_results_base_path):\n",
        "            os.makedirs(drive_results_base_path, exist_ok=True)\n",
        "        shutil.copytree(results_dir, drive_destination_path)\n",
        "        print(f\"\\n✓ Successfully copied results to Google Drive: {drive_destination_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"\\n✗ Error copying results to Google Drive: {e}\")\n",
        "\n",
        "    return df_results, best_params_summary\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    img_size = 512\n",
        "    patch_size = 16\n",
        "    print(\"\\nLoading SAM model...\")\n",
        "    model_type = \"vit_b\"\n",
        "    checkpoint_path = \"/content/semantic_correspondence/models/segment_anything/weights/finetuned/SAM_finetuned_4bl_15t_0.0001lr.pth\"\n",
        "    # Initialize the SAM model without loading checkpoint yet\n",
        "    sam_model = sam_model_registry[model_type](checkpoint=None) # Pass None to initialize without loading\n",
        "    sam_model.to(device)\n",
        "\n",
        "    # Load the custom finetuned checkpoint\n",
        "    print(f\"Loading finetuned SAM checkpoint from {checkpoint_path}\")\n",
        "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
        "\n",
        "    # The finetuned checkpoint likely contains more than just the model state_dict.\n",
        "    # Extract the actual model_state_dict and load it\n",
        "    if 'model_state_dict' in checkpoint:\n",
        "        sam_model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        print(\"Successfully loaded 'model_state_dict' from checkpoint.\")\n",
        "    else:\n",
        "        # If the checkpoint itself is just the state_dict, try loading it directly\n",
        "        sam_model.load_state_dict(checkpoint)\n",
        "        print(\"Successfully loaded checkpoint directly as state_dict.\")\n",
        "\n",
        "\n",
        "    base = '/content/semantic_correspondence/SPair71k'\n",
        "    pair_ann_path = f'{base}/PairAnnotation'\n",
        "    layout_path = f'{base}/Layout'\n",
        "    image_path = f'{base}/JPEGImages'\n",
        "    dataset_size = 'large'\n",
        "    pck_alpha = 0.1  # mock, it's not used in evaluation\n",
        "    val_dataset = SPairDataset(pair_ann_path, layout_path, image_path, dataset_size, pck_alpha, datatype='val')\n",
        "\n",
        "    df_results, best_params = run_grid_search(sam_model, val_dataset, device, 'grid_search_results/SAM/SAM_finetuned')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Smonta il Drive\n",
        "drive.flush_and_unmount()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
