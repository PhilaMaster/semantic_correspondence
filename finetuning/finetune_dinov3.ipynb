{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 23971,
     "status": "ok",
     "timestamp": 1767884501857,
     "user": {
      "displayName": "Simone Iachino",
      "userId": "17661475359145447135"
     },
     "user_tz": -60
    },
    "id": "RZX8dGTK_xIO",
    "outputId": "3441e55a-18aa-4bef-a3a8-c7582108f59d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "# Monta Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "executionInfo": {
     "elapsed": 60676,
     "status": "ok",
     "timestamp": 1767884565363,
     "user": {
      "displayName": "Simone Iachino",
      "userId": "17661475359145447135"
     },
     "user_tz": -60
    },
    "id": "wdRMIwKP_4VQ",
    "outputId": "c3fcc599-5e85-4a60-8d0a-f9dcfaccf920"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'/content/semantic_correspondence.zip'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Percorsi\n",
    "zip_su_drive = '/content/drive/MyDrive/semantic_correspondence.zip'\n",
    "zip_locale = '/content/semantic_correspondence.zip'\n",
    "cartella_destinazione = '/content/'\n",
    "\n",
    "# Copia lo zip in locale\n",
    "import shutil\n",
    "shutil.copy(zip_su_drive, zip_locale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 41120,
     "status": "ok",
     "timestamp": 1767884606486,
     "user": {
      "displayName": "Simone Iachino",
      "userId": "17661475359145447135"
     },
     "user_tz": -60
    },
    "id": "4spGp1KM_75c"
   },
   "outputs": [],
   "source": [
    "# Estrai lo zip\n",
    "import zipfile, os\n",
    "os.makedirs(cartella_destinazione, exist_ok=True)\n",
    "with zipfile.ZipFile(zip_locale, 'r') as z:\n",
    "    z.extractall(cartella_destinazione)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4175,
     "status": "ok",
     "timestamp": 1767884610658,
     "user": {
      "displayName": "Simone Iachino",
      "userId": "17661475359145447135"
     },
     "user_tz": -60
    },
    "id": "emjurSt8AE5P",
    "outputId": "d2171329-3ef4-4e95-ad63-f0f12582108d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ GPU: Tesla T4\n"
     ]
    }
   ],
   "source": [
    "# 5. Verify GPU\n",
    "import torch\n",
    "print(f\"\\n✓ GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'No GPU'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MdRqeiOWKm_0",
    "outputId": "5aa7f992-8615-497a-c23a-0f370d4dc088"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Dec 26 14:10:57 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
      "| N/A   48C    P8              9W /   70W |       2MiB /  15360MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1767884614447,
     "user": {
      "displayName": "Simone Iachino",
      "userId": "17661475359145447135"
     },
     "user_tz": -60
    },
    "id": "awU1xZU_BkTO"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('/content/semantic_correspondence')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1767884621037,
     "user": {
      "displayName": "Simone Iachino",
      "userId": "17661475359145447135"
     },
     "user_tz": -60
    },
    "id": "jTLJNjrBCfaM"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "def extract_dense_features(model, img_tensor, training=False):\n",
    "    \"\"\"Extract dense features from DINOv2 model given an input image tensor.\"\"\"\n",
    "    context = torch.no_grad() if not training else torch.enable_grad()\n",
    "\n",
    "    with context:\n",
    "        #get tokens\n",
    "        features_dict = model.forward_features(img_tensor)\n",
    "        patch_tokens = features_dict['x_norm_patchtokens']  # [B, N_patches, D]\n",
    "\n",
    "        #reshaping to dense feature map\n",
    "        B, N, D = patch_tokens.shape\n",
    "        H_patches = W_patches = int(N ** 0.5)  # per img 518x518 con patch 14: 37x37\n",
    "        dense_features = patch_tokens.reshape(B, H_patches, W_patches, D)\n",
    "    return dense_features\n",
    "\n",
    "\n",
    "def pixel_to_patch_coord(x, y, original_size, patch_size=14, resized_size=518):\n",
    "    \"\"\"convert pixel coordinates to patch coordinates\"\"\"\n",
    "    #scale to resized image\n",
    "    scale_x = resized_size / original_size[0]\n",
    "    scale_y = resized_size / original_size[1]\n",
    "    x_resized = x * scale_x\n",
    "    y_resized = y * scale_y\n",
    "\n",
    "    #compute patch coordinates\n",
    "    patch_x = int(x_resized // patch_size)\n",
    "    patch_y = int(y_resized // patch_size)\n",
    "\n",
    "    #clamp to valid range\n",
    "    max_patch = resized_size // patch_size - 1\n",
    "    patch_x = min(max(patch_x, 0), max_patch)\n",
    "    patch_y = min(max(patch_y, 0), max_patch)\n",
    "\n",
    "    return patch_x, patch_y\n",
    "\n",
    "\n",
    "def patch_to_pixel_coord(patch_x, patch_y, original_size, patch_size=14, resized_size=518):\n",
    "    \"\"\"Convert patch coordinates back to pixel coordinates with a centering strategy\"\"\"\n",
    "    #center of the patch in resized image\n",
    "    x_resized = patch_x * patch_size + patch_size / 2\n",
    "    y_resized = patch_y * patch_size + patch_size / 2\n",
    "\n",
    "    #scale back to original image size\n",
    "    scale_x = original_size[0] / resized_size\n",
    "    scale_y = original_size[1] / resized_size\n",
    "    x = x_resized * scale_x\n",
    "    y = y_resized * scale_y\n",
    "\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1767884815126,
     "user": {
      "displayName": "Simone Iachino",
      "userId": "17661475359145447135"
     },
     "user_tz": -60
    },
    "id": "hcZlHAWdVsBY"
   },
   "outputs": [],
   "source": [
    "\n",
    "base = '/content/semantic_correspondence/SPair71k'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12837405,
     "status": "ok",
     "timestamp": 1767898006090,
     "user": {
      "displayName": "Simone Iachino",
      "userId": "17661475359145447135"
     },
     "user_tz": -60
    },
    "id": "a8ruH4alGfIK",
    "outputId": "bed5126f-8e0b-4b10-a789-216ebe1b6d3e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Results will be saved to: results_colab/dinov3/t_5_blocks_2_20260108_151250\n",
      "\n",
      "Loading SPair-71k dataset...\n",
      "Training samples: 53340\n",
      "Val samples: 5384\n",
      "\n",
      "================================================================================\n",
      "FINETUNING WITH LAST 2 BLOCKS UNFROZEN\n",
      "================================================================================\n",
      "\n",
      "Loading DINOv3-base model...\n",
      "Unfrozen last 2 blocks + norm layer\n",
      "\n",
      "Trainable parameters: 14,180,352 / 85,669,632 (16.55%)\n",
      "\n",
      "============================================================\n",
      "STARTING TRAINING\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "Epoch 1/1\n",
      "============================================================\n",
      "Epoch 1, Batch 50/53340, Loss: 5.1446\n",
      "Epoch 1, Batch 100/53340, Loss: 3.3736\n",
      "Epoch 1, Batch 150/53340, Loss: 2.5399\n",
      "Epoch 1, Batch 200/53340, Loss: 3.8377\n",
      "Epoch 1, Batch 250/53340, Loss: 3.8487\n",
      "Epoch 1, Batch 300/53340, Loss: 3.8877\n",
      "Epoch 1, Batch 350/53340, Loss: 4.4386\n",
      "Epoch 1, Batch 400/53340, Loss: 3.5273\n",
      "Epoch 1, Batch 450/53340, Loss: 4.4266\n",
      "Epoch 1, Batch 500/53340, Loss: 3.0428\n",
      "Epoch 1, Batch 550/53340, Loss: 2.2854\n",
      "Epoch 1, Batch 600/53340, Loss: 2.7120\n",
      "Epoch 1, Batch 650/53340, Loss: 3.9658\n",
      "Epoch 1, Batch 700/53340, Loss: 2.5645\n",
      "Epoch 1, Batch 750/53340, Loss: 3.7300\n",
      "Epoch 1, Batch 800/53340, Loss: 3.6431\n",
      "Epoch 1, Batch 850/53340, Loss: 3.1198\n",
      "Epoch 1, Batch 900/53340, Loss: 4.2244\n",
      "Epoch 1, Batch 950/53340, Loss: 4.4221\n",
      "Epoch 1, Batch 1000/53340, Loss: 4.7264\n",
      "Epoch 1, Batch 1050/53340, Loss: 2.5596\n",
      "Epoch 1, Batch 1100/53340, Loss: 3.2132\n",
      "Epoch 1, Batch 1150/53340, Loss: 3.4754\n",
      "Epoch 1, Batch 1200/53340, Loss: 2.7759\n",
      "Epoch 1, Batch 1250/53340, Loss: 3.4167\n",
      "Epoch 1, Batch 1300/53340, Loss: 1.3349\n",
      "Epoch 1, Batch 1350/53340, Loss: 4.8449\n",
      "Epoch 1, Batch 1400/53340, Loss: 2.3662\n",
      "Epoch 1, Batch 1450/53340, Loss: 2.5264\n",
      "Epoch 1, Batch 1500/53340, Loss: 2.4075\n",
      "Epoch 1, Batch 1550/53340, Loss: 2.7848\n",
      "Epoch 1, Batch 1600/53340, Loss: 2.4561\n",
      "Epoch 1, Batch 1650/53340, Loss: 2.9643\n",
      "Epoch 1, Batch 1700/53340, Loss: 2.9241\n",
      "Epoch 1, Batch 1750/53340, Loss: 2.9879\n",
      "Epoch 1, Batch 1800/53340, Loss: 3.0360\n",
      "Epoch 1, Batch 1850/53340, Loss: 3.5245\n",
      "Epoch 1, Batch 1900/53340, Loss: 3.4535\n",
      "Epoch 1, Batch 1950/53340, Loss: 2.3112\n",
      "Epoch 1, Batch 2000/53340, Loss: 2.5474\n",
      "Epoch 1, Batch 2050/53340, Loss: 3.6142\n",
      "Epoch 1, Batch 2100/53340, Loss: 1.5771\n",
      "Epoch 1, Batch 2150/53340, Loss: 2.1274\n",
      "Epoch 1, Batch 2200/53340, Loss: 2.2389\n",
      "Epoch 1, Batch 2250/53340, Loss: 3.6251\n",
      "Epoch 1, Batch 2300/53340, Loss: 3.1104\n",
      "Epoch 1, Batch 2350/53340, Loss: 1.3313\n",
      "Epoch 1, Batch 2400/53340, Loss: 2.7256\n",
      "Epoch 1, Batch 2450/53340, Loss: 2.7879\n",
      "Epoch 1, Batch 2500/53340, Loss: 1.7462\n",
      "Epoch 1, Batch 2550/53340, Loss: 3.7461\n",
      "Epoch 1, Batch 2600/53340, Loss: 2.1264\n",
      "Epoch 1, Batch 2650/53340, Loss: 2.9052\n",
      "Epoch 1, Batch 2700/53340, Loss: 2.9310\n",
      "Epoch 1, Batch 2750/53340, Loss: 2.6657\n",
      "Epoch 1, Batch 2800/53340, Loss: 2.4248\n",
      "Epoch 1, Batch 2850/53340, Loss: 2.4399\n",
      "Epoch 1, Batch 2900/53340, Loss: 3.5158\n",
      "Epoch 1, Batch 2950/53340, Loss: 2.9007\n",
      "Epoch 1, Batch 3000/53340, Loss: 2.3893\n",
      "Epoch 1, Batch 3050/53340, Loss: 2.9174\n",
      "Epoch 1, Batch 3100/53340, Loss: 3.3515\n",
      "Epoch 1, Batch 3150/53340, Loss: 1.7742\n",
      "Epoch 1, Batch 3200/53340, Loss: 1.4210\n",
      "Epoch 1, Batch 3250/53340, Loss: 3.5918\n",
      "Epoch 1, Batch 3300/53340, Loss: 1.4191\n",
      "Epoch 1, Batch 3350/53340, Loss: 3.2946\n",
      "Epoch 1, Batch 3400/53340, Loss: 2.3511\n",
      "Epoch 1, Batch 3450/53340, Loss: 3.0281\n",
      "Epoch 1, Batch 3500/53340, Loss: 3.4340\n",
      "Epoch 1, Batch 3550/53340, Loss: 2.8684\n",
      "Epoch 1, Batch 3600/53340, Loss: 2.6317\n",
      "Epoch 1, Batch 3650/53340, Loss: 2.2106\n",
      "Epoch 1, Batch 3700/53340, Loss: 2.8747\n",
      "Epoch 1, Batch 3750/53340, Loss: 2.1198\n",
      "Epoch 1, Batch 3800/53340, Loss: 2.7779\n",
      "Epoch 1, Batch 3850/53340, Loss: 2.5306\n",
      "Epoch 1, Batch 3900/53340, Loss: 2.1052\n",
      "Epoch 1, Batch 3950/53340, Loss: 2.5332\n",
      "Epoch 1, Batch 4000/53340, Loss: 2.5519\n",
      "Epoch 1, Batch 4050/53340, Loss: 2.1310\n",
      "Epoch 1, Batch 4100/53340, Loss: 2.5744\n",
      "Epoch 1, Batch 4150/53340, Loss: 2.2808\n",
      "Epoch 1, Batch 4200/53340, Loss: 3.6645\n",
      "Epoch 1, Batch 4250/53340, Loss: 1.4101\n",
      "Epoch 1, Batch 4300/53340, Loss: 1.1984\n",
      "Epoch 1, Batch 4350/53340, Loss: 1.6931\n",
      "Epoch 1, Batch 4400/53340, Loss: 2.4797\n",
      "Epoch 1, Batch 4450/53340, Loss: 1.9642\n",
      "Epoch 1, Batch 4500/53340, Loss: 1.6386\n",
      "Epoch 1, Batch 4550/53340, Loss: 2.4400\n",
      "Epoch 1, Batch 4600/53340, Loss: 3.0855\n",
      "Epoch 1, Batch 4650/53340, Loss: 1.4552\n",
      "Epoch 1, Batch 4700/53340, Loss: 2.4892\n",
      "Epoch 1, Batch 4750/53340, Loss: 2.1958\n",
      "Epoch 1, Batch 4800/53340, Loss: 1.8146\n",
      "Epoch 1, Batch 4850/53340, Loss: 2.8418\n",
      "Epoch 1, Batch 4900/53340, Loss: 2.3927\n",
      "Epoch 1, Batch 4950/53340, Loss: 2.4673\n",
      "Epoch 1, Batch 5000/53340, Loss: 1.7272\n",
      "Epoch 1, Batch 5050/53340, Loss: 1.9536\n",
      "Epoch 1, Batch 5100/53340, Loss: 1.1231\n",
      "Epoch 1, Batch 5150/53340, Loss: 2.9519\n",
      "Epoch 1, Batch 5200/53340, Loss: 2.6079\n",
      "Epoch 1, Batch 5250/53340, Loss: 2.4751\n",
      "Epoch 1, Batch 5300/53340, Loss: 2.0779\n",
      "Epoch 1, Batch 5350/53340, Loss: 3.4824\n",
      "Epoch 1, Batch 5400/53340, Loss: 1.9811\n",
      "Epoch 1, Batch 5450/53340, Loss: 1.4638\n",
      "Epoch 1, Batch 5500/53340, Loss: 2.1132\n",
      "Epoch 1, Batch 5550/53340, Loss: 1.9816\n",
      "Epoch 1, Batch 5600/53340, Loss: 2.2584\n",
      "Epoch 1, Batch 5650/53340, Loss: 1.6778\n",
      "Epoch 1, Batch 5700/53340, Loss: 1.9380\n",
      "Epoch 1, Batch 5750/53340, Loss: 0.6018\n",
      "Epoch 1, Batch 5800/53340, Loss: 1.5882\n",
      "Epoch 1, Batch 5850/53340, Loss: 1.8739\n",
      "Epoch 1, Batch 5900/53340, Loss: 2.0238\n",
      "Epoch 1, Batch 5950/53340, Loss: 1.9789\n",
      "Epoch 1, Batch 6000/53340, Loss: 2.9809\n",
      "Epoch 1, Batch 6050/53340, Loss: 1.7343\n",
      "Epoch 1, Batch 6100/53340, Loss: 1.6526\n",
      "Epoch 1, Batch 6150/53340, Loss: 2.1383\n",
      "Epoch 1, Batch 6200/53340, Loss: 2.0087\n",
      "Epoch 1, Batch 6250/53340, Loss: 1.6454\n",
      "Epoch 1, Batch 6300/53340, Loss: 1.5387\n",
      "Epoch 1, Batch 6350/53340, Loss: 2.7184\n",
      "Epoch 1, Batch 6400/53340, Loss: 1.9275\n",
      "Epoch 1, Batch 6450/53340, Loss: 1.9795\n",
      "Epoch 1, Batch 6500/53340, Loss: 1.0946\n",
      "Epoch 1, Batch 6550/53340, Loss: 1.9360\n",
      "Epoch 1, Batch 6600/53340, Loss: 1.5449\n",
      "Epoch 1, Batch 6650/53340, Loss: 2.6732\n",
      "Epoch 1, Batch 6700/53340, Loss: 1.9063\n",
      "Epoch 1, Batch 6750/53340, Loss: 2.1561\n",
      "Epoch 1, Batch 6800/53340, Loss: 1.6746\n",
      "Epoch 1, Batch 6850/53340, Loss: 2.1822\n",
      "Epoch 1, Batch 6900/53340, Loss: 2.2002\n",
      "Epoch 1, Batch 6950/53340, Loss: 1.5201\n",
      "Epoch 1, Batch 7000/53340, Loss: 1.6792\n",
      "Epoch 1, Batch 7050/53340, Loss: 3.2757\n",
      "Epoch 1, Batch 7100/53340, Loss: 2.0034\n",
      "Epoch 1, Batch 7150/53340, Loss: 2.7275\n",
      "Epoch 1, Batch 7200/53340, Loss: 1.4037\n",
      "Epoch 1, Batch 7250/53340, Loss: 3.7988\n",
      "Epoch 1, Batch 7300/53340, Loss: 2.7664\n",
      "Epoch 1, Batch 7350/53340, Loss: 3.0612\n",
      "Epoch 1, Batch 7400/53340, Loss: 2.2119\n",
      "Epoch 1, Batch 7450/53340, Loss: 1.4945\n",
      "Epoch 1, Batch 7500/53340, Loss: 1.8783\n",
      "Epoch 1, Batch 7550/53340, Loss: 1.8860\n",
      "Epoch 1, Batch 7600/53340, Loss: 2.1289\n",
      "Epoch 1, Batch 7650/53340, Loss: 1.6719\n",
      "Epoch 1, Batch 7700/53340, Loss: 2.3239\n",
      "Epoch 1, Batch 7750/53340, Loss: 1.8832\n",
      "Epoch 1, Batch 7800/53340, Loss: 1.5721\n",
      "Epoch 1, Batch 7850/53340, Loss: 1.9227\n",
      "Epoch 1, Batch 7900/53340, Loss: 2.3282\n",
      "Epoch 1, Batch 7950/53340, Loss: 1.8177\n",
      "Epoch 1, Batch 8000/53340, Loss: 1.3053\n",
      "Epoch 1, Batch 8050/53340, Loss: 1.4890\n",
      "Epoch 1, Batch 8100/53340, Loss: 1.8312\n",
      "Epoch 1, Batch 8150/53340, Loss: 2.4896\n",
      "Epoch 1, Batch 8200/53340, Loss: 1.9968\n",
      "Epoch 1, Batch 8250/53340, Loss: 3.1741\n",
      "Epoch 1, Batch 8300/53340, Loss: 1.5159\n",
      "Epoch 1, Batch 8350/53340, Loss: 2.1689\n",
      "Epoch 1, Batch 8400/53340, Loss: 1.4876\n",
      "Epoch 1, Batch 8450/53340, Loss: 1.6942\n",
      "Epoch 1, Batch 8500/53340, Loss: 1.2065\n",
      "Epoch 1, Batch 8550/53340, Loss: 1.5683\n",
      "Epoch 1, Batch 8600/53340, Loss: 1.2364\n",
      "Epoch 1, Batch 8650/53340, Loss: 0.9875\n",
      "Epoch 1, Batch 8700/53340, Loss: 1.7423\n",
      "Epoch 1, Batch 8750/53340, Loss: 1.5069\n",
      "Epoch 1, Batch 8800/53340, Loss: 2.1009\n",
      "Epoch 1, Batch 8850/53340, Loss: 1.8579\n",
      "Epoch 1, Batch 8900/53340, Loss: 1.5936\n",
      "Epoch 1, Batch 8950/53340, Loss: 2.9118\n",
      "Epoch 1, Batch 9000/53340, Loss: 2.8679\n",
      "Epoch 1, Batch 9050/53340, Loss: 1.1906\n",
      "Epoch 1, Batch 9100/53340, Loss: 1.5835\n",
      "Epoch 1, Batch 9150/53340, Loss: 1.5143\n",
      "Epoch 1, Batch 9200/53340, Loss: 1.2812\n",
      "Epoch 1, Batch 9250/53340, Loss: 1.1277\n",
      "Epoch 1, Batch 9300/53340, Loss: 1.7605\n",
      "Epoch 1, Batch 9350/53340, Loss: 1.4959\n",
      "Epoch 1, Batch 9400/53340, Loss: 1.8813\n",
      "Epoch 1, Batch 9450/53340, Loss: 1.8405\n",
      "Epoch 1, Batch 9500/53340, Loss: 1.4357\n",
      "Epoch 1, Batch 9550/53340, Loss: 1.7845\n",
      "Epoch 1, Batch 9600/53340, Loss: 1.6804\n",
      "Epoch 1, Batch 9650/53340, Loss: 1.4159\n",
      "Epoch 1, Batch 9700/53340, Loss: 1.3505\n",
      "Epoch 1, Batch 9750/53340, Loss: 1.6559\n",
      "Epoch 1, Batch 9800/53340, Loss: 1.6832\n",
      "Epoch 1, Batch 9850/53340, Loss: 1.6048\n",
      "Epoch 1, Batch 9900/53340, Loss: 1.4083\n",
      "Epoch 1, Batch 9950/53340, Loss: 1.7004\n",
      "Epoch 1, Batch 10000/53340, Loss: 1.2590\n",
      "Epoch 1, Batch 10050/53340, Loss: 1.5134\n",
      "Epoch 1, Batch 10100/53340, Loss: 2.1844\n",
      "Epoch 1, Batch 10150/53340, Loss: 1.5442\n",
      "Epoch 1, Batch 10200/53340, Loss: 1.0482\n",
      "Epoch 1, Batch 10250/53340, Loss: 1.4038\n",
      "Epoch 1, Batch 10300/53340, Loss: 1.8619\n",
      "Epoch 1, Batch 10350/53340, Loss: 2.0152\n",
      "Epoch 1, Batch 10400/53340, Loss: 1.1582\n",
      "Epoch 1, Batch 10450/53340, Loss: 2.2469\n",
      "Epoch 1, Batch 10500/53340, Loss: 0.8838\n",
      "Epoch 1, Batch 10550/53340, Loss: 2.0649\n",
      "Epoch 1, Batch 10600/53340, Loss: 1.1495\n",
      "Epoch 1, Batch 10650/53340, Loss: 0.8190\n",
      "Epoch 1, Batch 10700/53340, Loss: 1.4828\n",
      "Epoch 1, Batch 10750/53340, Loss: 0.8618\n",
      "Epoch 1, Batch 10800/53340, Loss: 1.1715\n",
      "Epoch 1, Batch 10850/53340, Loss: 0.8705\n",
      "Epoch 1, Batch 10900/53340, Loss: 1.3349\n",
      "Epoch 1, Batch 10950/53340, Loss: 1.1718\n",
      "Epoch 1, Batch 11000/53340, Loss: 1.6027\n",
      "Epoch 1, Batch 11050/53340, Loss: 1.2910\n",
      "Epoch 1, Batch 11100/53340, Loss: 1.5969\n",
      "Epoch 1, Batch 11150/53340, Loss: 1.5050\n",
      "Epoch 1, Batch 11200/53340, Loss: 1.5184\n",
      "Epoch 1, Batch 11250/53340, Loss: 2.0039\n",
      "Epoch 1, Batch 11300/53340, Loss: 1.9617\n",
      "Epoch 1, Batch 11350/53340, Loss: 2.1343\n",
      "Epoch 1, Batch 11400/53340, Loss: 1.5261\n",
      "Epoch 1, Batch 11450/53340, Loss: 1.5760\n",
      "Epoch 1, Batch 11500/53340, Loss: 0.9239\n",
      "Epoch 1, Batch 11550/53340, Loss: 1.4788\n",
      "Epoch 1, Batch 11600/53340, Loss: 5.2857\n",
      "Epoch 1, Batch 11650/53340, Loss: 2.1585\n",
      "Epoch 1, Batch 11700/53340, Loss: 1.7338\n",
      "Epoch 1, Batch 11750/53340, Loss: 1.4382\n",
      "Epoch 1, Batch 11800/53340, Loss: 1.3289\n",
      "Epoch 1, Batch 11850/53340, Loss: 1.1990\n",
      "Epoch 1, Batch 11900/53340, Loss: 2.1216\n",
      "Epoch 1, Batch 11950/53340, Loss: 1.0098\n",
      "Epoch 1, Batch 12000/53340, Loss: 1.8303\n",
      "Epoch 1, Batch 12050/53340, Loss: 1.5578\n",
      "Epoch 1, Batch 12100/53340, Loss: 0.8236\n",
      "Epoch 1, Batch 12150/53340, Loss: 1.7634\n",
      "Epoch 1, Batch 12200/53340, Loss: 1.0942\n",
      "Epoch 1, Batch 12250/53340, Loss: 0.8877\n",
      "Epoch 1, Batch 12300/53340, Loss: 1.2947\n",
      "Epoch 1, Batch 12350/53340, Loss: 1.5081\n",
      "Epoch 1, Batch 12400/53340, Loss: 1.2307\n",
      "Epoch 1, Batch 12450/53340, Loss: 1.5327\n",
      "Epoch 1, Batch 12500/53340, Loss: 1.7474\n",
      "Epoch 1, Batch 12550/53340, Loss: 1.4718\n",
      "Epoch 1, Batch 12600/53340, Loss: 3.4855\n",
      "Epoch 1, Batch 12650/53340, Loss: 1.7171\n",
      "Epoch 1, Batch 12700/53340, Loss: 1.7186\n",
      "Epoch 1, Batch 12750/53340, Loss: 1.6898\n",
      "Epoch 1, Batch 12800/53340, Loss: 2.3446\n",
      "Epoch 1, Batch 12850/53340, Loss: 1.1378\n",
      "Epoch 1, Batch 12900/53340, Loss: 1.2662\n",
      "Epoch 1, Batch 12950/53340, Loss: 1.1110\n",
      "Epoch 1, Batch 13000/53340, Loss: 1.5538\n",
      "Epoch 1, Batch 13050/53340, Loss: 0.8579\n",
      "Epoch 1, Batch 13100/53340, Loss: 1.4434\n",
      "Epoch 1, Batch 13150/53340, Loss: 1.7048\n",
      "Epoch 1, Batch 13200/53340, Loss: 1.9763\n",
      "Epoch 1, Batch 13250/53340, Loss: 1.4810\n",
      "Epoch 1, Batch 13300/53340, Loss: 1.6383\n",
      "Epoch 1, Batch 13350/53340, Loss: 1.1887\n",
      "Epoch 1, Batch 13400/53340, Loss: 2.1560\n",
      "Epoch 1, Batch 13450/53340, Loss: 1.9563\n",
      "Epoch 1, Batch 13500/53340, Loss: 1.0589\n",
      "Epoch 1, Batch 13550/53340, Loss: 2.0169\n",
      "Epoch 1, Batch 13600/53340, Loss: 1.4033\n",
      "Epoch 1, Batch 13650/53340, Loss: 1.2219\n",
      "Epoch 1, Batch 13700/53340, Loss: 1.4546\n",
      "Epoch 1, Batch 13750/53340, Loss: 1.7097\n",
      "Epoch 1, Batch 13800/53340, Loss: 1.2013\n",
      "Epoch 1, Batch 13850/53340, Loss: 1.7775\n",
      "Epoch 1, Batch 13900/53340, Loss: 1.2497\n",
      "Epoch 1, Batch 13950/53340, Loss: 0.8827\n",
      "Epoch 1, Batch 14000/53340, Loss: 1.2706\n",
      "Epoch 1, Batch 14050/53340, Loss: 1.5163\n",
      "Epoch 1, Batch 14100/53340, Loss: 1.2199\n",
      "Epoch 1, Batch 14150/53340, Loss: 1.5509\n",
      "Epoch 1, Batch 14200/53340, Loss: 0.8073\n",
      "Epoch 1, Batch 14250/53340, Loss: 1.3460\n",
      "Epoch 1, Batch 14300/53340, Loss: 1.1719\n",
      "Epoch 1, Batch 14350/53340, Loss: 1.7828\n",
      "Epoch 1, Batch 14400/53340, Loss: 1.2546\n",
      "Epoch 1, Batch 14450/53340, Loss: 1.5262\n",
      "Epoch 1, Batch 14500/53340, Loss: 1.7877\n",
      "Epoch 1, Batch 14550/53340, Loss: 1.1484\n",
      "Epoch 1, Batch 14600/53340, Loss: 0.8908\n",
      "Epoch 1, Batch 14650/53340, Loss: 1.3352\n",
      "Epoch 1, Batch 14700/53340, Loss: 0.9086\n",
      "Epoch 1, Batch 14750/53340, Loss: 1.7951\n",
      "Epoch 1, Batch 14800/53340, Loss: 1.2963\n",
      "Epoch 1, Batch 14850/53340, Loss: 2.2241\n",
      "Epoch 1, Batch 14900/53340, Loss: 1.1659\n",
      "Epoch 1, Batch 14950/53340, Loss: 1.7283\n",
      "Epoch 1, Batch 15000/53340, Loss: 1.8170\n",
      "Epoch 1, Batch 15050/53340, Loss: 1.6195\n",
      "Epoch 1, Batch 15100/53340, Loss: 1.5157\n",
      "Epoch 1, Batch 15150/53340, Loss: 0.7822\n",
      "Epoch 1, Batch 15200/53340, Loss: 1.5178\n",
      "Epoch 1, Batch 15250/53340, Loss: 3.7018\n",
      "Epoch 1, Batch 15300/53340, Loss: 1.0817\n",
      "Epoch 1, Batch 15350/53340, Loss: 1.2332\n",
      "Epoch 1, Batch 15400/53340, Loss: 1.3913\n",
      "Epoch 1, Batch 15450/53340, Loss: 2.2774\n",
      "Epoch 1, Batch 15500/53340, Loss: 1.2994\n",
      "Epoch 1, Batch 15550/53340, Loss: 1.5015\n",
      "Epoch 1, Batch 15600/53340, Loss: 1.6601\n",
      "Epoch 1, Batch 15650/53340, Loss: 1.8365\n",
      "Epoch 1, Batch 15700/53340, Loss: 1.5136\n",
      "Epoch 1, Batch 15750/53340, Loss: 1.4078\n",
      "Epoch 1, Batch 15800/53340, Loss: 0.9391\n",
      "Epoch 1, Batch 15850/53340, Loss: 0.8041\n",
      "Epoch 1, Batch 15900/53340, Loss: 1.5768\n",
      "Epoch 1, Batch 15950/53340, Loss: 0.7586\n",
      "Epoch 1, Batch 16000/53340, Loss: 1.6544\n",
      "Epoch 1, Batch 16050/53340, Loss: 0.7018\n",
      "Epoch 1, Batch 16100/53340, Loss: 1.2737\n",
      "Epoch 1, Batch 16150/53340, Loss: 1.8045\n",
      "Epoch 1, Batch 16200/53340, Loss: 0.7377\n",
      "Epoch 1, Batch 16250/53340, Loss: 1.6027\n",
      "Epoch 1, Batch 16300/53340, Loss: 1.4010\n",
      "Epoch 1, Batch 16350/53340, Loss: 0.9153\n",
      "Epoch 1, Batch 16400/53340, Loss: 1.2794\n",
      "Epoch 1, Batch 16450/53340, Loss: 1.0907\n",
      "Epoch 1, Batch 16500/53340, Loss: 1.6766\n",
      "Epoch 1, Batch 16550/53340, Loss: 1.1646\n",
      "Epoch 1, Batch 16600/53340, Loss: 1.2587\n",
      "Epoch 1, Batch 16650/53340, Loss: 1.5010\n",
      "Epoch 1, Batch 16700/53340, Loss: 1.5155\n",
      "Epoch 1, Batch 16750/53340, Loss: 1.0138\n",
      "Epoch 1, Batch 16800/53340, Loss: 1.2755\n",
      "Epoch 1, Batch 16850/53340, Loss: 1.1498\n",
      "Epoch 1, Batch 16900/53340, Loss: 0.9067\n",
      "Epoch 1, Batch 16950/53340, Loss: 2.0067\n",
      "Epoch 1, Batch 17000/53340, Loss: 1.3645\n",
      "Epoch 1, Batch 17050/53340, Loss: 0.9301\n",
      "Epoch 1, Batch 17100/53340, Loss: 0.9192\n",
      "Epoch 1, Batch 17150/53340, Loss: 1.1204\n",
      "Epoch 1, Batch 17200/53340, Loss: 1.4215\n",
      "Epoch 1, Batch 17250/53340, Loss: 1.0939\n",
      "Epoch 1, Batch 17300/53340, Loss: 1.0541\n",
      "Epoch 1, Batch 17350/53340, Loss: 1.0898\n",
      "Epoch 1, Batch 17400/53340, Loss: 1.2433\n",
      "Epoch 1, Batch 17450/53340, Loss: 1.6188\n",
      "Epoch 1, Batch 17500/53340, Loss: 0.9607\n",
      "Epoch 1, Batch 17550/53340, Loss: 1.0301\n",
      "Epoch 1, Batch 17600/53340, Loss: 1.6816\n",
      "Epoch 1, Batch 17650/53340, Loss: 2.8188\n",
      "Epoch 1, Batch 17700/53340, Loss: 1.4566\n",
      "Epoch 1, Batch 17750/53340, Loss: 1.1078\n",
      "Epoch 1, Batch 17800/53340, Loss: 1.9115\n",
      "Epoch 1, Batch 17850/53340, Loss: 1.0165\n",
      "Epoch 1, Batch 17900/53340, Loss: 1.5688\n",
      "Epoch 1, Batch 17950/53340, Loss: 0.9589\n",
      "Epoch 1, Batch 18000/53340, Loss: 1.5238\n",
      "Epoch 1, Batch 18050/53340, Loss: 1.1710\n",
      "Epoch 1, Batch 18100/53340, Loss: 0.6363\n",
      "Epoch 1, Batch 18150/53340, Loss: 1.4699\n",
      "Epoch 1, Batch 18200/53340, Loss: 0.5224\n",
      "Epoch 1, Batch 18250/53340, Loss: 1.2515\n",
      "Epoch 1, Batch 18300/53340, Loss: 1.4417\n",
      "Epoch 1, Batch 18350/53340, Loss: 0.9602\n",
      "Epoch 1, Batch 18400/53340, Loss: 0.4181\n",
      "Epoch 1, Batch 18450/53340, Loss: 1.2678\n",
      "Epoch 1, Batch 18500/53340, Loss: 1.4063\n",
      "Epoch 1, Batch 18550/53340, Loss: 1.1793\n",
      "Epoch 1, Batch 18600/53340, Loss: 1.7795\n",
      "Epoch 1, Batch 18650/53340, Loss: 1.9133\n",
      "Epoch 1, Batch 18700/53340, Loss: 0.7911\n",
      "Epoch 1, Batch 18750/53340, Loss: 1.1167\n",
      "Epoch 1, Batch 18800/53340, Loss: 0.8974\n",
      "Epoch 1, Batch 18850/53340, Loss: 0.9006\n",
      "Epoch 1, Batch 18900/53340, Loss: 1.4874\n",
      "Epoch 1, Batch 18950/53340, Loss: 1.0249\n",
      "Epoch 1, Batch 19000/53340, Loss: 0.9801\n",
      "Epoch 1, Batch 19050/53340, Loss: 0.9658\n",
      "Epoch 1, Batch 19100/53340, Loss: 1.6414\n",
      "Epoch 1, Batch 19150/53340, Loss: 1.4821\n",
      "Epoch 1, Batch 19200/53340, Loss: 0.8344\n",
      "Epoch 1, Batch 19250/53340, Loss: 1.2340\n",
      "Epoch 1, Batch 19300/53340, Loss: 1.0738\n",
      "Epoch 1, Batch 19350/53340, Loss: 0.7731\n",
      "Epoch 1, Batch 19400/53340, Loss: 1.2578\n",
      "Epoch 1, Batch 19450/53340, Loss: 1.5398\n",
      "Epoch 1, Batch 19500/53340, Loss: 0.5242\n",
      "Epoch 1, Batch 19550/53340, Loss: 1.1605\n",
      "Epoch 1, Batch 19600/53340, Loss: 1.3723\n",
      "Epoch 1, Batch 19650/53340, Loss: 0.4533\n",
      "Epoch 1, Batch 19700/53340, Loss: 1.4175\n",
      "Epoch 1, Batch 19750/53340, Loss: 1.4309\n",
      "Epoch 1, Batch 19800/53340, Loss: 1.4341\n",
      "Epoch 1, Batch 19850/53340, Loss: 1.5559\n",
      "Epoch 1, Batch 19900/53340, Loss: 1.3719\n",
      "Epoch 1, Batch 19950/53340, Loss: 1.4382\n",
      "Epoch 1, Batch 20000/53340, Loss: 0.7820\n",
      "Epoch 1, Batch 20050/53340, Loss: 0.9863\n",
      "Epoch 1, Batch 20100/53340, Loss: 0.8590\n",
      "Epoch 1, Batch 20150/53340, Loss: 0.7969\n",
      "Epoch 1, Batch 20200/53340, Loss: 0.9838\n",
      "Epoch 1, Batch 20250/53340, Loss: 1.1668\n",
      "Epoch 1, Batch 20300/53340, Loss: 1.7513\n",
      "Epoch 1, Batch 20350/53340, Loss: 0.8285\n",
      "Epoch 1, Batch 20400/53340, Loss: 0.6738\n",
      "Epoch 1, Batch 20450/53340, Loss: 1.6775\n",
      "Epoch 1, Batch 20500/53340, Loss: 1.0801\n",
      "Epoch 1, Batch 20550/53340, Loss: 1.0262\n",
      "Epoch 1, Batch 20600/53340, Loss: 1.1851\n",
      "Epoch 1, Batch 20650/53340, Loss: 0.8094\n",
      "Epoch 1, Batch 20700/53340, Loss: 0.9900\n",
      "Epoch 1, Batch 20750/53340, Loss: 0.9515\n",
      "Epoch 1, Batch 20800/53340, Loss: 0.9975\n",
      "Epoch 1, Batch 20850/53340, Loss: 1.3126\n",
      "Epoch 1, Batch 20900/53340, Loss: 1.0081\n",
      "Epoch 1, Batch 20950/53340, Loss: 0.6707\n",
      "Epoch 1, Batch 21000/53340, Loss: 0.7863\n",
      "Epoch 1, Batch 21050/53340, Loss: 1.5116\n",
      "Epoch 1, Batch 21100/53340, Loss: 1.3351\n",
      "Epoch 1, Batch 21150/53340, Loss: 0.4590\n",
      "Epoch 1, Batch 21200/53340, Loss: 1.1871\n",
      "Epoch 1, Batch 21250/53340, Loss: 1.1026\n",
      "Epoch 1, Batch 21300/53340, Loss: 1.5233\n",
      "Epoch 1, Batch 21350/53340, Loss: 1.3124\n",
      "Epoch 1, Batch 21400/53340, Loss: 0.7011\n",
      "Epoch 1, Batch 21450/53340, Loss: 0.8409\n",
      "Epoch 1, Batch 21500/53340, Loss: 1.3728\n",
      "Epoch 1, Batch 21550/53340, Loss: 1.3596\n",
      "Epoch 1, Batch 21600/53340, Loss: 0.7221\n",
      "Epoch 1, Batch 21650/53340, Loss: 1.8358\n",
      "Epoch 1, Batch 21700/53340, Loss: 0.4877\n",
      "Epoch 1, Batch 21750/53340, Loss: 1.1092\n",
      "Epoch 1, Batch 21800/53340, Loss: 0.7116\n",
      "Epoch 1, Batch 21850/53340, Loss: 0.7045\n",
      "Epoch 1, Batch 21900/53340, Loss: 1.3733\n",
      "Epoch 1, Batch 21950/53340, Loss: 1.2830\n",
      "Epoch 1, Batch 22000/53340, Loss: 1.0376\n",
      "Epoch 1, Batch 22050/53340, Loss: 1.1727\n",
      "Epoch 1, Batch 22100/53340, Loss: 0.9643\n",
      "Epoch 1, Batch 22150/53340, Loss: 1.1193\n",
      "Epoch 1, Batch 22200/53340, Loss: 2.3086\n",
      "Epoch 1, Batch 22250/53340, Loss: 0.6382\n",
      "Epoch 1, Batch 22300/53340, Loss: 1.3071\n",
      "Epoch 1, Batch 22350/53340, Loss: 0.9917\n",
      "Epoch 1, Batch 22400/53340, Loss: 0.7976\n",
      "Epoch 1, Batch 22450/53340, Loss: 0.6915\n",
      "Epoch 1, Batch 22500/53340, Loss: 1.0943\n",
      "Epoch 1, Batch 22550/53340, Loss: 0.9446\n",
      "Epoch 1, Batch 22600/53340, Loss: 0.8924\n",
      "Epoch 1, Batch 22650/53340, Loss: 1.3184\n",
      "Epoch 1, Batch 22700/53340, Loss: 1.1087\n",
      "Epoch 1, Batch 22750/53340, Loss: 1.1137\n",
      "Epoch 1, Batch 22800/53340, Loss: 0.9293\n",
      "Epoch 1, Batch 22850/53340, Loss: 1.0359\n",
      "Epoch 1, Batch 22900/53340, Loss: 1.5058\n",
      "Epoch 1, Batch 22950/53340, Loss: 0.8777\n",
      "Epoch 1, Batch 23000/53340, Loss: 1.0229\n",
      "Epoch 1, Batch 23050/53340, Loss: 1.0887\n",
      "Epoch 1, Batch 23100/53340, Loss: 2.9294\n",
      "Epoch 1, Batch 23150/53340, Loss: 1.2465\n",
      "Epoch 1, Batch 23200/53340, Loss: 0.6506\n",
      "Epoch 1, Batch 23250/53340, Loss: 0.8217\n",
      "Epoch 1, Batch 23300/53340, Loss: 0.7529\n",
      "Epoch 1, Batch 23350/53340, Loss: 2.1924\n",
      "Epoch 1, Batch 23400/53340, Loss: 0.7989\n",
      "Epoch 1, Batch 23450/53340, Loss: 1.3485\n",
      "Epoch 1, Batch 23500/53340, Loss: 0.5769\n",
      "Epoch 1, Batch 23550/53340, Loss: 1.5005\n",
      "Epoch 1, Batch 23600/53340, Loss: 1.1712\n",
      "Epoch 1, Batch 23650/53340, Loss: 1.0526\n",
      "Epoch 1, Batch 23700/53340, Loss: 0.9397\n",
      "Epoch 1, Batch 23750/53340, Loss: 0.9099\n",
      "Epoch 1, Batch 23800/53340, Loss: 1.2967\n",
      "Epoch 1, Batch 23850/53340, Loss: 0.3990\n",
      "Epoch 1, Batch 23900/53340, Loss: 1.3163\n",
      "Epoch 1, Batch 23950/53340, Loss: 1.3615\n",
      "Epoch 1, Batch 24000/53340, Loss: 0.9716\n",
      "Epoch 1, Batch 24050/53340, Loss: 1.8378\n",
      "Epoch 1, Batch 24100/53340, Loss: 1.0313\n",
      "Epoch 1, Batch 24150/53340, Loss: 0.6639\n",
      "Epoch 1, Batch 24200/53340, Loss: 1.2004\n",
      "Epoch 1, Batch 24250/53340, Loss: 1.1324\n",
      "Epoch 1, Batch 24300/53340, Loss: 1.0198\n",
      "Epoch 1, Batch 24350/53340, Loss: 0.6336\n",
      "Epoch 1, Batch 24400/53340, Loss: 1.5511\n",
      "Epoch 1, Batch 24450/53340, Loss: 0.6545\n",
      "Epoch 1, Batch 24500/53340, Loss: 1.1838\n",
      "Epoch 1, Batch 24550/53340, Loss: 1.1363\n",
      "Epoch 1, Batch 24600/53340, Loss: 0.9254\n",
      "Epoch 1, Batch 24650/53340, Loss: 0.8047\n",
      "Epoch 1, Batch 24700/53340, Loss: 0.7570\n",
      "Epoch 1, Batch 24750/53340, Loss: 0.5346\n",
      "Epoch 1, Batch 24800/53340, Loss: 0.7565\n",
      "Epoch 1, Batch 24850/53340, Loss: 1.4561\n",
      "Epoch 1, Batch 24900/53340, Loss: 0.8965\n",
      "Epoch 1, Batch 24950/53340, Loss: 1.1841\n",
      "Epoch 1, Batch 25000/53340, Loss: 0.9319\n",
      "Epoch 1, Batch 25050/53340, Loss: 1.4426\n",
      "Epoch 1, Batch 25100/53340, Loss: 1.3711\n",
      "Epoch 1, Batch 25150/53340, Loss: 1.0706\n",
      "Epoch 1, Batch 25200/53340, Loss: 1.0168\n",
      "Epoch 1, Batch 25250/53340, Loss: 1.0377\n",
      "Epoch 1, Batch 25300/53340, Loss: 0.6554\n",
      "Epoch 1, Batch 25350/53340, Loss: 1.2670\n",
      "Epoch 1, Batch 25400/53340, Loss: 1.1479\n",
      "Epoch 1, Batch 25450/53340, Loss: 1.1895\n",
      "Epoch 1, Batch 25500/53340, Loss: 0.9934\n",
      "Epoch 1, Batch 25550/53340, Loss: 0.7859\n",
      "Epoch 1, Batch 25600/53340, Loss: 0.7149\n",
      "Epoch 1, Batch 25650/53340, Loss: 1.2853\n",
      "Epoch 1, Batch 25700/53340, Loss: 1.3201\n",
      "Epoch 1, Batch 25750/53340, Loss: 0.7258\n",
      "Epoch 1, Batch 25800/53340, Loss: 0.5878\n",
      "Epoch 1, Batch 25850/53340, Loss: 0.8774\n",
      "Epoch 1, Batch 25900/53340, Loss: 1.1727\n",
      "Epoch 1, Batch 25950/53340, Loss: 1.4091\n",
      "Epoch 1, Batch 26000/53340, Loss: 1.8700\n",
      "Epoch 1, Batch 26050/53340, Loss: 0.8881\n",
      "Epoch 1, Batch 26100/53340, Loss: 0.7703\n",
      "Epoch 1, Batch 26150/53340, Loss: 0.9258\n",
      "Epoch 1, Batch 26200/53340, Loss: 0.9744\n",
      "Epoch 1, Batch 26250/53340, Loss: 1.8749\n",
      "Epoch 1, Batch 26300/53340, Loss: 0.3353\n",
      "Epoch 1, Batch 26350/53340, Loss: 0.5763\n",
      "Epoch 1, Batch 26400/53340, Loss: 0.6892\n",
      "Epoch 1, Batch 26450/53340, Loss: 0.6912\n",
      "Epoch 1, Batch 26500/53340, Loss: 1.2686\n",
      "Epoch 1, Batch 26550/53340, Loss: 0.6633\n",
      "Epoch 1, Batch 26600/53340, Loss: 1.4511\n",
      "Epoch 1, Batch 26650/53340, Loss: 0.9978\n",
      "Epoch 1, Batch 26700/53340, Loss: 0.8008\n",
      "Epoch 1, Batch 26750/53340, Loss: 1.2772\n",
      "Epoch 1, Batch 26800/53340, Loss: 0.8042\n",
      "Epoch 1, Batch 26850/53340, Loss: 0.7146\n",
      "Epoch 1, Batch 26900/53340, Loss: 1.4891\n",
      "Epoch 1, Batch 26950/53340, Loss: 1.4709\n",
      "Epoch 1, Batch 27000/53340, Loss: 1.4592\n",
      "Epoch 1, Batch 27050/53340, Loss: 0.8731\n",
      "Epoch 1, Batch 27100/53340, Loss: 1.1964\n",
      "Epoch 1, Batch 27150/53340, Loss: 1.0046\n",
      "Epoch 1, Batch 27200/53340, Loss: 0.5209\n",
      "Epoch 1, Batch 27250/53340, Loss: 1.2504\n",
      "Epoch 1, Batch 27300/53340, Loss: 1.0325\n",
      "Epoch 1, Batch 27350/53340, Loss: 0.7826\n",
      "Epoch 1, Batch 27400/53340, Loss: 0.6504\n",
      "Epoch 1, Batch 27450/53340, Loss: 0.5608\n",
      "Epoch 1, Batch 27500/53340, Loss: 1.3238\n",
      "Epoch 1, Batch 27550/53340, Loss: 3.0771\n",
      "Epoch 1, Batch 27600/53340, Loss: 0.8957\n",
      "Epoch 1, Batch 27650/53340, Loss: 1.2301\n",
      "Epoch 1, Batch 27700/53340, Loss: 0.8575\n",
      "Epoch 1, Batch 27750/53340, Loss: 1.0928\n",
      "Epoch 1, Batch 27800/53340, Loss: 0.9764\n",
      "Epoch 1, Batch 27850/53340, Loss: 1.1890\n",
      "Epoch 1, Batch 27900/53340, Loss: 0.7242\n",
      "Epoch 1, Batch 27950/53340, Loss: 0.6309\n",
      "Epoch 1, Batch 28000/53340, Loss: 0.7596\n",
      "Epoch 1, Batch 28050/53340, Loss: 0.7023\n",
      "Epoch 1, Batch 28100/53340, Loss: 1.3653\n",
      "Epoch 1, Batch 28150/53340, Loss: 0.5528\n",
      "Epoch 1, Batch 28200/53340, Loss: 0.8328\n",
      "Epoch 1, Batch 28250/53340, Loss: 1.0916\n",
      "Epoch 1, Batch 28300/53340, Loss: 0.4608\n",
      "Epoch 1, Batch 28350/53340, Loss: 0.4714\n",
      "Epoch 1, Batch 28400/53340, Loss: 0.6909\n",
      "Epoch 1, Batch 28450/53340, Loss: 1.4622\n",
      "Epoch 1, Batch 28500/53340, Loss: 2.5306\n",
      "Epoch 1, Batch 28550/53340, Loss: 0.9831\n",
      "Epoch 1, Batch 28600/53340, Loss: 0.7682\n",
      "Epoch 1, Batch 28650/53340, Loss: 0.7563\n",
      "Epoch 1, Batch 28700/53340, Loss: 1.5505\n",
      "Epoch 1, Batch 28750/53340, Loss: 1.5257\n",
      "Epoch 1, Batch 28800/53340, Loss: 1.0887\n",
      "Epoch 1, Batch 28850/53340, Loss: 1.1719\n",
      "Epoch 1, Batch 28900/53340, Loss: 0.8725\n",
      "Epoch 1, Batch 28950/53340, Loss: 0.8093\n",
      "Epoch 1, Batch 29000/53340, Loss: 0.5502\n",
      "Epoch 1, Batch 29050/53340, Loss: 0.9582\n",
      "Epoch 1, Batch 29100/53340, Loss: 0.7984\n",
      "Epoch 1, Batch 29150/53340, Loss: 1.0295\n",
      "Epoch 1, Batch 29200/53340, Loss: 0.6145\n",
      "Epoch 1, Batch 29250/53340, Loss: 1.3913\n",
      "Epoch 1, Batch 29300/53340, Loss: 1.1196\n",
      "Epoch 1, Batch 29350/53340, Loss: 1.4446\n",
      "Epoch 1, Batch 29400/53340, Loss: 1.5171\n",
      "Epoch 1, Batch 29450/53340, Loss: 1.2026\n",
      "Epoch 1, Batch 29500/53340, Loss: 1.3676\n",
      "Epoch 1, Batch 29550/53340, Loss: 0.6985\n",
      "Epoch 1, Batch 29600/53340, Loss: 1.0636\n",
      "Epoch 1, Batch 29650/53340, Loss: 1.2660\n",
      "Epoch 1, Batch 29700/53340, Loss: 1.1932\n",
      "Epoch 1, Batch 29750/53340, Loss: 0.4685\n",
      "Epoch 1, Batch 29800/53340, Loss: 0.7747\n",
      "Epoch 1, Batch 29850/53340, Loss: 0.9299\n",
      "Epoch 1, Batch 29900/53340, Loss: 0.4691\n",
      "Epoch 1, Batch 29950/53340, Loss: 1.2298\n",
      "Epoch 1, Batch 30000/53340, Loss: 1.3889\n",
      "Epoch 1, Batch 30050/53340, Loss: 1.0841\n",
      "Epoch 1, Batch 30100/53340, Loss: 0.7895\n",
      "Epoch 1, Batch 30150/53340, Loss: 0.5377\n",
      "Epoch 1, Batch 30200/53340, Loss: 0.7465\n",
      "Epoch 1, Batch 30250/53340, Loss: 0.8431\n",
      "Epoch 1, Batch 30300/53340, Loss: 2.0171\n",
      "Epoch 1, Batch 30350/53340, Loss: 0.9254\n",
      "Epoch 1, Batch 30400/53340, Loss: 1.2923\n",
      "Epoch 1, Batch 30450/53340, Loss: 1.2000\n",
      "Epoch 1, Batch 30500/53340, Loss: 2.1233\n",
      "Epoch 1, Batch 30550/53340, Loss: 0.7330\n",
      "Epoch 1, Batch 30600/53340, Loss: 0.7429\n",
      "Epoch 1, Batch 30650/53340, Loss: 0.4522\n",
      "Epoch 1, Batch 30700/53340, Loss: 1.2366\n",
      "Epoch 1, Batch 30750/53340, Loss: 0.7201\n",
      "Epoch 1, Batch 30800/53340, Loss: 1.2550\n",
      "Epoch 1, Batch 30850/53340, Loss: 0.8140\n",
      "Epoch 1, Batch 30900/53340, Loss: 0.9668\n",
      "Epoch 1, Batch 30950/53340, Loss: 0.6078\n",
      "Epoch 1, Batch 31000/53340, Loss: 0.8736\n",
      "Epoch 1, Batch 31050/53340, Loss: 1.1904\n",
      "Epoch 1, Batch 31100/53340, Loss: 1.4136\n",
      "Epoch 1, Batch 31150/53340, Loss: 1.3761\n",
      "Epoch 1, Batch 31200/53340, Loss: 0.7193\n",
      "Epoch 1, Batch 31250/53340, Loss: 1.2293\n",
      "Epoch 1, Batch 31300/53340, Loss: 0.6537\n",
      "Epoch 1, Batch 31350/53340, Loss: 1.4709\n",
      "Epoch 1, Batch 31400/53340, Loss: 1.1219\n",
      "Epoch 1, Batch 31450/53340, Loss: 0.8658\n",
      "Epoch 1, Batch 31500/53340, Loss: 0.6297\n",
      "Epoch 1, Batch 31550/53340, Loss: 1.4358\n",
      "Epoch 1, Batch 31600/53340, Loss: 0.5880\n",
      "Epoch 1, Batch 31650/53340, Loss: 0.7138\n",
      "Epoch 1, Batch 31700/53340, Loss: 0.7467\n",
      "Epoch 1, Batch 31750/53340, Loss: 0.4365\n",
      "Epoch 1, Batch 31800/53340, Loss: 0.6918\n",
      "Epoch 1, Batch 31850/53340, Loss: 0.7249\n",
      "Epoch 1, Batch 31900/53340, Loss: 0.8138\n",
      "Epoch 1, Batch 31950/53340, Loss: 0.9145\n",
      "Epoch 1, Batch 32000/53340, Loss: 1.2134\n",
      "Epoch 1, Batch 32050/53340, Loss: 0.6979\n",
      "Epoch 1, Batch 32100/53340, Loss: 1.0138\n",
      "Epoch 1, Batch 32150/53340, Loss: 0.9792\n",
      "Epoch 1, Batch 32200/53340, Loss: 0.9478\n",
      "Epoch 1, Batch 32250/53340, Loss: 1.0531\n",
      "Epoch 1, Batch 32300/53340, Loss: 0.6156\n",
      "Epoch 1, Batch 32350/53340, Loss: 1.1635\n",
      "Epoch 1, Batch 32400/53340, Loss: 0.5926\n",
      "Epoch 1, Batch 32450/53340, Loss: 0.5780\n",
      "Epoch 1, Batch 32500/53340, Loss: 0.5426\n",
      "Epoch 1, Batch 32550/53340, Loss: 0.7934\n",
      "Epoch 1, Batch 32600/53340, Loss: 0.4229\n",
      "Epoch 1, Batch 32650/53340, Loss: 0.6142\n",
      "Epoch 1, Batch 32700/53340, Loss: 0.9285\n",
      "Epoch 1, Batch 32750/53340, Loss: 0.6336\n",
      "Epoch 1, Batch 32800/53340, Loss: 0.8484\n",
      "Epoch 1, Batch 32850/53340, Loss: 2.4362\n",
      "Epoch 1, Batch 32900/53340, Loss: 0.4196\n",
      "Epoch 1, Batch 32950/53340, Loss: 0.8171\n",
      "Epoch 1, Batch 33000/53340, Loss: 0.9940\n",
      "Epoch 1, Batch 33050/53340, Loss: 1.0495\n",
      "Epoch 1, Batch 33100/53340, Loss: 0.8349\n",
      "Epoch 1, Batch 33150/53340, Loss: 0.4790\n",
      "Epoch 1, Batch 33200/53340, Loss: 1.4124\n",
      "Epoch 1, Batch 33250/53340, Loss: 0.8433\n",
      "Epoch 1, Batch 33300/53340, Loss: 0.7081\n",
      "Epoch 1, Batch 33350/53340, Loss: 0.9481\n",
      "Epoch 1, Batch 33400/53340, Loss: 1.2371\n",
      "Epoch 1, Batch 33450/53340, Loss: 0.7766\n",
      "Epoch 1, Batch 33500/53340, Loss: 0.3597\n",
      "Epoch 1, Batch 33550/53340, Loss: 0.7497\n",
      "Epoch 1, Batch 33600/53340, Loss: 0.7052\n",
      "Epoch 1, Batch 33650/53340, Loss: 0.4631\n",
      "Epoch 1, Batch 33700/53340, Loss: 0.5558\n",
      "Epoch 1, Batch 33750/53340, Loss: 0.7625\n",
      "Epoch 1, Batch 33800/53340, Loss: 0.9885\n",
      "Epoch 1, Batch 33850/53340, Loss: 0.4249\n",
      "Epoch 1, Batch 33900/53340, Loss: 0.8026\n",
      "Epoch 1, Batch 33950/53340, Loss: 0.4786\n",
      "Epoch 1, Batch 34000/53340, Loss: 1.1609\n",
      "Epoch 1, Batch 34050/53340, Loss: 0.9691\n",
      "Epoch 1, Batch 34100/53340, Loss: 0.7775\n",
      "Epoch 1, Batch 34150/53340, Loss: 1.1276\n",
      "Epoch 1, Batch 34200/53340, Loss: 1.3549\n",
      "Epoch 1, Batch 34250/53340, Loss: 0.5709\n",
      "Epoch 1, Batch 34300/53340, Loss: 0.8486\n",
      "Epoch 1, Batch 34350/53340, Loss: 0.8285\n",
      "Epoch 1, Batch 34400/53340, Loss: 0.9007\n",
      "Epoch 1, Batch 34450/53340, Loss: 1.0158\n",
      "Epoch 1, Batch 34500/53340, Loss: 1.1306\n",
      "Epoch 1, Batch 34550/53340, Loss: 0.8551\n",
      "Epoch 1, Batch 34600/53340, Loss: 0.8815\n",
      "Epoch 1, Batch 34650/53340, Loss: 1.0233\n",
      "Epoch 1, Batch 34700/53340, Loss: 0.8007\n",
      "Epoch 1, Batch 34750/53340, Loss: 0.8820\n",
      "Epoch 1, Batch 34800/53340, Loss: 0.6460\n",
      "Epoch 1, Batch 34850/53340, Loss: 0.5392\n",
      "Epoch 1, Batch 34900/53340, Loss: 0.5827\n",
      "Epoch 1, Batch 34950/53340, Loss: 0.7671\n",
      "Epoch 1, Batch 35000/53340, Loss: 0.8351\n",
      "Epoch 1, Batch 35050/53340, Loss: 1.9426\n",
      "Epoch 1, Batch 35100/53340, Loss: 1.6567\n",
      "Epoch 1, Batch 35150/53340, Loss: 1.0620\n",
      "Epoch 1, Batch 35200/53340, Loss: 0.5310\n",
      "Epoch 1, Batch 35250/53340, Loss: 0.7699\n",
      "Epoch 1, Batch 35300/53340, Loss: 0.8847\n",
      "Epoch 1, Batch 35350/53340, Loss: 0.7586\n",
      "Epoch 1, Batch 35400/53340, Loss: 1.0950\n",
      "Epoch 1, Batch 35450/53340, Loss: 0.8325\n",
      "Epoch 1, Batch 35500/53340, Loss: 0.9469\n",
      "Epoch 1, Batch 35550/53340, Loss: 1.1452\n",
      "Epoch 1, Batch 35600/53340, Loss: 0.8152\n",
      "Epoch 1, Batch 35650/53340, Loss: 0.6457\n",
      "Epoch 1, Batch 35700/53340, Loss: 0.7872\n",
      "Epoch 1, Batch 35750/53340, Loss: 0.8759\n",
      "Epoch 1, Batch 35800/53340, Loss: 1.2333\n",
      "Epoch 1, Batch 35850/53340, Loss: 0.5657\n",
      "Epoch 1, Batch 35900/53340, Loss: 0.8238\n",
      "Epoch 1, Batch 35950/53340, Loss: 1.9398\n",
      "Epoch 1, Batch 36000/53340, Loss: 0.4470\n",
      "Epoch 1, Batch 36050/53340, Loss: 1.4300\n",
      "Epoch 1, Batch 36100/53340, Loss: 1.0413\n",
      "Epoch 1, Batch 36150/53340, Loss: 0.6384\n",
      "Epoch 1, Batch 36200/53340, Loss: 1.0875\n",
      "Epoch 1, Batch 36250/53340, Loss: 0.4549\n",
      "Epoch 1, Batch 36300/53340, Loss: 0.9721\n",
      "Epoch 1, Batch 36350/53340, Loss: 0.9436\n",
      "Epoch 1, Batch 36400/53340, Loss: 0.4737\n",
      "Epoch 1, Batch 36450/53340, Loss: 0.6612\n",
      "Epoch 1, Batch 36500/53340, Loss: 0.4075\n",
      "Epoch 1, Batch 36550/53340, Loss: 2.7865\n",
      "Epoch 1, Batch 36600/53340, Loss: 1.2581\n",
      "Epoch 1, Batch 36650/53340, Loss: 0.8215\n",
      "Epoch 1, Batch 36700/53340, Loss: 0.6462\n",
      "Epoch 1, Batch 36750/53340, Loss: 1.1427\n",
      "Epoch 1, Batch 36800/53340, Loss: 1.3612\n",
      "Epoch 1, Batch 36850/53340, Loss: 0.7730\n",
      "Epoch 1, Batch 36900/53340, Loss: 0.6646\n",
      "Epoch 1, Batch 36950/53340, Loss: 0.7695\n",
      "Epoch 1, Batch 37000/53340, Loss: 0.9781\n",
      "Epoch 1, Batch 37050/53340, Loss: 0.6202\n",
      "Epoch 1, Batch 37100/53340, Loss: 0.8056\n",
      "Epoch 1, Batch 37150/53340, Loss: 1.6124\n",
      "Epoch 1, Batch 37200/53340, Loss: 0.5446\n",
      "Epoch 1, Batch 37250/53340, Loss: 1.7681\n",
      "Epoch 1, Batch 37300/53340, Loss: 0.9145\n",
      "Epoch 1, Batch 37350/53340, Loss: 0.6917\n",
      "Epoch 1, Batch 37400/53340, Loss: 1.0429\n",
      "Epoch 1, Batch 37450/53340, Loss: 0.8164\n",
      "Epoch 1, Batch 37500/53340, Loss: 0.3307\n",
      "Epoch 1, Batch 37550/53340, Loss: 2.3670\n",
      "Epoch 1, Batch 37600/53340, Loss: 1.2413\n",
      "Epoch 1, Batch 37650/53340, Loss: 1.0329\n",
      "Epoch 1, Batch 37700/53340, Loss: 1.1458\n",
      "Epoch 1, Batch 37750/53340, Loss: 1.0745\n",
      "Epoch 1, Batch 37800/53340, Loss: 0.5880\n",
      "Epoch 1, Batch 37850/53340, Loss: 0.5713\n",
      "Epoch 1, Batch 37900/53340, Loss: 1.1546\n",
      "Epoch 1, Batch 37950/53340, Loss: 0.8866\n",
      "Epoch 1, Batch 38000/53340, Loss: 0.6973\n",
      "Epoch 1, Batch 38050/53340, Loss: 0.3899\n",
      "Epoch 1, Batch 38100/53340, Loss: 0.5250\n",
      "Epoch 1, Batch 38150/53340, Loss: 0.7107\n",
      "Epoch 1, Batch 38200/53340, Loss: 0.8256\n",
      "Epoch 1, Batch 38250/53340, Loss: 0.8703\n",
      "Epoch 1, Batch 38300/53340, Loss: 0.5506\n",
      "Epoch 1, Batch 38350/53340, Loss: 0.9629\n",
      "Epoch 1, Batch 38400/53340, Loss: 0.7739\n",
      "Epoch 1, Batch 38450/53340, Loss: 1.0607\n",
      "Epoch 1, Batch 38500/53340, Loss: 0.8190\n",
      "Epoch 1, Batch 38550/53340, Loss: 1.4312\n",
      "Epoch 1, Batch 38600/53340, Loss: 0.7771\n",
      "Epoch 1, Batch 38650/53340, Loss: 0.6727\n",
      "Epoch 1, Batch 38700/53340, Loss: 1.0087\n",
      "Epoch 1, Batch 38750/53340, Loss: 0.5942\n",
      "Epoch 1, Batch 38800/53340, Loss: 0.8314\n",
      "Epoch 1, Batch 38850/53340, Loss: 0.6664\n",
      "Epoch 1, Batch 38900/53340, Loss: 0.9953\n",
      "Epoch 1, Batch 38950/53340, Loss: 1.2634\n",
      "Epoch 1, Batch 39000/53340, Loss: 0.7097\n",
      "Epoch 1, Batch 39050/53340, Loss: 0.7023\n",
      "Epoch 1, Batch 39100/53340, Loss: 1.0900\n",
      "Epoch 1, Batch 39150/53340, Loss: 0.3140\n",
      "Epoch 1, Batch 39200/53340, Loss: 0.8258\n",
      "Epoch 1, Batch 39250/53340, Loss: 0.4372\n",
      "Epoch 1, Batch 39300/53340, Loss: 0.5664\n",
      "Epoch 1, Batch 39350/53340, Loss: 0.8234\n",
      "Epoch 1, Batch 39400/53340, Loss: 0.8416\n",
      "Epoch 1, Batch 39450/53340, Loss: 1.0983\n",
      "Epoch 1, Batch 39500/53340, Loss: 0.7236\n",
      "Epoch 1, Batch 39550/53340, Loss: 0.9018\n",
      "Epoch 1, Batch 39600/53340, Loss: 1.0798\n",
      "Epoch 1, Batch 39650/53340, Loss: 0.9879\n",
      "Epoch 1, Batch 39700/53340, Loss: 1.0781\n",
      "Epoch 1, Batch 39750/53340, Loss: 0.9021\n",
      "Epoch 1, Batch 39800/53340, Loss: 1.2530\n",
      "Epoch 1, Batch 39850/53340, Loss: 0.9848\n",
      "Epoch 1, Batch 39900/53340, Loss: 0.6246\n",
      "Epoch 1, Batch 39950/53340, Loss: 0.6723\n",
      "Epoch 1, Batch 40000/53340, Loss: 0.9954\n",
      "Epoch 1, Batch 40050/53340, Loss: 0.3439\n",
      "Epoch 1, Batch 40100/53340, Loss: 0.6485\n",
      "Epoch 1, Batch 40150/53340, Loss: 0.7603\n",
      "Epoch 1, Batch 40200/53340, Loss: 0.8944\n",
      "Epoch 1, Batch 40250/53340, Loss: 0.5278\n",
      "Epoch 1, Batch 40300/53340, Loss: 0.4717\n",
      "Epoch 1, Batch 40350/53340, Loss: 0.7347\n",
      "Epoch 1, Batch 40400/53340, Loss: 0.7477\n",
      "Epoch 1, Batch 40450/53340, Loss: 0.7338\n",
      "Epoch 1, Batch 40500/53340, Loss: 0.4950\n",
      "Epoch 1, Batch 40550/53340, Loss: 1.4903\n",
      "Epoch 1, Batch 40600/53340, Loss: 0.8412\n",
      "Epoch 1, Batch 40650/53340, Loss: 1.0443\n",
      "Epoch 1, Batch 40700/53340, Loss: 0.8243\n",
      "Epoch 1, Batch 40750/53340, Loss: 0.6306\n",
      "Epoch 1, Batch 40800/53340, Loss: 0.6273\n",
      "Epoch 1, Batch 40850/53340, Loss: 0.6418\n",
      "Epoch 1, Batch 40900/53340, Loss: 0.3662\n",
      "Epoch 1, Batch 40950/53340, Loss: 0.8749\n",
      "Epoch 1, Batch 41000/53340, Loss: 0.7272\n",
      "Epoch 1, Batch 41050/53340, Loss: 1.1987\n",
      "Epoch 1, Batch 41100/53340, Loss: 0.8837\n",
      "Epoch 1, Batch 41150/53340, Loss: 0.8288\n",
      "Epoch 1, Batch 41200/53340, Loss: 0.8313\n",
      "Epoch 1, Batch 41250/53340, Loss: 0.6522\n",
      "Epoch 1, Batch 41300/53340, Loss: 0.9138\n",
      "Epoch 1, Batch 41350/53340, Loss: 0.4290\n",
      "Epoch 1, Batch 41400/53340, Loss: 0.5821\n",
      "Epoch 1, Batch 41450/53340, Loss: 1.1127\n",
      "Epoch 1, Batch 41500/53340, Loss: 0.8111\n",
      "Epoch 1, Batch 41550/53340, Loss: 0.6111\n",
      "Epoch 1, Batch 41600/53340, Loss: 0.3607\n",
      "Epoch 1, Batch 41650/53340, Loss: 0.9020\n",
      "Epoch 1, Batch 41700/53340, Loss: 0.7251\n",
      "Epoch 1, Batch 41750/53340, Loss: 0.7951\n",
      "Epoch 1, Batch 41800/53340, Loss: 0.7975\n",
      "Epoch 1, Batch 41850/53340, Loss: 0.4619\n",
      "Epoch 1, Batch 41900/53340, Loss: 1.3229\n",
      "Epoch 1, Batch 41950/53340, Loss: 0.8479\n",
      "Epoch 1, Batch 42000/53340, Loss: 1.3566\n",
      "Epoch 1, Batch 42050/53340, Loss: 0.9486\n",
      "Epoch 1, Batch 42100/53340, Loss: 0.7431\n",
      "Epoch 1, Batch 42150/53340, Loss: 1.0040\n",
      "Epoch 1, Batch 42200/53340, Loss: 0.6600\n",
      "Epoch 1, Batch 42250/53340, Loss: 0.3996\n",
      "Epoch 1, Batch 42300/53340, Loss: 0.6077\n",
      "Epoch 1, Batch 42350/53340, Loss: 1.0686\n",
      "Epoch 1, Batch 42400/53340, Loss: 0.6243\n",
      "Epoch 1, Batch 42450/53340, Loss: 0.6087\n",
      "Epoch 1, Batch 42500/53340, Loss: 0.8609\n",
      "Epoch 1, Batch 42550/53340, Loss: 0.8440\n",
      "Epoch 1, Batch 42600/53340, Loss: 1.1674\n",
      "Epoch 1, Batch 42650/53340, Loss: 0.6977\n",
      "Epoch 1, Batch 42700/53340, Loss: 0.4496\n",
      "Epoch 1, Batch 42750/53340, Loss: 1.1879\n",
      "Epoch 1, Batch 42800/53340, Loss: 0.6326\n",
      "Epoch 1, Batch 42850/53340, Loss: 1.5805\n",
      "Epoch 1, Batch 42900/53340, Loss: 1.1463\n",
      "Epoch 1, Batch 42950/53340, Loss: 0.8354\n",
      "Epoch 1, Batch 43000/53340, Loss: 1.0482\n",
      "Epoch 1, Batch 43050/53340, Loss: 0.6996\n",
      "Epoch 1, Batch 43100/53340, Loss: 0.9041\n",
      "Epoch 1, Batch 43150/53340, Loss: 0.8920\n",
      "Epoch 1, Batch 43200/53340, Loss: 0.5048\n",
      "Epoch 1, Batch 43250/53340, Loss: 0.8571\n",
      "Epoch 1, Batch 43300/53340, Loss: 0.8736\n",
      "Epoch 1, Batch 43350/53340, Loss: 0.9399\n",
      "Epoch 1, Batch 43400/53340, Loss: 0.7126\n",
      "Epoch 1, Batch 43450/53340, Loss: 0.8553\n",
      "Epoch 1, Batch 43500/53340, Loss: 1.3760\n",
      "Epoch 1, Batch 43550/53340, Loss: 0.6018\n",
      "Epoch 1, Batch 43600/53340, Loss: 0.7445\n",
      "Epoch 1, Batch 43650/53340, Loss: 1.2556\n",
      "Epoch 1, Batch 43700/53340, Loss: 0.3945\n",
      "Epoch 1, Batch 43750/53340, Loss: 0.7804\n",
      "Epoch 1, Batch 43800/53340, Loss: 0.6199\n",
      "Epoch 1, Batch 43850/53340, Loss: 0.6786\n",
      "Epoch 1, Batch 43900/53340, Loss: 0.8514\n",
      "Epoch 1, Batch 43950/53340, Loss: 0.5775\n",
      "Epoch 1, Batch 44000/53340, Loss: 0.5889\n",
      "Epoch 1, Batch 44050/53340, Loss: 0.7416\n",
      "Epoch 1, Batch 44100/53340, Loss: 0.8408\n",
      "Epoch 1, Batch 44150/53340, Loss: 0.4839\n",
      "Epoch 1, Batch 44200/53340, Loss: 0.6118\n",
      "Epoch 1, Batch 44250/53340, Loss: 0.9785\n",
      "Epoch 1, Batch 44300/53340, Loss: 0.9011\n",
      "Epoch 1, Batch 44350/53340, Loss: 0.7136\n",
      "Epoch 1, Batch 44400/53340, Loss: 0.8117\n",
      "Epoch 1, Batch 44450/53340, Loss: 0.7183\n",
      "Epoch 1, Batch 44500/53340, Loss: 0.5211\n",
      "Epoch 1, Batch 44550/53340, Loss: 0.9721\n",
      "Epoch 1, Batch 44600/53340, Loss: 0.6839\n",
      "Epoch 1, Batch 44650/53340, Loss: 0.9435\n",
      "Epoch 1, Batch 44700/53340, Loss: 0.5041\n",
      "Epoch 1, Batch 44750/53340, Loss: 0.6709\n",
      "Epoch 1, Batch 44800/53340, Loss: 0.3613\n",
      "Epoch 1, Batch 44850/53340, Loss: 0.8053\n",
      "Epoch 1, Batch 44900/53340, Loss: 0.7486\n",
      "Epoch 1, Batch 44950/53340, Loss: 0.6496\n",
      "Epoch 1, Batch 45000/53340, Loss: 1.0378\n",
      "Epoch 1, Batch 45050/53340, Loss: 0.7822\n",
      "Epoch 1, Batch 45100/53340, Loss: 1.3433\n",
      "Epoch 1, Batch 45150/53340, Loss: 0.6059\n",
      "Epoch 1, Batch 45200/53340, Loss: 0.8258\n",
      "Epoch 1, Batch 45250/53340, Loss: 0.7181\n",
      "Epoch 1, Batch 45300/53340, Loss: 1.6588\n",
      "Epoch 1, Batch 45350/53340, Loss: 0.7083\n",
      "Epoch 1, Batch 45400/53340, Loss: 0.6972\n",
      "Epoch 1, Batch 45450/53340, Loss: 1.5857\n",
      "Epoch 1, Batch 45500/53340, Loss: 1.4100\n",
      "Epoch 1, Batch 45550/53340, Loss: 0.4356\n",
      "Epoch 1, Batch 45600/53340, Loss: 0.6670\n",
      "Epoch 1, Batch 45650/53340, Loss: 0.7892\n",
      "Epoch 1, Batch 45700/53340, Loss: 0.4955\n",
      "Epoch 1, Batch 45750/53340, Loss: 1.2175\n",
      "Epoch 1, Batch 45800/53340, Loss: 1.0675\n",
      "Epoch 1, Batch 45850/53340, Loss: 0.6695\n",
      "Epoch 1, Batch 45900/53340, Loss: 0.6988\n",
      "Epoch 1, Batch 45950/53340, Loss: 0.8027\n",
      "Epoch 1, Batch 46000/53340, Loss: 0.5764\n",
      "Epoch 1, Batch 46050/53340, Loss: 0.8647\n",
      "Epoch 1, Batch 46100/53340, Loss: 0.6719\n",
      "Epoch 1, Batch 46150/53340, Loss: 0.9779\n",
      "Epoch 1, Batch 46200/53340, Loss: 0.6613\n",
      "Epoch 1, Batch 46250/53340, Loss: 1.1943\n",
      "Epoch 1, Batch 46300/53340, Loss: 0.6620\n",
      "Epoch 1, Batch 46350/53340, Loss: 0.2609\n",
      "Epoch 1, Batch 46400/53340, Loss: 0.6414\n",
      "Epoch 1, Batch 46450/53340, Loss: 0.6750\n",
      "Epoch 1, Batch 46500/53340, Loss: 0.5640\n",
      "Epoch 1, Batch 46550/53340, Loss: 0.6621\n",
      "Epoch 1, Batch 46600/53340, Loss: 0.4646\n",
      "Epoch 1, Batch 46650/53340, Loss: 0.5183\n",
      "Epoch 1, Batch 46700/53340, Loss: 0.9692\n",
      "Epoch 1, Batch 46750/53340, Loss: 0.6617\n",
      "Epoch 1, Batch 46800/53340, Loss: 0.5698\n",
      "Epoch 1, Batch 46850/53340, Loss: 0.4702\n",
      "Epoch 1, Batch 46900/53340, Loss: 0.7960\n",
      "Epoch 1, Batch 46950/53340, Loss: 0.7865\n",
      "Epoch 1, Batch 47000/53340, Loss: 0.3632\n",
      "Epoch 1, Batch 47050/53340, Loss: 0.4349\n",
      "Epoch 1, Batch 47100/53340, Loss: 0.6804\n",
      "Epoch 1, Batch 47150/53340, Loss: 0.8338\n",
      "Epoch 1, Batch 47200/53340, Loss: 0.6526\n",
      "Epoch 1, Batch 47250/53340, Loss: 0.4188\n",
      "Epoch 1, Batch 47300/53340, Loss: 0.4200\n",
      "Epoch 1, Batch 47350/53340, Loss: 0.4197\n",
      "Epoch 1, Batch 47400/53340, Loss: 0.4420\n",
      "Epoch 1, Batch 47450/53340, Loss: 0.6748\n",
      "Epoch 1, Batch 47500/53340, Loss: 0.5955\n",
      "Epoch 1, Batch 47550/53340, Loss: 0.9174\n",
      "Epoch 1, Batch 47600/53340, Loss: 0.7874\n",
      "Epoch 1, Batch 47650/53340, Loss: 0.9398\n",
      "Epoch 1, Batch 47700/53340, Loss: 1.3053\n",
      "Epoch 1, Batch 47750/53340, Loss: 0.6376\n",
      "Epoch 1, Batch 47800/53340, Loss: 0.7758\n",
      "Epoch 1, Batch 47850/53340, Loss: 0.5009\n",
      "Epoch 1, Batch 47900/53340, Loss: 0.9764\n",
      "Epoch 1, Batch 47950/53340, Loss: 0.7352\n",
      "Epoch 1, Batch 48000/53340, Loss: 1.0998\n",
      "Epoch 1, Batch 48050/53340, Loss: 1.0487\n",
      "Epoch 1, Batch 48100/53340, Loss: 0.4115\n",
      "Epoch 1, Batch 48150/53340, Loss: 0.6031\n",
      "Epoch 1, Batch 48200/53340, Loss: 1.2110\n",
      "Epoch 1, Batch 48250/53340, Loss: 0.9626\n",
      "Epoch 1, Batch 48300/53340, Loss: 0.6342\n",
      "Epoch 1, Batch 48350/53340, Loss: 0.8790\n",
      "Epoch 1, Batch 48400/53340, Loss: 0.7275\n",
      "Epoch 1, Batch 48450/53340, Loss: 0.5657\n",
      "Epoch 1, Batch 48500/53340, Loss: 0.6514\n",
      "Epoch 1, Batch 48550/53340, Loss: 0.7739\n",
      "Epoch 1, Batch 48600/53340, Loss: 0.9868\n",
      "Epoch 1, Batch 48650/53340, Loss: 1.4885\n",
      "Epoch 1, Batch 48700/53340, Loss: 0.3858\n",
      "Epoch 1, Batch 48750/53340, Loss: 0.5604\n",
      "Epoch 1, Batch 48800/53340, Loss: 2.8095\n",
      "Epoch 1, Batch 48850/53340, Loss: 0.8335\n",
      "Epoch 1, Batch 48900/53340, Loss: 0.5277\n",
      "Epoch 1, Batch 48950/53340, Loss: 1.1231\n",
      "Epoch 1, Batch 49000/53340, Loss: 0.6196\n",
      "Epoch 1, Batch 49050/53340, Loss: 1.7288\n",
      "Epoch 1, Batch 49100/53340, Loss: 1.1480\n",
      "Epoch 1, Batch 49150/53340, Loss: 0.4568\n",
      "Epoch 1, Batch 49200/53340, Loss: 1.0589\n",
      "Epoch 1, Batch 49250/53340, Loss: 0.8481\n",
      "Epoch 1, Batch 49300/53340, Loss: 0.9993\n",
      "Epoch 1, Batch 49350/53340, Loss: 0.5085\n",
      "Epoch 1, Batch 49400/53340, Loss: 0.5042\n",
      "Epoch 1, Batch 49450/53340, Loss: 0.8121\n",
      "Epoch 1, Batch 49500/53340, Loss: 0.6315\n",
      "Epoch 1, Batch 49550/53340, Loss: 0.9438\n",
      "Epoch 1, Batch 49600/53340, Loss: 0.8683\n",
      "Epoch 1, Batch 49650/53340, Loss: 1.0892\n",
      "Epoch 1, Batch 49700/53340, Loss: 0.3644\n",
      "Epoch 1, Batch 49750/53340, Loss: 0.9415\n",
      "Epoch 1, Batch 49800/53340, Loss: 0.7880\n",
      "Epoch 1, Batch 49850/53340, Loss: 0.4819\n",
      "Epoch 1, Batch 49900/53340, Loss: 0.6230\n",
      "Epoch 1, Batch 49950/53340, Loss: 0.8572\n",
      "Epoch 1, Batch 50000/53340, Loss: 0.5586\n",
      "Epoch 1, Batch 50050/53340, Loss: 0.4510\n",
      "Epoch 1, Batch 50100/53340, Loss: 0.3827\n",
      "Epoch 1, Batch 50150/53340, Loss: 1.0849\n",
      "Epoch 1, Batch 50200/53340, Loss: 0.7259\n",
      "Epoch 1, Batch 50250/53340, Loss: 0.4403\n",
      "Epoch 1, Batch 50300/53340, Loss: 0.9560\n",
      "Epoch 1, Batch 50350/53340, Loss: 0.7036\n",
      "Epoch 1, Batch 50400/53340, Loss: 0.9814\n",
      "Epoch 1, Batch 50450/53340, Loss: 0.8534\n",
      "Epoch 1, Batch 50500/53340, Loss: 0.7383\n",
      "Epoch 1, Batch 50550/53340, Loss: 0.7665\n",
      "Epoch 1, Batch 50600/53340, Loss: 0.7351\n",
      "Epoch 1, Batch 50650/53340, Loss: 0.8833\n",
      "Epoch 1, Batch 50700/53340, Loss: 1.0927\n",
      "Epoch 1, Batch 50750/53340, Loss: 0.6350\n",
      "Epoch 1, Batch 50800/53340, Loss: 0.5706\n",
      "Epoch 1, Batch 50850/53340, Loss: 0.6678\n",
      "Epoch 1, Batch 50900/53340, Loss: 0.6588\n",
      "Epoch 1, Batch 50950/53340, Loss: 0.7032\n",
      "Epoch 1, Batch 51000/53340, Loss: 0.7226\n",
      "Epoch 1, Batch 51050/53340, Loss: 0.7056\n",
      "Epoch 1, Batch 51100/53340, Loss: 0.7098\n",
      "Epoch 1, Batch 51150/53340, Loss: 0.6916\n",
      "Epoch 1, Batch 51200/53340, Loss: 0.8010\n",
      "Epoch 1, Batch 51250/53340, Loss: 0.5056\n",
      "Epoch 1, Batch 51300/53340, Loss: 1.3018\n",
      "Epoch 1, Batch 51350/53340, Loss: 0.5721\n",
      "Epoch 1, Batch 51400/53340, Loss: 0.9486\n",
      "Epoch 1, Batch 51450/53340, Loss: 1.0593\n",
      "Epoch 1, Batch 51500/53340, Loss: 2.4554\n",
      "Epoch 1, Batch 51550/53340, Loss: 0.7075\n",
      "Epoch 1, Batch 51600/53340, Loss: 0.5156\n",
      "Epoch 1, Batch 51650/53340, Loss: 0.9937\n",
      "Epoch 1, Batch 51700/53340, Loss: 1.3194\n",
      "Epoch 1, Batch 51750/53340, Loss: 0.5300\n",
      "Epoch 1, Batch 51800/53340, Loss: 0.9810\n",
      "Epoch 1, Batch 51850/53340, Loss: 0.7064\n",
      "Epoch 1, Batch 51900/53340, Loss: 0.5160\n",
      "Epoch 1, Batch 51950/53340, Loss: 0.3126\n",
      "Epoch 1, Batch 52000/53340, Loss: 0.8451\n",
      "Epoch 1, Batch 52050/53340, Loss: 0.8011\n",
      "Epoch 1, Batch 52100/53340, Loss: 0.4124\n",
      "Epoch 1, Batch 52150/53340, Loss: 0.7691\n",
      "Epoch 1, Batch 52200/53340, Loss: 0.9372\n",
      "Epoch 1, Batch 52250/53340, Loss: 0.3719\n",
      "Epoch 1, Batch 52300/53340, Loss: 0.3668\n",
      "Epoch 1, Batch 52350/53340, Loss: 1.1055\n",
      "Epoch 1, Batch 52400/53340, Loss: 0.3963\n",
      "Epoch 1, Batch 52450/53340, Loss: 0.4606\n",
      "Epoch 1, Batch 52500/53340, Loss: 0.7263\n",
      "Epoch 1, Batch 52550/53340, Loss: 0.5728\n",
      "Epoch 1, Batch 52600/53340, Loss: 0.3874\n",
      "Epoch 1, Batch 52650/53340, Loss: 0.9203\n",
      "Epoch 1, Batch 52700/53340, Loss: 0.7312\n",
      "Epoch 1, Batch 52750/53340, Loss: 3.0164\n",
      "Epoch 1, Batch 52800/53340, Loss: 0.4352\n",
      "Epoch 1, Batch 52850/53340, Loss: 0.7752\n",
      "Epoch 1, Batch 52900/53340, Loss: 1.0463\n",
      "Epoch 1, Batch 52950/53340, Loss: 0.8163\n",
      "Epoch 1, Batch 53000/53340, Loss: 0.5923\n",
      "Epoch 1, Batch 53050/53340, Loss: 0.3217\n",
      "Epoch 1, Batch 53100/53340, Loss: 1.2003\n",
      "Epoch 1, Batch 53150/53340, Loss: 0.8621\n",
      "Epoch 1, Batch 53200/53340, Loss: 0.8817\n",
      "Epoch 1, Batch 53250/53340, Loss: 0.9540\n",
      "Epoch 1, Batch 53300/53340, Loss: 0.4732\n",
      "\n",
      "Average training loss: 1.2990\n",
      "Learning rate: 0.000100\n",
      "\n",
      "Evaluating on test set...\n",
      "Evaluating on 5384 image pairs...\n",
      "Evaluated 100/5384 pairs...\n",
      "Evaluated 200/5384 pairs...\n",
      "Evaluated 300/5384 pairs...\n",
      "Evaluated 400/5384 pairs...\n",
      "Evaluated 500/5384 pairs...\n",
      "Evaluated 600/5384 pairs...\n",
      "Evaluated 700/5384 pairs...\n",
      "Evaluated 800/5384 pairs...\n",
      "Evaluated 900/5384 pairs...\n",
      "Evaluated 1000/5384 pairs...\n",
      "Evaluated 1100/5384 pairs...\n",
      "Evaluated 1200/5384 pairs...\n",
      "Evaluated 1300/5384 pairs...\n",
      "Evaluated 1400/5384 pairs...\n",
      "Evaluated 1500/5384 pairs...\n",
      "Evaluated 1600/5384 pairs...\n",
      "Evaluated 1700/5384 pairs...\n",
      "Evaluated 1800/5384 pairs...\n",
      "Evaluated 1900/5384 pairs...\n",
      "Evaluated 2000/5384 pairs...\n",
      "Evaluated 2100/5384 pairs...\n",
      "Evaluated 2200/5384 pairs...\n",
      "Evaluated 2300/5384 pairs...\n",
      "Evaluated 2400/5384 pairs...\n",
      "Evaluated 2500/5384 pairs...\n",
      "Evaluated 2600/5384 pairs...\n",
      "Evaluated 2700/5384 pairs...\n",
      "Evaluated 2800/5384 pairs...\n",
      "Evaluated 2900/5384 pairs...\n",
      "Evaluated 3000/5384 pairs...\n",
      "Evaluated 3100/5384 pairs...\n",
      "Evaluated 3200/5384 pairs...\n",
      "Evaluated 3300/5384 pairs...\n",
      "Evaluated 3400/5384 pairs...\n",
      "Evaluated 3500/5384 pairs...\n",
      "Evaluated 3600/5384 pairs...\n",
      "Evaluated 3700/5384 pairs...\n",
      "Evaluated 3800/5384 pairs...\n",
      "Evaluated 3900/5384 pairs...\n",
      "Evaluated 4000/5384 pairs...\n",
      "Evaluated 4100/5384 pairs...\n",
      "Evaluated 4200/5384 pairs...\n",
      "Evaluated 4300/5384 pairs...\n",
      "Evaluated 4400/5384 pairs...\n",
      "Evaluated 4500/5384 pairs...\n",
      "Evaluated 4600/5384 pairs...\n",
      "Evaluated 4700/5384 pairs...\n",
      "Evaluated 4800/5384 pairs...\n",
      "Evaluated 4900/5384 pairs...\n",
      "Evaluated 5000/5384 pairs...\n",
      "Evaluated 5100/5384 pairs...\n",
      "Evaluated 5200/5384 pairs...\n",
      "Evaluated 5300/5384 pairs...\n",
      "Val Results:\n",
      "  PCK@0.05: 40.74%\n",
      "  PCK@0.10: 53.24%\n",
      "  PCK@0.20: 64.36%\n",
      "✓ Checkpoint saved: results_colab/dinov3/t_5_blocks_2_20260108_151250/epoch_1.pth\n",
      "\n",
      "============================================================\n",
      "TRAINING COMPLETED\n",
      "============================================================\n",
      "Results saved to: results_colab/dinov3/t_5_blocks_2_20260108_151250\n",
      "✓ Metadata saved: results_colab/dinov3/t_5_blocks_2_20260108_151250/metadata.json\n",
      "\n",
      "✓ Successfully copied results to Google Drive: /content/drive/MyDrive/Colab_dinov3_finetuning_temp_validation_results/t_5_blocks_2_20260108_151250\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import shutil # Added for copying to Google Drive\n",
    "from finetuning.simple_eval import simple_evaluate\n",
    "import json\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "from datetime import datetime\n",
    "import torch.nn.functional as F\n",
    "import sys\n",
    "# sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))\n",
    "\n",
    "from SPair71k.devkit.SPairDataset import SPairDataset\n",
    "from helper_functions import extract_dense_features, pixel_to_patch_coord, patch_to_pixel_coord\n",
    "from finetuning.simple_eval import simple_evaluate\n",
    "from matching_strategies import find_best_match_argmax\n",
    "from pck import compute_pck_spair71k\n",
    "# from models.dinov2.dinov2.models.vision_transformer import vit_base\n",
    "from models.dinov3.dinov3.models.vision_transformer import vit_base\n",
    "# from finetune_dinov2 import freeze_model, unfreeze_last_n_blocks, train_epoch\n",
    "\n",
    "def freeze_model(model):\n",
    "    \"\"\"Freeze all model parameters\"\"\"\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "\n",
    "def unfreeze_last_n_blocks(model, n_blocks):\n",
    "    \"\"\"\n",
    "    Unfreeze the last n_blocks transformer blocks + final norm layer\n",
    "\n",
    "    Args:\n",
    "        model: DINOv2 model\n",
    "        n_blocks: number of blocks to unfreeze (counting from the end)\n",
    "    \"\"\"\n",
    "    total_blocks = len(model.blocks)\n",
    "\n",
    "    # Unfreeze last n blocks\n",
    "    for i in range(total_blocks - n_blocks, total_blocks):\n",
    "        for param in model.blocks[i].parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "    # Also unfreeze the final normalization layer\n",
    "    for param in model.norm.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "    print(f\"Unfrozen last {n_blocks} blocks + norm layer\")\n",
    "\n",
    "\n",
    "def compute_cross_entropy_loss(src_features, tgt_features, src_kps, trg_kps,\n",
    "                               src_original_size, tgt_original_size, img_size, patch_size, temperature=10.0):\n",
    "    \"\"\"\n",
    "    Compute cross-entropy loss for semantic correspondence.\n",
    "    Treats correspondence as a classification problem where each target patch is a class.\n",
    "\n",
    "    Args:\n",
    "        src_features: [1, H, W, D] source dense features\n",
    "        tgt_features: [1, H, W, D] target dense features\n",
    "        src_kps: [N, 2] source keypoints in pixel coordinates\n",
    "        trg_kps: [N, 2] target keypoints in pixel coordinates\n",
    "        src_original_size: (width, height) of original source image\n",
    "        tgt_original_size: (width, height) of original target image\n",
    "        img_size: resizing size used during feature extraction\n",
    "        patch_size: size of each patch\n",
    "        temperature: softmax temperature (higher = more peaked distribution)\n",
    "\n",
    "    Returns:\n",
    "        loss: mean cross-entropy loss across all keypoints\n",
    "    \"\"\"\n",
    "    _, H, W, D = tgt_features.shape\n",
    "    tgt_flat = tgt_features.reshape(H * W, D)  # [H*W, D]\n",
    "\n",
    "    losses = []\n",
    "\n",
    "    for i in range(src_kps.shape[0]):\n",
    "        src_x, src_y = src_kps[i]\n",
    "        tgt_x, tgt_y = trg_kps[i]\n",
    "\n",
    "        # Get source feature at keypoint location\n",
    "        src_patch_x, src_patch_y = pixel_to_patch_coord(src_x, src_y, src_original_size, patch_size=patch_size, resized_size=img_size)\n",
    "        src_feature = src_features[0, src_patch_y, src_patch_x, :]  # [D]\n",
    "\n",
    "        # Get ground truth target patch coordinates\n",
    "        tgt_patch_x, tgt_patch_y = pixel_to_patch_coord(tgt_x, tgt_y, tgt_original_size, patch_size=patch_size, resized_size=img_size)\n",
    "        # Compute cosine similarities with all target patches\n",
    "        similarities = F.cosine_similarity(\n",
    "            src_feature.unsqueeze(0),  # [1, D]\n",
    "            tgt_flat,  # [H*W, D]\n",
    "            dim=1\n",
    "        )  # [H*W]\n",
    "\n",
    "        # Convert similarities to log-probabilities\n",
    "        log_probs = F.log_softmax(similarities * temperature, dim=0)\n",
    "\n",
    "        # Ground truth index (flatten 2D coordinates to 1D)\n",
    "        gt_idx = tgt_patch_y * W + tgt_patch_x\n",
    "\n",
    "        # Negative log-likelihood loss\n",
    "        loss = -log_probs[gt_idx]\n",
    "        losses.append(loss)\n",
    "\n",
    "    return torch.stack(losses).mean()\n",
    "\n",
    "def evaluate(model, dataset, device, img_size, patch_size, thresholds=[0.05, 0.1, 0.2]):\n",
    "    \"\"\"\n",
    "    Evaluate model on test set using PCK metric\n",
    "\n",
    "    Args:\n",
    "        model: DINOv2 model\n",
    "        dataset: test dataset\n",
    "        device: 'cuda' or 'cpu'\n",
    "        thresholds: list of PCK thresholds to evaluate\n",
    "\n",
    "    Returns:\n",
    "        results: dictionary with PCK scores at different thresholds\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    per_image_metrics = []\n",
    "\n",
    "    print(f\"Evaluating on {len(dataset)} image pairs...\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, sample in enumerate(dataset):\n",
    "            # Prepare images\n",
    "            src_tensor = sample['src_img'].unsqueeze(0).to(device)\n",
    "            tgt_tensor = sample['trg_img'].unsqueeze(0).to(device)\n",
    "\n",
    "            src_tensor = F.interpolate(src_tensor, size=(img_size, img_size), mode='bilinear', align_corners=False)\n",
    "            tgt_tensor = F.interpolate(tgt_tensor, size=(img_size, img_size), mode='bilinear', align_corners=False)\n",
    "\n",
    "            src_original_size = (sample['src_imsize'][2], sample['src_imsize'][1])\n",
    "            tgt_original_size = (sample['trg_imsize'][2], sample['trg_imsize'][1])\n",
    "\n",
    "            # Extract features\n",
    "            src_features = extract_dense_features(model, src_tensor)\n",
    "            tgt_features = extract_dense_features(model, tgt_tensor)\n",
    "\n",
    "            _, H, W, D = tgt_features.shape\n",
    "            tgt_flat = tgt_features.reshape(H * W, D)\n",
    "\n",
    "            # Get keypoints and bbox\n",
    "            src_kps = sample['src_kps'].numpy()\n",
    "            trg_kps = sample['trg_kps'].numpy()\n",
    "            trg_bbox = sample['trg_bbox']\n",
    "\n",
    "            # Predict matches for all keypoints\n",
    "            pred_matches = []\n",
    "\n",
    "            for i in range(src_kps.shape[0]):\n",
    "                src_x, src_y = src_kps[i]\n",
    "\n",
    "                # Get source feature\n",
    "                patch_x, patch_y = pixel_to_patch_coord(src_x, src_y, src_original_size, patch_size=patch_size, resized_size=img_size)\n",
    "                src_feature = src_features[0, patch_y, patch_x, :]\n",
    "\n",
    "                # Compute similarities\n",
    "                similarities = F.cosine_similarity(\n",
    "                    src_feature.unsqueeze(0),\n",
    "                    tgt_flat,\n",
    "                    dim=1\n",
    "                )\n",
    "\n",
    "                # Find best match using argmax\n",
    "                match_patch_x, match_patch_y = find_best_match_argmax(similarities, W)\n",
    "                match_x, match_y = patch_to_pixel_coord(\n",
    "                    match_patch_x, match_patch_y, tgt_original_size,\n",
    "                    patch_size=patch_size, resized_size=img_size\n",
    "                )\n",
    "\n",
    "                pred_matches.append([match_x, match_y])\n",
    "\n",
    "            # Compute PCK for different thresholds\n",
    "            image_pcks = {}\n",
    "            category = sample['category']\n",
    "\n",
    "            for threshold in thresholds:\n",
    "                pck, _, _ = compute_pck_spair71k(\n",
    "                    pred_matches,\n",
    "                    trg_kps.tolist(),\n",
    "                    trg_bbox,\n",
    "                    threshold\n",
    "                )\n",
    "                image_pcks[threshold] = pck\n",
    "                # category_metrics[category][threshold].append(pck)\n",
    "\n",
    "            # store per-image metrics\n",
    "            per_image_metrics.append({\n",
    "                'category': category,\n",
    "                'pck_scores': image_pcks,\n",
    "            })\n",
    "\n",
    "            if (idx + 1) % 100 == 0:\n",
    "                print(f\"Evaluated {idx + 1}/{len(dataset)} pairs...\")\n",
    "\n",
    "    # Compute overall statistics\n",
    "    results = {}\n",
    "    for threshold in thresholds:\n",
    "        all_pcks = [img['pck_scores'][threshold] for img in per_image_metrics]\n",
    "        results[f'pck@{threshold:.2f}'] = {\n",
    "            'mean': float(np.mean(all_pcks)),\n",
    "            'std': float(np.std(all_pcks)),\n",
    "            'median': float(np.median(all_pcks)),\n",
    "        }\n",
    "\n",
    "    return results, per_image_metrics\n",
    "\n",
    "\n",
    "def train_epoch(model, dataloader, optimizer, device, epoch, img_size, patch_size, temperature=10.0):\n",
    "    \"\"\"\n",
    "    Train for one epoch\n",
    "\n",
    "    Args:\n",
    "        model: DINOv3 model\n",
    "        dataloader: training data loader\n",
    "        optimizer: optimizer\n",
    "        device: 'cuda' or 'cpu'\n",
    "        epoch: current epoch number\n",
    "        img_size: size to which images are resized for feature extraction\n",
    "        patch_size: size of each patch\n",
    "        temperature: softmax temperature for loss\n",
    "\n",
    "    Returns:\n",
    "        avg_loss: average loss over the epoch\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    num_batches = 0\n",
    "\n",
    "    for idx, sample in enumerate(dataloader):\n",
    "        # Prepare data\n",
    "        src_tensor = sample['src_img'].to(device)  # [1, 3, H, W]\n",
    "        tgt_tensor = sample['trg_img'].to(device)  # [1, 3, H, W]\n",
    "\n",
    "        # Resize to 518x518 (DINOv2 expects this size)\n",
    "        src_tensor = F.interpolate(src_tensor, size=(img_size, img_size), mode='bilinear', align_corners=False)\n",
    "        tgt_tensor = F.interpolate(tgt_tensor, size=(img_size, img_size), mode='bilinear', align_corners=False)\n",
    "\n",
    "        # Store original sizes for coordinate conversion\n",
    "        src_original_size = (sample['src_imsize'][2], sample['src_imsize'][1])\n",
    "        tgt_original_size = (sample['trg_imsize'][2], sample['trg_imsize'][1])\n",
    "\n",
    "        # Get keypoints\n",
    "        src_kps = sample['src_kps'].numpy()[0]  # [N, 2]\n",
    "        trg_kps = sample['trg_kps'].numpy()[0]  # [N, 2]\n",
    "\n",
    "        # Extract dense features\n",
    "        src_features = extract_dense_features(model, src_tensor, training=True)\n",
    "        tgt_features = extract_dense_features(model, tgt_tensor, training=True)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = compute_cross_entropy_loss(\n",
    "            src_features, tgt_features,\n",
    "            src_kps, trg_kps,\n",
    "            src_original_size, tgt_original_size,\n",
    "            img_size, patch_size,\n",
    "            temperature=temperature\n",
    "        )\n",
    "\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        num_batches += 1\n",
    "\n",
    "        # Print progress\n",
    "        if (idx + 1) % 50 == 0:\n",
    "            print(f\"Epoch {epoch}, Batch {idx + 1}/{len(dataloader)}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "    avg_loss = total_loss / num_batches\n",
    "    return avg_loss\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main training and evaluation pipeline\"\"\"\n",
    "\n",
    "    # ========== CONFIGURATION ==========\n",
    "    n_blocks = 2  #to try: 1, 2, 3, 4\n",
    "    num_epochs = 1\n",
    "    learning_rate = 1e-4\n",
    "    batch_size = 1  #SPair-71k has variable-sized images\n",
    "    temperature = 15  #softmax temperature for cross-entropy loss\n",
    "    img_size = 512\n",
    "    patch_size = 16\n",
    "    weight_decay = 0.01\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Create results_SPair71K directory with timestamp\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    results_dir = f'results_colab/dinov3/t_{temperature}_blocks_{n_blocks}_{timestamp}'\n",
    "    # results_dir = f'results_SPair71K/dinov3_base_finetuned_{timestamp}'\n",
    "    os.makedirs(results_dir, exist_ok=True)\n",
    "    print(f\"Results will be saved to: {results_dir}\")\n",
    "\n",
    "    # ========== LOAD DATASETS ==========\n",
    "    print(\"\\nLoading SPair-71k dataset...\")\n",
    "    pair_ann_path = f'{base}/PairAnnotation'\n",
    "    layout_path = f'{base}/Layout'\n",
    "    image_path = f'{base}/JPEGImages'\n",
    "    dataset_size = 'large'\n",
    "    pck_alpha = 0.1 #mock, it's not used in evaluation\n",
    "\n",
    "    train_dataset = SPairDataset(\n",
    "        pair_ann_path,\n",
    "        layout_path,\n",
    "        image_path,\n",
    "        dataset_size,\n",
    "        pck_alpha,  # dummy pck_alpha, not used during training\n",
    "        datatype='trn'  # training split\n",
    "    )\n",
    "\n",
    "    val_dataset = SPairDataset(\n",
    "        pair_ann_path,\n",
    "        layout_path,\n",
    "        image_path,\n",
    "        dataset_size,\n",
    "        pck_alpha,\n",
    "        datatype='val'\n",
    "    )\n",
    "\n",
    "    print(f\"Training samples: {len(train_dataset)}\")\n",
    "    print(f\"Val samples: {len(val_dataset)}\")\n",
    "\n",
    "    # Create data loader\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=1,\n",
    "        pin_memory=True if device == 'cuda' else False\n",
    "    )\n",
    "\n",
    "    # for n_blocks in [1,2,3,4]:\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(f\"FINETUNING WITH LAST {n_blocks} BLOCKS UNFROZEN\")\n",
    "    print(\"=\" * 80)\n",
    "    # ========== LOAD MODEL ==========\n",
    "    print(\"\\nLoading DINOv3-base model...\")\n",
    "    model = vit_base(\n",
    "        img_size= (img_size, img_size),        # base / nominal size\n",
    "        patch_size=patch_size,             # patch size that matches the checkpoint\n",
    "        n_storage_tokens=4,\n",
    "        layerscale_init= 1.0,\n",
    "        mask_k_bias=True,\n",
    "    )\n",
    "\n",
    "    # load pretrained weights\n",
    "    ckpt = torch.load(\"models/dinov3/weights/dinov3_vitb16_pretrain.pth\", map_location=device)\n",
    "    model.load_state_dict(ckpt, strict=True)\n",
    "    model.to(device)\n",
    "\n",
    "    # freeze entire model, then unfreeze last N blocks\n",
    "    freeze_model(model)\n",
    "    unfreeze_last_n_blocks(model, n_blocks)\n",
    "\n",
    "    # count trainable parameters\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"\\nTrainable parameters: {trainable_params:,} / {total_params:,} \"\n",
    "            f\"({100 * trainable_params / total_params:.2f}%)\")\n",
    "\n",
    "\n",
    "    # ========== OPTIMIZER ==========\n",
    "    optimizer = optim.AdamW(\n",
    "        filter(lambda p: p.requires_grad, model.parameters()),\n",
    "        lr=learning_rate,\n",
    "        weight_decay=weight_decay\n",
    "    )\n",
    "\n",
    "    # Optional: Learning rate scheduler\n",
    "    # scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
    "\n",
    "    # ========== TRAINING LOOP ==========\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"STARTING TRAINING\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # best_pck = -1.0\n",
    "    # best_epoch = -1\n",
    "    training_history = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"\\n{'=' * 60}\")\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "        print('=' * 60)\n",
    "\n",
    "        # Train for one epoch\n",
    "        train_loss = train_epoch(\n",
    "            model, train_loader, optimizer, device, epoch + 1, img_size, patch_size, temperature=temperature\n",
    "        )\n",
    "        print(f\"\\nAverage training loss: {train_loss:.4f}\")\n",
    "\n",
    "        # Update learning rate\n",
    "        # scheduler.step()\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        print(f\"Learning rate: {current_lr:.6f}\")\n",
    "\n",
    "        # Validate on val set\n",
    "        print(\"\\nEvaluating on test set...\")\n",
    "        results_val, per_image_metrics = simple_evaluate(model, val_dataset, device, img_size, patch_size)\n",
    "\n",
    "        pck_005 = results_val['pck@0.05']['mean']\n",
    "        pck_010 = results_val['pck@0.10']['mean']\n",
    "        pck_020 = results_val['pck@0.20']['mean']\n",
    "\n",
    "        print(f\"Val Results:\")\n",
    "        print(f\"  PCK@0.05: {pck_005:.2f}%\")\n",
    "        print(f\"  PCK@0.10: {pck_010:.2f}%\")\n",
    "        print(f\"  PCK@0.20: {pck_020:.2f}%\")\n",
    "\n",
    "\n",
    "        # Save model checkpoint\n",
    "        # Save checkpoint for this epoch\n",
    "        ckpt_path = f'{results_dir}/epoch_{epoch + 1}.pth'\n",
    "        torch.save({\n",
    "            'epoch': epoch + 1,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'n_blocks': n_blocks,\n",
    "            'temperature': temperature,\n",
    "            'learning_rate': learning_rate,\n",
    "            'val_pck@0.05': pck_005,\n",
    "            'val_pck@0.10': pck_010,\n",
    "            'val_pck@0.20': pck_020,\n",
    "        }, ckpt_path)\n",
    "        print(f\"✓ Checkpoint saved: {ckpt_path}\")\n",
    "\n",
    "        # Track best by PCK@0.10\n",
    "        # if pck_010 > best_pck:\n",
    "        #     best_pck = pck_010\n",
    "        #     best_epoch = epoch + 1\n",
    "        #     best_ckpt_path = f'{results_dir}/best_model.pth'\n",
    "        #     torch.save({\n",
    "        #         'epoch': best_epoch,\n",
    "        #         'model_state_dict': model.state_dict(),\n",
    "        #         'optimizer_state_dict': optimizer.state_dict(),\n",
    "        #         'n_blocks': n_blocks,\n",
    "        #         'temperature': temperature,\n",
    "        #         'learning_rate': learning_rate,\n",
    "        #         'val_pck@0.05': pck_005,\n",
    "        #         'val_pck@0.10': pck_010,\n",
    "        #         'val_pck@0.20': pck_020,\n",
    "        #     }, best_ckpt_path)\n",
    "        #     print(f\"✓ Best model saved: {best_ckpt_path}\")\n",
    "\n",
    "        # Store training history\n",
    "        training_history.append({\n",
    "            'epoch': epoch + 1,\n",
    "            'train_loss': train_loss,\n",
    "            'learning_rate': current_lr,\n",
    "            'val_pck@0.05': pck_005,\n",
    "            'val_pck@0.10': pck_010,\n",
    "            'val_pck@0.20': pck_020,\n",
    "        })\n",
    "\n",
    "        # Save intermediate results_SPair71K\n",
    "        # with open(f'{results_dir}/training_history.json', 'w') as f:\n",
    "        #     json.dump(training_history, f, indent=2)\n",
    "\n",
    "        # ========== FINAL RESULTS ==========\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"TRAINING COMPLETED\")\n",
    "        print(\"=\" * 60)\n",
    "        # print(f\"Best PCK@0.1: {best_pck:.2f}% (Epoch {best_epoch})\")\n",
    "        print(f\"Results saved to: {results_dir}\")\n",
    "\n",
    "\n",
    "        # Save metadata for comparison\n",
    "        metadata = {\n",
    "            'n_blocks': n_blocks,\n",
    "            'temperature': temperature,\n",
    "            'learning_rate': learning_rate,\n",
    "            'num_epochs': num_epochs,\n",
    "            # 'best_epoch': best_epoch,\n",
    "            # 'best_pck@0.05': float(training_history[best_epoch - 1]['val_pck@0.05']),\n",
    "            # 'best_pck@0.10': float(best_pck),\n",
    "            # 'best_pck@0.20': float(training_history[best_epoch - 1]['val_pck@0.20']),\n",
    "            'pck@0.05': float(training_history[-1]['val_pck@0.05']),\n",
    "            'pck@0.10': float(training_history[-1]['val_pck@0.10']),\n",
    "            'pck@0.20': float(training_history[-1]['val_pck@0.20']),\n",
    "            'training_history': training_history,\n",
    "        }\n",
    "\n",
    "        with open(f'{results_dir}/metadata.json', 'w') as f:\n",
    "            json.dump(metadata, f, indent=2)\n",
    "        print(f\"✓ Metadata saved: {results_dir}/metadata.json\")\n",
    "\n",
    "    # Automatically copy results to Google Drive\n",
    "    drive_results_base_path = '/content/drive/MyDrive/Colab_dinov3_finetuning_temp_validation_results/'\n",
    "    drive_destination_path = os.path.join(drive_results_base_path, os.path.basename(results_dir))\n",
    "\n",
    "    try:\n",
    "        if not os.path.exists(drive_results_base_path):\n",
    "            os.makedirs(drive_results_base_path, exist_ok=True)\n",
    "        shutil.copytree(results_dir, drive_destination_path)\n",
    "        print(f\"\\n✓ Successfully copied results to Google Drive: {drive_destination_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\n✗ Error copying results to Google Drive: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VaVcequ-Vxt2"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Smonta il Drive\n",
    "drive.flush_and_unmount()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
