{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b2cb0f6d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b2cb0f6d",
        "outputId": "a3954524-f98a-4517-e668-048ab0cc9a39"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Monta Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9f01d26a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "9f01d26a",
        "outputId": "b386b437-ae79-4876-ccd6-9dbd038e3ebe"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/semantic_correspondence.zip'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "# Percorsi\n",
        "zip_su_drive = '/content/drive/MyDrive/semantic_correspondence.zip'\n",
        "zip_locale = '/content/semantic_correspondence.zip'\n",
        "cartella_destinazione = '/content/'\n",
        "\n",
        "# Copia lo zip in locale\n",
        "import shutil\n",
        "shutil.copy(zip_su_drive, zip_locale)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2aa7ad34",
      "metadata": {
        "id": "2aa7ad34"
      },
      "outputs": [],
      "source": [
        "# Estrai lo zip\n",
        "import zipfile, os\n",
        "os.makedirs(cartella_destinazione, exist_ok=True)\n",
        "with zipfile.ZipFile(zip_locale, 'r') as z:\n",
        "    z.extractall(cartella_destinazione)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "992bd44a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "992bd44a",
        "outputId": "ebcef5f0-7829-47de-d080-fc91feb73ec3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✓ GPU: Tesla T4\n"
          ]
        }
      ],
      "source": [
        "# 5. Verify GPU\n",
        "import torch\n",
        "print(f\"\\n✓ GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'No GPU'}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "142b448d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "142b448d",
        "outputId": "bee6a9ac-c23a-440c-c716-c3aec4a5fa20"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu Jan 15 16:34:44 2026       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   45C    P8             11W /   70W |       2MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a6a7bfb3",
      "metadata": {
        "id": "a6a7bfb3"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.chdir('/content/semantic_correspondence')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "28dae975",
      "metadata": {
        "id": "28dae975"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "\n",
        "\n",
        "def extract_dense_features(model, img_tensor, training=False):\n",
        "    \"\"\"Extract dense features from DINOv2 model given an input image tensor.\"\"\"\n",
        "    context = torch.no_grad() if not training else torch.enable_grad()\n",
        "\n",
        "    with context:\n",
        "        #get tokens\n",
        "        features_dict = model.forward_features(img_tensor)\n",
        "        patch_tokens = features_dict['x_norm_patchtokens']  # [B, N_patches, D]\n",
        "\n",
        "        #reshaping to dense feature map\n",
        "        B, N, D = patch_tokens.shape\n",
        "        H_patches = W_patches = int(N ** 0.5)  # per img 518x518 con patch 14: 37x37\n",
        "        dense_features = patch_tokens.reshape(B, H_patches, W_patches, D)\n",
        "    return dense_features\n",
        "\n",
        "\n",
        "def pixel_to_patch_coord(x, y, original_size, patch_size=14, resized_size=518):\n",
        "    \"\"\"convert pixel coordinates to patch coordinates\"\"\"\n",
        "    #scale to resized image\n",
        "    scale_x = resized_size / original_size[0]\n",
        "    scale_y = resized_size / original_size[1]\n",
        "    x_resized = x * scale_x\n",
        "    y_resized = y * scale_y\n",
        "\n",
        "    #compute patch coordinates\n",
        "    patch_x = int(x_resized // patch_size)\n",
        "    patch_y = int(y_resized // patch_size)\n",
        "\n",
        "    #clamp to valid range\n",
        "    max_patch = resized_size // patch_size - 1\n",
        "    patch_x = min(max(patch_x, 0), max_patch)\n",
        "    patch_y = min(max(patch_y, 0), max_patch)\n",
        "\n",
        "    return patch_x, patch_y\n",
        "\n",
        "\n",
        "def patch_to_pixel_coord(patch_x, patch_y, original_size, patch_size=14, resized_size=518):\n",
        "    \"\"\"Convert patch coordinates back to pixel coordinates with a centering strategy\"\"\"\n",
        "    #center of the patch in resized image\n",
        "    x_resized = patch_x * patch_size + patch_size / 2\n",
        "    y_resized = patch_y * patch_size + patch_size / 2\n",
        "\n",
        "    #scale back to original image size\n",
        "    scale_x = original_size[0] / resized_size\n",
        "    scale_y = original_size[1] / resized_size\n",
        "    x = x_resized * scale_x\n",
        "    y = y_resized * scale_y\n",
        "\n",
        "    return x, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "1a495cd3",
      "metadata": {
        "id": "1a495cd3"
      },
      "outputs": [],
      "source": [
        "\n",
        "base = '/content/semantic_correspondence/SPair71k'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/semantic_correspondence/models/segment_anything\n",
        "!pip install -e ."
      ],
      "metadata": {
        "id": "CTRNVFhQksEV",
        "outputId": "e2a8cb01-c4bd-4ecc-e21e-ba183c07ef11",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        }
      },
      "id": "CTRNVFhQksEV",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/semantic_correspondence/models/segment_anything\n",
            "Obtaining file:///content/semantic_correspondence/models/segment_anything\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Installing collected packages: segment_anything\n",
            "  Attempting uninstall: segment_anything\n",
            "    Found existing installation: segment_anything 1.0\n",
            "    Uninstalling segment_anything-1.0:\n",
            "      Successfully uninstalled segment_anything-1.0\n",
            "  Running setup.py develop for segment_anything\n",
            "Successfully installed segment_anything-1.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "segment_anything"
                ]
              },
              "id": "159031dad6b849e288abb802c74dd52e"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/semantic_correspondence/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WhJQRseolBb5",
        "outputId": "127a6aec-2a85-4837-e3c0-d388f97a9437"
      },
      "id": "WhJQRseolBb5",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/semantic_correspondence\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "12380214",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 842
        },
        "id": "12380214",
        "outputId": "610f29b0-8447-4a26-94e4-c63cc838bc45"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Results will be saved to: results_colab/SAM/lr_0.0001_t_5_blocks_2_20260115_170020\n",
            "\n",
            "Loading SPair-71k dataset...\n",
            "Training samples: 53340\n",
            "Val samples: 5384\n",
            "\n",
            "================================================================================\n",
            "FINETUNING WITH LAST 2 BLOCKS UNFROZEN\n",
            "================================================================================\n",
            "\n",
            "Loading SAM model...\n",
            "Unfrozen last 2 blocks of SAM image encoder. Final norm layer not found or accessible via 'neck.ln_final'.\n",
            "\n",
            "Trainable parameters: 14,195,456 / 93,735,472 (15.14%)\n",
            "\n",
            "============================================================\n",
            "STARTING TRAINING\n",
            "============================================================\n",
            "\n",
            "============================================================\n",
            "Epoch 1/1\n",
            "============================================================\n",
            "Epoch 1, Batch 51/53340, Loss: 5.6469\n",
            "\n",
            "Average training loss: 5.9605\n",
            "Learning rate: 0.000100\n",
            "\n",
            "Evaluating on test set...\n",
            "Evaluating on 5384 image pairs...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-77382915.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    411\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    412\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 413\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-77382915.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    308\u001b[0m         \u001b[0;31m# Validate on val set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nEvaluating on test set...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 310\u001b[0;31m         \u001b[0mresults_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mper_image_metrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msimple_evaluate_SAM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msam_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    311\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m         \u001b[0mpck_005\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresults_val\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'pck@0.05'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'mean'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/semantic_correspondence/finetuning/simple_eval.py\u001b[0m in \u001b[0;36msimple_evaluate_SAM\u001b[0;34m(model, dataset, device, img_size, patch_size, thresholds)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m             \u001b[0;31m# Extract features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m             \u001b[0msrc_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_dense_features_SAM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimg_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m             \u001b[0mtgt_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_dense_features_SAM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimg_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/semantic_correspondence/helper_functions.py\u001b[0m in \u001b[0;36mextract_dense_features_SAM\u001b[0;34m(model, img_tensor, training, image_size)\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mautocast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m                 \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_encoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_resized\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;31m# restore original positional embeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/semantic_correspondence/models/segment_anything/segment_anything/modeling/image_encoder.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mblk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mblk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mneck\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/semantic_correspondence/models/segment_anything/segment_anything/modeling/image_encoder.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    172\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpad_hw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwindow_partition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m         \u001b[0;31m# Reverse window partition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow_size\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/semantic_correspondence/models/segment_anything/segment_anything/modeling/image_encoder.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_rel_pos\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m             \u001b[0mattn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madd_decomposed_rel_pos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrel_pos_h\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrel_pos_w\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m         \u001b[0mattn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/semantic_correspondence/models/segment_anything/segment_anything/modeling/image_encoder.py\u001b[0m in \u001b[0;36madd_decomposed_rel_pos\u001b[0;34m(attn, q, rel_pos_h, rel_pos_w, q_size, k_size)\u001b[0m\n\u001b[1;32m    352\u001b[0m     \u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m     \u001b[0mr_q\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq_h\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq_w\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m     \u001b[0mrel_h\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meinsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"bhwc,hkc->bhwk\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr_q\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    355\u001b[0m     \u001b[0mrel_w\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meinsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"bhwc,wkc->bhwk\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr_q\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/functional.py\u001b[0m in \u001b[0;36meinsum\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    371\u001b[0m         \u001b[0;31m# the path for contracting 0 or 1 time(s) is already optimized\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    372\u001b[0m         \u001b[0;31m# or the user has disabled using opt_einsum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 373\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_VF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meinsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mequation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperands\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    374\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m     \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import shutil # Added for copying to Google Drive\n",
        "from finetuning.simple_eval import simple_evaluate_SAM\n",
        "import json\n",
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "import os\n",
        "from datetime import datetime\n",
        "import torch.nn.functional as F\n",
        "import sys\n",
        "import time\n",
        "\n",
        "# Add the parent directory of the 'segment_anything' package to sys.path\n",
        "# sys.path.append(os.path.abspath('/content/semantic_correspondence/models')) # Removed this line as it was not needed for the original error\n",
        "\n",
        "from SPair71k.devkit.SPairDataset import SPairDataset\n",
        "from helper_functions import extract_dense_features_SAM, pixel_to_patch_coord, patch_to_pixel_coord\n",
        "from finetuning.simple_eval import simple_evaluate_SAM\n",
        "from matching_strategies import find_best_match_argmax\n",
        "from pck import compute_pck_spair71k\n",
        "from models.segment_anything.segment_anything import SamPredictor, sam_model_registry # Reverted to original import path\n",
        "\n",
        "def freeze_model(model):\n",
        "    \"\"\"Freeze all model parameters\"\"\"\n",
        "    for param in model.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "\n",
        "def unfreeze_last_n_blocks(model, n_blocks):\n",
        "    \"\"\"\n",
        "    Unfreeze the last n_blocks transformer blocks + final norm layer of the SAM image encoder.\n",
        "\n",
        "    Args:\n",
        "        model: SAM model\n",
        "        n_blocks: number of blocks to unfreeze (counting from the end)\n",
        "    \"\"\"\n",
        "    # Access the image encoder part of the SAM model\n",
        "    image_encoder = model.image_encoder\n",
        "\n",
        "    total_blocks = len(image_encoder.blocks)\n",
        "\n",
        "    # Unfreeze last n blocks\n",
        "    for i in range(total_blocks - n_blocks, total_blocks):\n",
        "        for param in image_encoder.blocks[i].parameters():\n",
        "            param.requires_grad = True\n",
        "\n",
        "    # Also unfreeze the final normalization layer\n",
        "    # For SAM's ViT, this is typically model.image_encoder.neck.ln_final\n",
        "    if hasattr(image_encoder, 'neck') and hasattr(image_encoder.neck, 'ln_final'):\n",
        "        for param in image_encoder.neck.ln_final.parameters():\n",
        "            param.requires_grad = True\n",
        "        print(f\"Unfrozen last {n_blocks} blocks + final norm layer of SAM image encoder\")\n",
        "    else:\n",
        "        print(f\"Unfrozen last {n_blocks} blocks of SAM image encoder. Final norm layer not found or accessible via 'neck.ln_final'.\")\n",
        "\n",
        "\n",
        "def compute_cross_entropy_loss(src_features, tgt_features, src_kps, trg_kps,\n",
        "                               src_original_size, tgt_original_size, img_size, patch_size, temperature=10.0):\n",
        "    \"\"\"\n",
        "    Compute cross-entropy loss for semantic correspondence.\n",
        "    Treats correspondence as a classification problem where each target patch is a class.\n",
        "\n",
        "    Args:\n",
        "        src_features: [1, H, W, D] source dense features\n",
        "        tgt_features: [1, H, W, D] target dense features\n",
        "        src_kps: [N, 2] source keypoints in pixel coordinates\n",
        "        trg_kps: [N, 2] target keypoints in pixel coordinates\n",
        "        src_original_size: (width, height) of original source image\n",
        "        tgt_original_size: (width, height) of original target image\n",
        "        img_size: resizing size used during feature extraction\n",
        "        patch_size: size of each patch\n",
        "        temperature: softmax temperature (higher = more peaked distribution)\n",
        "\n",
        "    Returns:\n",
        "        loss: mean cross-entropy loss across all keypoints\n",
        "    \"\"\"\n",
        "    _, H, W, D = tgt_features.shape\n",
        "    tgt_flat = tgt_features.reshape(H * W, D)  # [H*W, D]\n",
        "\n",
        "    losses = []\n",
        "\n",
        "    for i in range(src_kps.shape[0]):\n",
        "        src_x, src_y = src_kps[i]\n",
        "        tgt_x, tgt_y = trg_kps[i]\n",
        "\n",
        "        # Get source feature at keypoint location\n",
        "        src_patch_x, src_patch_y = pixel_to_patch_coord(src_x, src_y, src_original_size, patch_size=patch_size, resized_size=img_size)\n",
        "        src_feature = src_features[0, src_patch_y, src_patch_x, :]  # [D]\n",
        "\n",
        "        # Get ground truth target patch coordinates\n",
        "        tgt_patch_x, tgt_patch_y = pixel_to_patch_coord(tgt_x, tgt_y, tgt_original_size, patch_size=patch_size, resized_size=img_size)\n",
        "        # Compute cosine similarities with all target patches\n",
        "        similarities = F.cosine_similarity(\n",
        "            src_feature.unsqueeze(0),  # [1, D]\n",
        "            tgt_flat,  # [H*W, D]\n",
        "            dim=1\n",
        "        )  # [H*W]\n",
        "\n",
        "        # Convert similarities to log-probabilities\n",
        "        log_probs = F.log_softmax(similarities * temperature, dim=0)\n",
        "\n",
        "        # Ground truth index (flatten 2D coordinates to 1D)\n",
        "        gt_idx = tgt_patch_y * W + tgt_patch_x\n",
        "\n",
        "        # Negative log-likelihood loss\n",
        "        loss = -log_probs[gt_idx]\n",
        "        losses.append(loss)\n",
        "\n",
        "    return torch.stack(losses).mean()\n",
        "\n",
        "\n",
        "def train_epoch(model, dataloader, optimizer, device, epoch, img_size, patch_size, temperature=10.0):\n",
        "    \"\"\"\n",
        "    Train for one epoch\n",
        "\n",
        "    Args:\n",
        "        model: SAM model\n",
        "        dataloader: training data loader\n",
        "        optimizer: optimizer\n",
        "        device: 'cuda' or 'cpu'\n",
        "        epoch: current epoch number\n",
        "        img_size: size to which images are resized for feature extraction\n",
        "        patch_size: size of each patch\n",
        "        temperature: softmax temperature for loss\n",
        "\n",
        "    Returns:\n",
        "        avg_loss: average loss over the epoch\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    num_batches = 0\n",
        "\n",
        "    for idx, sample in enumerate(dataloader):\n",
        "        # Prepare data\n",
        "        src_tensor = sample['src_img'].to(device)  # [1, 3, H, W]\n",
        "        tgt_tensor = sample['trg_img'].to(device)  # [1, 3, H, W]\n",
        "\n",
        "        # Resize to 518x518 (DINOv2 expects this size)\n",
        "        src_tensor = F.interpolate(src_tensor, size=(img_size, img_size), mode='bilinear', align_corners=False)\n",
        "        tgt_tensor = F.interpolate(tgt_tensor, size=(img_size, img_size), mode='bilinear', align_corners=False)\n",
        "\n",
        "        #match SAM half precision\n",
        "        # src_tensor = src_tensor.half() # Removed .half()\n",
        "        # tgt_tensor = tgt_tensor.half() # Removed .half()\n",
        "\n",
        "        # Store original sizes for coordinate conversion\n",
        "        src_original_size = (sample['src_imsize'][2], sample['src_imsize'][1])\n",
        "        tgt_original_size = (sample['trg_imsize'][2], sample['trg_imsize'][1])\n",
        "\n",
        "        # Get keypoints\n",
        "        src_kps = sample['src_kps'].numpy()[0]  # [N, 2]\n",
        "        trg_kps = sample['trg_kps'].numpy()[0]  # [N, 2]\n",
        "\n",
        "        # Extract dense features\n",
        "        src_features = extract_dense_features_SAM(model, src_tensor, image_size=img_size, training=True)\n",
        "        tgt_features = extract_dense_features_SAM(model, tgt_tensor, image_size=img_size, training=True)\n",
        "\n",
        "        # Compute loss\n",
        "        loss = compute_cross_entropy_loss(\n",
        "            src_features, tgt_features,\n",
        "            src_kps, trg_kps,\n",
        "            src_original_size, tgt_original_size,\n",
        "            img_size, patch_size,\n",
        "            temperature=temperature\n",
        "        )\n",
        "\n",
        "        # Backward pass\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        num_batches += 1\n",
        "\n",
        "        if idx == 50:\n",
        "            print(f\"Epoch {epoch}, Batch {idx + 1}/{len(dataloader)}, Loss: {loss.item():.4f}\")\n",
        "            break  # DEBUG: limit to 50 batches per epoch\n",
        "\n",
        "        # Print progress\n",
        "        if (idx + 1) % 100 == 0:\n",
        "            print(f\"Epoch {epoch}, Batch {idx + 1}/{len(dataloader)}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "    avg_loss = total_loss / num_batches\n",
        "    return avg_loss\n",
        "\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main training and evaluation pipeline\"\"\"\n",
        "\n",
        "    # ========== CONFIGURATION ==========\n",
        "    n_blocks = 2  #to try: 1, 2, 3, 4\n",
        "    num_epochs = 1\n",
        "    learning_rate = 1e-4\n",
        "    batch_size = 1  #SPair-71k has variable-sized images\n",
        "    temperature = 5  #softmax temperature for cross-entropy loss try 5,10,15\n",
        "    img_size = 512\n",
        "    patch_size = 16\n",
        "    weight_decay = 0.01\n",
        "\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # Create results_SPair71k directory with timestamp\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    results_dir = f'results_colab/SAM/lr_{learning_rate}_t_{temperature}_blocks_{n_blocks}_{timestamp}'\n",
        "    # results_dir = f'results_SPair71k/dinov3_base_finetuned_{timestamp}'\n",
        "    os.makedirs(results_dir, exist_ok=True)\n",
        "    print(f\"Results will be saved to: {results_dir}\")\n",
        "\n",
        "    # ========== LOAD DATASETS ==========\n",
        "    print(\"\\nLoading SPair-71k dataset...\")\n",
        "    pair_ann_path = f'{base}/PairAnnotation'\n",
        "    layout_path = f'{base}/Layout'\n",
        "    image_path = f'{base}/JPEGImages'\n",
        "    dataset_size = 'large'\n",
        "    pck_alpha = 0.1 #mock, it's not used in evaluation\n",
        "\n",
        "    train_dataset = SPairDataset(\n",
        "        pair_ann_path,\n",
        "        layout_path,\n",
        "        image_path,\n",
        "        dataset_size,\n",
        "        pck_alpha,  # dummy pck_alpha, not used during training\n",
        "        datatype='trn'  # training split\n",
        "    )\n",
        "\n",
        "    val_dataset = SPairDataset(\n",
        "        pair_ann_path,\n",
        "        layout_path,\n",
        "        image_path,\n",
        "        dataset_size,\n",
        "        pck_alpha,\n",
        "        datatype='val'\n",
        "    )\n",
        "\n",
        "    print(f\"Training samples: {len(train_dataset)}\")\n",
        "    print(f\"Val samples: {len(val_dataset)}\")\n",
        "\n",
        "    # Create data loader\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=1,\n",
        "        pin_memory=True if device == 'cuda' else False\n",
        "    )\n",
        "\n",
        "    # for n_blocks in [1,2,3,4]:\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(f\"FINETUNING WITH LAST {n_blocks} BLOCKS UNFROZEN\")\n",
        "    print(\"=\" * 80)\n",
        "    # ========== LOAD MODEL ==========\n",
        "    print(\"\\nLoading SAM model...\")\n",
        "    model_type = \"vit_b\"\n",
        "    checkpoint_path = \"models/segment_anything/weights/sam_vit_b_01ec64.pth\"\n",
        "    sam_model = sam_model_registry[model_type](checkpoint=checkpoint_path)\n",
        "    sam_model.to(device)\n",
        "    # sam_model.half() # Removed .half()\n",
        "\n",
        "    # freeze entire model, then unfreeze last N blocks\n",
        "    freeze_model(sam_model)\n",
        "    unfreeze_last_n_blocks(sam_model, n_blocks)\n",
        "\n",
        "    # count trainable parameters\n",
        "    trainable_params = sum(p.numel() for p in sam_model.parameters() if p.requires_grad)\n",
        "    total_params = sum(p.numel() for p in sam_model.parameters())\n",
        "    print(f\"\\nTrainable parameters: {trainable_params:,} / {total_params:,} \"\n",
        "            f\"({100 * trainable_params / total_params:.2f}%)\")\n",
        "\n",
        "\n",
        "    # ========== OPTIMIZER ==========\n",
        "    optimizer = optim.AdamW(\n",
        "        filter(lambda p: p.requires_grad, sam_model.parameters()),\n",
        "        lr=learning_rate,\n",
        "        weight_decay=weight_decay\n",
        "    )\n",
        "\n",
        "    # Optional: Learning rate scheduler\n",
        "    # scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
        "\n",
        "    # ========== TRAINING LOOP ==========\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"STARTING TRAINING\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # best_pck = -1.0\n",
        "    # best_epoch = -1\n",
        "    training_history = []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f\"\\n{'=' * 60}\")\n",
        "        print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
        "        print('=' * 60)\n",
        "\n",
        "        # Train for one epoch\n",
        "        train_loss = train_epoch(\n",
        "            sam_model, train_loader, optimizer, device, epoch + 1, img_size, patch_size, temperature=temperature\n",
        "        )\n",
        "        print(f\"\\nAverage training loss: {train_loss:.4f}\")\n",
        "\n",
        "        # Update learning rate\n",
        "        # scheduler.step()\n",
        "        current_lr = optimizer.param_groups[0]['lr']\n",
        "        print(f\"Learning rate: {current_lr:.6f}\")\n",
        "\n",
        "        # Validate on val set\n",
        "        print(\"\\nEvaluating on test set...\")\n",
        "        results_val, per_image_metrics = simple_evaluate_SAM(sam_model, val_dataset, device, img_size, patch_size)\n",
        "\n",
        "        pck_005 = results_val['pck@0.05']['mean']\n",
        "        pck_010 = results_val['pck@0.10']['mean']\n",
        "        pck_020 = results_val['pck@0.20']['mean']\n",
        "\n",
        "        print(f\"Val Results:\")\n",
        "        print(f\"  PCK@0.05: {pck_005:.2f}%\")\n",
        "        print(f\"  PCK@0.10: {pck_010:.2f}%\")\n",
        "        print(f\"  PCK@0.20: {pck_020:.2f}%\")\n",
        "\n",
        "\n",
        "        # Save model checkpoint\n",
        "        # Save checkpoint for this epoch\n",
        "        ckpt_path = f'{results_dir}/epoch_{epoch + 1}.pth'\n",
        "        torch.save({\n",
        "            'epoch': epoch + 1,\n",
        "            'model_state_dict': sam_model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'n_blocks': n_blocks,\n",
        "            'temperature': temperature,\n",
        "            'learning_rate': learning_rate,\n",
        "            'val_pck@0.05': pck_005,\n",
        "            'val_pck@0.10': pck_010,\n",
        "            'val_pck@0.20': pck_020,\n",
        "        }, ckpt_path)\n",
        "        print(f\"\\u2713 Checkpoint saved: {ckpt_path}\")\n",
        "\n",
        "        # Track best by PCK@0.10\n",
        "        # if pck_010 > best_pck:\n",
        "        #     best_pck = pck_010\n",
        "        #     best_epoch = epoch + 1\n",
        "        #     best_ckpt_path = f'{results_dir}/best_model.pth'\n",
        "        #     torch.save({\n",
        "        #         'epoch': best_epoch,\n",
        "        #         'model_state_dict': model.state_dict(),\n",
        "        #         'optimizer_state_dict': optimizer.state_dict(),\n",
        "        #         'n_blocks': n_blocks,\n",
        "        #         'temperature': temperature,\n",
        "        #         'learning_rate': learning_rate,\n",
        "        #         'val_pck@0.05': pck_005,\n",
        "        #         'val_pck@0.10': pck_010,\n",
        "        #         'val_pck@0.20': pck_020,\n",
        "        #     }, best_ckpt_path)\n",
        "        #     print(f\"\\u2713 Best model saved: {best_ckpt_path}\")\n",
        "\n",
        "        # Store training history\n",
        "        training_history.append({\n",
        "            'epoch': epoch + 1,\n",
        "            'train_loss': train_loss,\n",
        "            'learning_rate': current_lr,\n",
        "            'val_pck@0.05': pck_005,\n",
        "            'val_pck@0.10': pck_010,\n",
        "            'val_pck@0.20': pck_020,\n",
        "        })\n",
        "\n",
        "        # Save intermediate results_SPair71k\n",
        "        # with open(f'{results_dir}/training_history.json', 'w') as f:\n",
        "        #     json.dump(training_history, f, indent=2)\n",
        "\n",
        "        # ========== FINAL RESULTS ==========\n",
        "        print(\"\\n\" + \"=\" * 60)\n",
        "        print(\"TRAINING COMPLETED\")\n",
        "        print(\"=\" * 60)\n",
        "        # print(f\"Best PCK@0.1: {best_pck:.2f}% (Epoch {best_epoch})\")\n",
        "        print(f\"Results saved to: {results_dir}\")\n",
        "\n",
        "\n",
        "        # Save metadata for comparison\n",
        "        metadata = {\n",
        "            'n_blocks': n_blocks,\n",
        "            'temperature': temperature,\n",
        "            'learning_rate': learning_rate,\n",
        "            'num_epochs': num_epochs,\n",
        "            # 'best_epoch': best_epoch,\n",
        "            # 'best_pck@0.05': float(training_history[best_epoch - 1]['val_pck@0.05']),\n",
        "            # 'best_pck@0.10': float(best_pck),\n",
        "            # 'best_pck@0.20': float(training_history[best_epoch - 1]['val_pck@0.20']),\n",
        "            'pck@0.05': float(training_history[-1]['val_pck@0.05']),\n",
        "            'pck@0.10': float(training_history[-1]['val_pck@0.10']),\n",
        "            'pck@0.20': float(training_history[-1]['val_pck@0.20']),\n",
        "            'training_history': training_history,\n",
        "        }\n",
        "\n",
        "        with open(f'{results_dir}/metadata.json', 'w') as f:\n",
        "            json.dump(metadata, f, indent=2)\n",
        "        print(f\"\\u2713 Metadata saved: {results_dir}/metadata.json\")\n",
        "\n",
        "    # Automatically copy results to Google Drive\n",
        "    drive_results_base_path = '/content/drive/MyDrive/Colab_dinov3_finetuning_temp_validation_results_prova/'\n",
        "    drive_destination_path = os.path.join(drive_results_base_path, os.path.basename(results_dir))\n",
        "\n",
        "    try:\n",
        "        if not os.path.exists(drive_results_base_path):\n",
        "            os.makedirs(drive_results_base_path, exist_ok=True)\n",
        "        shutil.copytree(results_dir, drive_destination_path)\n",
        "        print(f\"\\n\\u2713 Successfully copied results to Google Drive: {drive_destination_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"\\n\\u2717 Error copying results to Google Drive: {e}\")\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c5c2f11d",
      "metadata": {
        "id": "c5c2f11d"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Smonta il Drive\n",
        "drive.flush_and_unmount()"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}