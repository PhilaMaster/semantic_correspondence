{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "# Monta Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RZX8dGTK_xIO",
    "outputId": "ce45de16-f8b7-415b-9846-51a09e241947"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Percorsi\n",
    "zip_su_drive = '/content/drive/MyDrive/semantic_correspondence.zip'\n",
    "zip_locale = '/content/semantic_correspondence.zip'\n",
    "cartella_destinazione = '/content/'\n",
    "\n",
    "# Copia lo zip in locale\n",
    "import shutil\n",
    "shutil.copy(zip_su_drive, zip_locale)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "wdRMIwKP_4VQ",
    "outputId": "f18c566f-61f7-4347-db7f-b65364bb6338"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'/content/semantic_correspondence.zip'"
      ],
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      }
     },
     "metadata": {},
     "execution_count": 2
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Smonta il Drive\n",
    "drive.flush_and_unmount()"
   ],
   "metadata": {
    "id": "5j4cRKkF_6kB"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Estrai lo zip\n",
    "import zipfile, os\n",
    "os.makedirs(cartella_destinazione, exist_ok=True)\n",
    "with zipfile.ZipFile(zip_locale, 'r') as z:\n",
    "    z.extractall(cartella_destinazione)\n"
   ],
   "metadata": {
    "id": "4spGp1KM_75c"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# 5. Verify GPU\n",
    "import torch\n",
    "print(f\"\\n✓ GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'No GPU'}\")\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "emjurSt8AE5P",
    "outputId": "162171cc-fd79-4f08-c177-9d9690f95f26"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "✓ GPU: Tesla T4\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "!nvidia-smi"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MdRqeiOWKm_0",
    "outputId": "7d8b9cfc-a829-41c6-a786-efebd07e0aa8"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Sun Dec 21 18:27:31 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
      "| N/A   54C    P8             12W /   70W |       2MiB /  15360MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "os.chdir('/content/semantic_correspondence')"
   ],
   "metadata": {
    "id": "awU1xZU_BkTO"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "import glob\n",
    "import json\n",
    "import os\n",
    "\n",
    "\n",
    "class Normalize(object):\n",
    "    def __init__(self, image_keys):\n",
    "        self.image_keys = image_keys\n",
    "        self.normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "\n",
    "    def __call__(self, image):\n",
    "        for key in self.image_keys:\n",
    "            image[key] /= 255.0\n",
    "            image[key] = self.normalize(image[key])\n",
    "        return image\n",
    "\n",
    "\n",
    "def read_img(path):\n",
    "    img = np.array(Image.open(path).convert('RGB'))\n",
    "\n",
    "    return torch.tensor(img.transpose(2, 0, 1).astype(np.float32))\n",
    "\n",
    "\n",
    "class SPairDataset(Dataset):\n",
    "    def __init__(self, pair_ann_path, layout_path, image_path, dataset_size, pck_alpha, datatype):\n",
    "\n",
    "        self.datatype = datatype\n",
    "        self.pck_alpha = pck_alpha\n",
    "        self.ann_files = open(os.path.join(layout_path, dataset_size, datatype + '.txt'), \"r\").read().split('\\n')\n",
    "        self.ann_files = self.ann_files[:len(self.ann_files) - 1]\n",
    "        self.pair_ann_path = pair_ann_path\n",
    "        self.image_path = image_path\n",
    "        self.categories = list(map(lambda x: os.path.basename(x), glob.glob('%s/*' % image_path)))\n",
    "        self.categories.sort()\n",
    "        self.transform = Normalize(['src_img', 'trg_img'])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ann_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # get pre-processed images\n",
    "        ann_file = self.ann_files[idx] + '.json'\n",
    "        with open(os.path.join(self.pair_ann_path, self.datatype, ann_file)) as f:\n",
    "            annotation = json.load(f)\n",
    "\n",
    "        category = annotation['category']\n",
    "        src_img = read_img(os.path.join(self.image_path, category, annotation['src_imname']))\n",
    "        trg_img = read_img(os.path.join(self.image_path, category, annotation['trg_imname']))\n",
    "\n",
    "        trg_bbox = annotation['trg_bndbox']\n",
    "        pck_threshold = max(trg_bbox[2] - trg_bbox[0],  trg_bbox[3] - trg_bbox[1]) * self.pck_alpha\n",
    "\n",
    "        sample = {'pair_id': annotation['pair_id'],\n",
    "                  'filename': annotation['filename'],\n",
    "                  'src_imname': annotation['src_imname'],\n",
    "                  'trg_imname': annotation['trg_imname'],\n",
    "                  'src_imsize': src_img.size(),\n",
    "                  'trg_imsize': trg_img.size(),\n",
    "\n",
    "                  'src_bbox': annotation['src_bndbox'],\n",
    "                  'trg_bbox': annotation['trg_bndbox'],\n",
    "                  'category': annotation['category'],\n",
    "\n",
    "                  'src_pose': annotation['src_pose'],\n",
    "                  'trg_pose': annotation['trg_pose'],\n",
    "\n",
    "                  'src_img': src_img,\n",
    "                  'trg_img': trg_img,\n",
    "                  'src_kps': torch.tensor(annotation['src_kps']).float(),\n",
    "                  'trg_kps': torch.tensor(annotation['trg_kps']).float(),\n",
    "                  'kps_ids': annotation['kps_ids'],\n",
    "\n",
    "                  'mirror': annotation['mirror'],\n",
    "                  'vp_var': annotation['viewpoint_variation'],\n",
    "                  'sc_var': annotation['scale_variation'],\n",
    "                  'truncn': annotation['truncation'],\n",
    "                  'occlsn': annotation['occlusion'],\n",
    "\n",
    "                  'pck_threshold': pck_threshold}\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample\n"
   ],
   "metadata": {
    "id": "R08LhcRyBTXY"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "def extract_dense_features(model, img_tensor, training=False):\n",
    "    \"\"\"Extract dense features from DINOv2 model given an input image tensor.\"\"\"\n",
    "    context = torch.no_grad() if not training else torch.enable_grad()\n",
    "\n",
    "    with context:\n",
    "        #get tokens\n",
    "        features_dict = model.forward_features(img_tensor)\n",
    "        patch_tokens = features_dict['x_norm_patchtokens']  # [B, N_patches, D]\n",
    "\n",
    "        #reshaping to dense feature map\n",
    "        B, N, D = patch_tokens.shape\n",
    "        H_patches = W_patches = int(N ** 0.5)  # per img 518x518 con patch 14: 37x37\n",
    "        dense_features = patch_tokens.reshape(B, H_patches, W_patches, D)\n",
    "    return dense_features\n",
    "\n",
    "\n",
    "def pixel_to_patch_coord(x, y, original_size, patch_size=14, resized_size=518):\n",
    "    \"\"\"convert pixel coordinates to patch coordinates\"\"\"\n",
    "    #scale to resized image\n",
    "    scale_x = resized_size / original_size[0]\n",
    "    scale_y = resized_size / original_size[1]\n",
    "    x_resized = x * scale_x\n",
    "    y_resized = y * scale_y\n",
    "\n",
    "    #compute patch coordinates\n",
    "    patch_x = int(x_resized // patch_size)\n",
    "    patch_y = int(y_resized // patch_size)\n",
    "\n",
    "    #clamp to valid range\n",
    "    max_patch = resized_size // patch_size - 1\n",
    "    patch_x = min(max(patch_x, 0), max_patch)\n",
    "    patch_y = min(max(patch_y, 0), max_patch)\n",
    "\n",
    "    return patch_x, patch_y\n",
    "\n",
    "\n",
    "def patch_to_pixel_coord(patch_x, patch_y, original_size, patch_size=14, resized_size=518):\n",
    "    \"\"\"Convert patch coordinates back to pixel coordinates with a centering strategy\"\"\"\n",
    "    #center of the patch in resized image\n",
    "    x_resized = patch_x * patch_size + patch_size / 2\n",
    "    y_resized = patch_y * patch_size + patch_size / 2\n",
    "\n",
    "    #scale back to original image size\n",
    "    scale_x = original_size[0] / resized_size\n",
    "    scale_y = original_size[1] / resized_size\n",
    "    x = x_resized * scale_x\n",
    "    y = y_resized * scale_y\n",
    "\n",
    "    return x, y"
   ],
   "metadata": {
    "id": "jTLJNjrBCfaM"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Copyright (c) Meta Platforms, Inc. and affiliates.\n",
    "#\n",
    "# This source code is licensed under the Apache License, Version 2.0\n",
    "# found in the LICENSE file in the root directory of this source tree.\n",
    "\n",
    "# References:\n",
    "#   https://github.com/facebookresearch/dino/blob/main/vision_transformer.py\n",
    "#   https://github.com/rwightman/pytorch-image-models/tree/master/timm/models/vision_transformer.py\n",
    "\n",
    "from functools import partial\n",
    "import math\n",
    "import logging\n",
    "from typing import Sequence, Tuple, Union, Callable\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.checkpoint\n",
    "from torch.nn.init import trunc_normal_\n",
    "\n",
    "from models.dinov2.dinov2.layers import Mlp, PatchEmbed, SwiGLUFFNFused, MemEffAttention, NestedTensorBlock as Block\n",
    "\n",
    "\n",
    "logger = logging.getLogger(\"dinov2\")\n",
    "\n",
    "\n",
    "def named_apply(fn: Callable, module: nn.Module, name=\"\", depth_first=True, include_root=False) -> nn.Module:\n",
    "    if not depth_first and include_root:\n",
    "        fn(module=module, name=name)\n",
    "    for child_name, child_module in module.named_children():\n",
    "        child_name = \".\".join((name, child_name)) if name else child_name\n",
    "        named_apply(fn=fn, module=child_module, name=child_name, depth_first=depth_first, include_root=True)\n",
    "    if depth_first and include_root:\n",
    "        fn(module=module, name=name)\n",
    "    return module\n",
    "\n",
    "\n",
    "class BlockChunk(nn.ModuleList):\n",
    "    def forward(self, x):\n",
    "        for b in self:\n",
    "            x = b(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class DinoVisionTransformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        img_size=224,\n",
    "        patch_size=16,\n",
    "        in_chans=3,\n",
    "        embed_dim=768,\n",
    "        depth=12,\n",
    "        num_heads=12,\n",
    "        mlp_ratio=4.0,\n",
    "        qkv_bias=True,\n",
    "        ffn_bias=True,\n",
    "        proj_bias=True,\n",
    "        drop_path_rate=0.0,\n",
    "        drop_path_uniform=False,\n",
    "        init_values=None,  # for layerscale: None or 0 => no layerscale\n",
    "        embed_layer=PatchEmbed,\n",
    "        act_layer=nn.GELU,\n",
    "        block_fn=Block,\n",
    "        ffn_layer=\"mlp\",\n",
    "        block_chunks=1,\n",
    "        num_register_tokens=0,\n",
    "        interpolate_antialias=False,\n",
    "        interpolate_offset=0.1,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            img_size (int, tuple): input image size\n",
    "            patch_size (int, tuple): patch size\n",
    "            in_chans (int): number of input channels\n",
    "            embed_dim (int): embedding dimension\n",
    "            depth (int): depth of transformer\n",
    "            num_heads (int): number of attention heads\n",
    "            mlp_ratio (int): ratio of mlp hidden dim to embedding dim\n",
    "            qkv_bias (bool): enable bias for qkv if True\n",
    "            proj_bias (bool): enable bias for proj in attn if True\n",
    "            ffn_bias (bool): enable bias for ffn if True\n",
    "            drop_path_rate (float): stochastic depth rate\n",
    "            drop_path_uniform (bool): apply uniform drop rate across blocks\n",
    "            weight_init (str): weight init scheme\n",
    "            init_values (float): layer-scale init values\n",
    "            embed_layer (nn.Module): patch embedding layer\n",
    "            act_layer (nn.Module): MLP activation layer\n",
    "            block_fn (nn.Module): transformer block class\n",
    "            ffn_layer (str): \"mlp\", \"swiglu\", \"swiglufused\" or \"identity\"\n",
    "            block_chunks: (int) split block sequence into block_chunks units for FSDP wrap\n",
    "            num_register_tokens: (int) number of extra cls tokens (so-called \"registers\")\n",
    "            interpolate_antialias: (str) flag to apply anti-aliasing when interpolating positional embeddings\n",
    "            interpolate_offset: (float) work-around offset to apply when interpolating positional embeddings\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        norm_layer = partial(nn.LayerNorm, eps=1e-6)\n",
    "\n",
    "        self.num_features = self.embed_dim = embed_dim  # num_features for consistency with other models\n",
    "        self.num_tokens = 1\n",
    "        self.n_blocks = depth\n",
    "        self.num_heads = num_heads\n",
    "        self.patch_size = patch_size\n",
    "        self.num_register_tokens = num_register_tokens\n",
    "        self.interpolate_antialias = interpolate_antialias\n",
    "        self.interpolate_offset = interpolate_offset\n",
    "\n",
    "        self.patch_embed = embed_layer(img_size=img_size, patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim)\n",
    "        num_patches = self.patch_embed.num_patches\n",
    "\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + self.num_tokens, embed_dim))\n",
    "        assert num_register_tokens >= 0\n",
    "        self.register_tokens = (\n",
    "            nn.Parameter(torch.zeros(1, num_register_tokens, embed_dim)) if num_register_tokens else None\n",
    "        )\n",
    "\n",
    "        if drop_path_uniform is True:\n",
    "            dpr = [drop_path_rate] * depth\n",
    "        else:\n",
    "            dpr = np.linspace(0, drop_path_rate, depth).tolist()  # stochastic depth decay rule\n",
    "\n",
    "        if ffn_layer == \"mlp\":\n",
    "            logger.info(\"using MLP layer as FFN\")\n",
    "            ffn_layer = Mlp\n",
    "        elif ffn_layer == \"swiglufused\" or ffn_layer == \"swiglu\":\n",
    "            logger.info(\"using SwiGLU layer as FFN\")\n",
    "            ffn_layer = SwiGLUFFNFused\n",
    "        elif ffn_layer == \"identity\":\n",
    "            logger.info(\"using Identity layer as FFN\")\n",
    "\n",
    "            def f(*args, **kwargs):\n",
    "                return nn.Identity()\n",
    "\n",
    "            ffn_layer = f\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        blocks_list = [\n",
    "            block_fn(\n",
    "                dim=embed_dim,\n",
    "                num_heads=num_heads,\n",
    "                mlp_ratio=mlp_ratio,\n",
    "                qkv_bias=qkv_bias,\n",
    "                proj_bias=proj_bias,\n",
    "                ffn_bias=ffn_bias,\n",
    "                drop_path=dpr[i],\n",
    "                norm_layer=norm_layer,\n",
    "                act_layer=act_layer,\n",
    "                ffn_layer=ffn_layer,\n",
    "                init_values=init_values,\n",
    "            )\n",
    "            for i in range(depth)\n",
    "        ]\n",
    "        if block_chunks > 0:\n",
    "            self.chunked_blocks = True\n",
    "            chunked_blocks = []\n",
    "            chunksize = depth // block_chunks\n",
    "            for i in range(0, depth, chunksize):\n",
    "                # this is to keep the block index consistent if we chunk the block list\n",
    "                chunked_blocks.append([nn.Identity()] * i + blocks_list[i : i + chunksize])\n",
    "            self.blocks = nn.ModuleList([BlockChunk(p) for p in chunked_blocks])\n",
    "        else:\n",
    "            self.chunked_blocks = False\n",
    "            self.blocks = nn.ModuleList(blocks_list)\n",
    "\n",
    "        self.norm = norm_layer(embed_dim)\n",
    "        self.head = nn.Identity()\n",
    "\n",
    "        self.mask_token = nn.Parameter(torch.zeros(1, embed_dim))\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        trunc_normal_(self.pos_embed, std=0.02)\n",
    "        nn.init.normal_(self.cls_token, std=1e-6)\n",
    "        if self.register_tokens is not None:\n",
    "            nn.init.normal_(self.register_tokens, std=1e-6)\n",
    "        named_apply(init_weights_vit_timm, self)\n",
    "\n",
    "    def interpolate_pos_encoding(self, x, w, h):\n",
    "        previous_dtype = x.dtype\n",
    "        npatch = x.shape[1] - 1\n",
    "        N = self.pos_embed.shape[1] - 1\n",
    "        if npatch == N and w == h:\n",
    "            return self.pos_embed\n",
    "        pos_embed = self.pos_embed.float()\n",
    "        class_pos_embed = pos_embed[:, 0]\n",
    "        patch_pos_embed = pos_embed[:, 1:]\n",
    "        dim = x.shape[-1]\n",
    "        w0 = w // self.patch_size\n",
    "        h0 = h // self.patch_size\n",
    "        M = int(math.sqrt(N))  # Recover the number of patches in each dimension\n",
    "        assert N == M * M\n",
    "        kwargs = {}\n",
    "        if self.interpolate_offset:\n",
    "            # Historical kludge: add a small number to avoid floating point error in the interpolation, see https://github.com/facebookresearch/dino/issues/8\n",
    "            # Note: still needed for backward-compatibility, the underlying operators are using both output size and scale factors\n",
    "            sx = float(w0 + self.interpolate_offset) / M\n",
    "            sy = float(h0 + self.interpolate_offset) / M\n",
    "            kwargs[\"scale_factor\"] = (sx, sy)\n",
    "        else:\n",
    "            # Simply specify an output size instead of a scale factor\n",
    "            kwargs[\"size\"] = (w0, h0)\n",
    "        patch_pos_embed = nn.functional.interpolate(\n",
    "            patch_pos_embed.reshape(1, M, M, dim).permute(0, 3, 1, 2),\n",
    "            mode=\"bicubic\",\n",
    "            antialias=self.interpolate_antialias,\n",
    "            **kwargs,\n",
    "        )\n",
    "        assert (w0, h0) == patch_pos_embed.shape[-2:]\n",
    "        patch_pos_embed = patch_pos_embed.permute(0, 2, 3, 1).view(1, -1, dim)\n",
    "        return torch.cat((class_pos_embed.unsqueeze(0), patch_pos_embed), dim=1).to(previous_dtype)\n",
    "\n",
    "    def prepare_tokens_with_masks(self, x, masks=None):\n",
    "        B, nc, w, h = x.shape\n",
    "        x = self.patch_embed(x)\n",
    "        if masks is not None:\n",
    "            x = torch.where(masks.unsqueeze(-1), self.mask_token.to(x.dtype).unsqueeze(0), x)\n",
    "\n",
    "        x = torch.cat((self.cls_token.expand(x.shape[0], -1, -1), x), dim=1)\n",
    "        x = x + self.interpolate_pos_encoding(x, w, h)\n",
    "\n",
    "        if self.register_tokens is not None:\n",
    "            x = torch.cat(\n",
    "                (\n",
    "                    x[:, :1],\n",
    "                    self.register_tokens.expand(x.shape[0], -1, -1),\n",
    "                    x[:, 1:],\n",
    "                ),\n",
    "                dim=1,\n",
    "            )\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward_features_list(self, x_list, masks_list):\n",
    "        x = [self.prepare_tokens_with_masks(x, masks) for x, masks in zip(x_list, masks_list)]\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x)\n",
    "\n",
    "        all_x = x\n",
    "        output = []\n",
    "        for x, masks in zip(all_x, masks_list):\n",
    "            x_norm = self.norm(x)\n",
    "            output.append(\n",
    "                {\n",
    "                    \"x_norm_clstoken\": x_norm[:, 0],\n",
    "                    \"x_norm_regtokens\": x_norm[:, 1 : self.num_register_tokens + 1],\n",
    "                    \"x_norm_patchtokens\": x_norm[:, self.num_register_tokens + 1 :],\n",
    "                    \"x_prenorm\": x,\n",
    "                    \"masks\": masks,\n",
    "                }\n",
    "            )\n",
    "        return output\n",
    "\n",
    "    def forward_features(self, x, masks=None):\n",
    "        if isinstance(x, list):\n",
    "            return self.forward_features_list(x, masks)\n",
    "\n",
    "        x = self.prepare_tokens_with_masks(x, masks)\n",
    "\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x)\n",
    "\n",
    "        x_norm = self.norm(x)\n",
    "        return {\n",
    "            \"x_norm_clstoken\": x_norm[:, 0],\n",
    "            \"x_norm_regtokens\": x_norm[:, 1 : self.num_register_tokens + 1],\n",
    "            \"x_norm_patchtokens\": x_norm[:, self.num_register_tokens + 1 :],\n",
    "            \"x_prenorm\": x,\n",
    "            \"masks\": masks,\n",
    "        }\n",
    "\n",
    "    def _get_intermediate_layers_not_chunked(self, x, n=1):\n",
    "        x = self.prepare_tokens_with_masks(x)\n",
    "        # If n is an int, take the n last blocks. If it's a list, take them\n",
    "        output, total_block_len = [], len(self.blocks)\n",
    "        blocks_to_take = range(total_block_len - n, total_block_len) if isinstance(n, int) else n\n",
    "        for i, blk in enumerate(self.blocks):\n",
    "            x = blk(x)\n",
    "            if i in blocks_to_take:\n",
    "                output.append(x)\n",
    "        assert len(output) == len(blocks_to_take), f\"only {len(output)} / {len(blocks_to_take)} blocks found\"\n",
    "        return output\n",
    "\n",
    "    def _get_intermediate_layers_chunked(self, x, n=1):\n",
    "        x = self.prepare_tokens_with_masks(x)\n",
    "        output, i, total_block_len = [], 0, len(self.blocks[-1])\n",
    "        # If n is an int, take the n last blocks. If it's a list, take them\n",
    "        blocks_to_take = range(total_block_len - n, total_block_len) if isinstance(n, int) else n\n",
    "        for block_chunk in self.blocks:\n",
    "            for blk in block_chunk[i:]:  # Passing the nn.Identity()\n",
    "                x = blk(x)\n",
    "                if i in blocks_to_take:\n",
    "                    output.append(x)\n",
    "                i += 1\n",
    "        assert len(output) == len(blocks_to_take), f\"only {len(output)} / {len(blocks_to_take)} blocks found\"\n",
    "        return output\n",
    "\n",
    "    def get_intermediate_layers(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        n: Union[int, Sequence] = 1,  # Layers or n last layers to take\n",
    "        reshape: bool = False,\n",
    "        return_class_token: bool = False,\n",
    "        norm=True,\n",
    "    ) -> Tuple[Union[torch.Tensor, Tuple[torch.Tensor]]]:\n",
    "        if self.chunked_blocks:\n",
    "            outputs = self._get_intermediate_layers_chunked(x, n)\n",
    "        else:\n",
    "            outputs = self._get_intermediate_layers_not_chunked(x, n)\n",
    "        if norm:\n",
    "            outputs = [self.norm(out) for out in outputs]\n",
    "        class_tokens = [out[:, 0] for out in outputs]\n",
    "        outputs = [out[:, 1 + self.num_register_tokens :] for out in outputs]\n",
    "        if reshape:\n",
    "            B, _, w, h = x.shape\n",
    "            outputs = [\n",
    "                out.reshape(B, w // self.patch_size, h // self.patch_size, -1).permute(0, 3, 1, 2).contiguous()\n",
    "                for out in outputs\n",
    "            ]\n",
    "        if return_class_token:\n",
    "            return tuple(zip(outputs, class_tokens))\n",
    "        return tuple(outputs)\n",
    "\n",
    "    def forward(self, *args, is_training=False, **kwargs):\n",
    "        ret = self.forward_features(*args, **kwargs)\n",
    "        if is_training:\n",
    "            return ret\n",
    "        else:\n",
    "            return self.head(ret[\"x_norm_clstoken\"])\n",
    "\n",
    "\n",
    "def init_weights_vit_timm(module: nn.Module, name: str = \"\"):\n",
    "    \"\"\"ViT weight initialization, original timm impl (for reproducibility)\"\"\"\n",
    "    if isinstance(module, nn.Linear):\n",
    "        trunc_normal_(module.weight, std=0.02)\n",
    "        if module.bias is not None:\n",
    "            nn.init.zeros_(module.bias)\n",
    "\n",
    "\n",
    "def vit_small(patch_size=16, num_register_tokens=0, **kwargs):\n",
    "    model = DinoVisionTransformer(\n",
    "        patch_size=patch_size,\n",
    "        embed_dim=384,\n",
    "        depth=12,\n",
    "        num_heads=6,\n",
    "        mlp_ratio=4,\n",
    "        block_fn=partial(Block, attn_class=MemEffAttention),\n",
    "        num_register_tokens=num_register_tokens,\n",
    "        **kwargs,\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "def vit_base(patch_size=16, num_register_tokens=0, **kwargs):\n",
    "    model = DinoVisionTransformer(\n",
    "        patch_size=patch_size,\n",
    "        embed_dim=768,\n",
    "        depth=12,\n",
    "        num_heads=12,\n",
    "        mlp_ratio=4,\n",
    "        block_fn=partial(Block, attn_class=MemEffAttention),\n",
    "        num_register_tokens=num_register_tokens,\n",
    "        **kwargs,\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "def vit_large(patch_size=16, num_register_tokens=0, **kwargs):\n",
    "    model = DinoVisionTransformer(\n",
    "        patch_size=patch_size,\n",
    "        embed_dim=1024,\n",
    "        depth=24,\n",
    "        num_heads=16,\n",
    "        mlp_ratio=4,\n",
    "        block_fn=partial(Block, attn_class=MemEffAttention),\n",
    "        num_register_tokens=num_register_tokens,\n",
    "        **kwargs,\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "def vit_giant2(patch_size=16, num_register_tokens=0, **kwargs):\n",
    "    \"\"\"\n",
    "    Close to ViT-giant, with embed-dim 1536 and 24 heads => embed-dim per head 64\n",
    "    \"\"\"\n",
    "    model = DinoVisionTransformer(\n",
    "        patch_size=patch_size,\n",
    "        embed_dim=1536,\n",
    "        depth=40,\n",
    "        num_heads=24,\n",
    "        mlp_ratio=4,\n",
    "        block_fn=partial(Block, attn_class=MemEffAttention),\n",
    "        num_register_tokens=num_register_tokens,\n",
    "        **kwargs,\n",
    "    )\n",
    "    return model\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kzLWdzhDCk3j",
    "outputId": "24e67860-ebaf-4629-84b2-1277d8133b24"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/content/semantic_correspondence/models/dinov2/dinov2/layers/swiglu_ffn.py:51: UserWarning: xFormers is not available (SwiGLU)\n",
      "  warnings.warn(\"xFormers is not available (SwiGLU)\")\n",
      "/content/semantic_correspondence/models/dinov2/dinov2/layers/attention.py:33: UserWarning: xFormers is not available (Attention)\n",
      "  warnings.warn(\"xFormers is not available (Attention)\")\n",
      "/content/semantic_correspondence/models/dinov2/dinov2/layers/block.py:40: UserWarning: xFormers is not available (Block)\n",
      "  warnings.warn(\"xFormers is not available (Block)\")\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "Quick test script to verify finetuning pipeline works correctly.\n",
    "Runs only a few iterations to check for errors before full training.\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "#from SPair71k.devkit.SPairDataset import SPairDataset\n",
    "#from dinov2 import extract_dense_features, pixel_to_patch_coord\n",
    "#from models.dinov2.dinov2.models.vision_transformer import vit_base\n",
    "\n",
    "\n",
    "def freeze_model(model):\n",
    "    \"\"\"Freeze all model parameters\"\"\"\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "\n",
    "def unfreeze_last_n_blocks(model, n_blocks):\n",
    "    \"\"\"Unfreeze the last n_blocks transformer blocks + final norm layer\"\"\"\n",
    "    total_blocks = len(model.blocks)\n",
    "    for i in range(total_blocks - n_blocks, total_blocks):\n",
    "        for param in model.blocks[i].parameters():\n",
    "            param.requires_grad = True\n",
    "    for param in model.norm.parameters():\n",
    "        param.requires_grad = True\n",
    "    print(f\"✓ Unfrozen last {n_blocks} blocks + norm layer\")\n",
    "\n",
    "\n",
    "def compute_cross_entropy_loss(src_features, tgt_features, src_kps, trg_kps,\n",
    "                               src_original_size, tgt_original_size, temperature=10.0):\n",
    "    \"\"\"Compute cross-entropy loss for semantic correspondence\"\"\"\n",
    "    _, H, W, D = tgt_features.shape\n",
    "    tgt_flat = tgt_features.reshape(H * W, D)\n",
    "\n",
    "    losses = []\n",
    "\n",
    "    for i in range(src_kps.shape[0]):\n",
    "        src_x, src_y = src_kps[i]\n",
    "        tgt_x, tgt_y = trg_kps[i]\n",
    "\n",
    "        src_patch_x, src_patch_y = pixel_to_patch_coord(src_x, src_y, src_original_size)\n",
    "        src_feature = src_features[0, src_patch_y, src_patch_x, :]\n",
    "\n",
    "        tgt_patch_x, tgt_patch_y = pixel_to_patch_coord(tgt_x, tgt_y, tgt_original_size)\n",
    "\n",
    "        similarities = F.cosine_similarity(\n",
    "            src_feature.unsqueeze(0),\n",
    "            tgt_flat,\n",
    "            dim=1\n",
    "        )\n",
    "\n",
    "        log_probs = F.log_softmax(similarities * temperature, dim=0)\n",
    "        gt_idx = tgt_patch_y * W + tgt_patch_x\n",
    "        loss = -log_probs[gt_idx]\n",
    "        losses.append(loss)\n",
    "\n",
    "    return torch.stack(losses).mean()\n",
    "\n",
    "\n",
    "def quick_test():\n",
    "    \"\"\"Quick test with just a few samples\"\"\"\n",
    "\n",
    "    print(\"=\"*60)\n",
    "    print(\"QUICK FINETUNING TEST\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # Configuration\n",
    "    n_blocks = 2\n",
    "    n_train_samples = 5  # Only 5 samples for quick test\n",
    "    n_test_samples = 3   # Only 3 for evaluation\n",
    "    temperature = 10.0\n",
    "    learning_rate = 1e-4\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print(f\"\\n✓ Using device: {device}\")\n",
    "\n",
    "    # Load model\n",
    "    print(\"\\n[1/5] Loading DINOv2 model...\")\n",
    "    model = vit_base(\n",
    "        img_size=(518, 518),\n",
    "        patch_size=14,\n",
    "        num_register_tokens=0,\n",
    "        block_chunks=0,\n",
    "        init_values=1.0,\n",
    "    )\n",
    "\n",
    "    ckpt = torch.load(\"models/dinov2/dinov2_vitb14_pretrain.pth\", map_location=device)\n",
    "    model.load_state_dict(ckpt, strict=True)\n",
    "    model.to(device)\n",
    "    print(\"✓ Model loaded\")\n",
    "\n",
    "    # Freeze and unfreeze\n",
    "    print(f\"\\n[2/5] Freezing model and unfreezing last {n_blocks} blocks...\")\n",
    "    freeze_model(model)\n",
    "    unfreeze_last_n_blocks(model, n_blocks)\n",
    "\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"✓ Trainable: {trainable_params:,} / {total_params:,} ({100*trainable_params/total_params:.2f}%)\")\n",
    "\n",
    "    # Load dataset (small subset)\n",
    "    print(f\"\\n[3/5] Loading {n_train_samples} training samples...\")\n",
    "    base = '/content/semantic_correspondence/SPair71k'\n",
    "    train_dataset = SPairDataset(\n",
    "        f'{base}/PairAnnotation',\n",
    "        f'{base}/Layout',\n",
    "        f'{base}/JPEGImages',\n",
    "        'large',\n",
    "        0.1,\n",
    "        datatype='trn'\n",
    "    )\n",
    "    print(f\"✓ Dataset loaded (total: {len(train_dataset)} samples)\")\n",
    "\n",
    "    # Create optimizer\n",
    "    optimizer = optim.AdamW(\n",
    "        filter(lambda p: p.requires_grad, model.parameters()),\n",
    "        lr=learning_rate,\n",
    "        weight_decay=0.01\n",
    "    )\n",
    "\n",
    "    # Test training loop\n",
    "    print(f\"\\n[4/5] Testing training loop with {n_train_samples} samples...\")\n",
    "    model.train()\n",
    "\n",
    "    for idx in range(n_train_samples):\n",
    "        sample = train_dataset[idx]\n",
    "\n",
    "        # Prepare data\n",
    "        src_tensor = sample['src_img'].unsqueeze(0).to(device)\n",
    "        tgt_tensor = sample['trg_img'].unsqueeze(0).to(device)\n",
    "\n",
    "        src_tensor = F.interpolate(src_tensor, size=(518, 518), mode='bilinear', align_corners=False)\n",
    "        tgt_tensor = F.interpolate(tgt_tensor, size=(518, 518), mode='bilinear', align_corners=False)\n",
    "\n",
    "        src_original_size = (sample['src_imsize'][2], sample['src_imsize'][1])\n",
    "        tgt_original_size = (sample['trg_imsize'][2], sample['trg_imsize'][1])\n",
    "\n",
    "        src_kps = sample['src_kps'].numpy()\n",
    "        trg_kps = sample['trg_kps'].numpy()\n",
    "\n",
    "        # Extract features\n",
    "        src_features = extract_dense_features(model, src_tensor, training=True)\n",
    "        tgt_features = extract_dense_features(model, tgt_tensor, training=True)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = compute_cross_entropy_loss(\n",
    "            src_features, tgt_features,\n",
    "            src_kps, trg_kps,\n",
    "            src_original_size, tgt_original_size,\n",
    "            temperature=temperature\n",
    "        )\n",
    "\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        print(f\"  Sample {idx+1}/{n_train_samples}: Loss = {loss.item():.4f}\")\n",
    "\n",
    "    print(\"✓ Training loop completed successfully!\")\n",
    "\n",
    "    # Test evaluation\n",
    "    print(f\"\\n[5/5] Testing evaluation with {n_test_samples} samples...\")\n",
    "    model.eval()\n",
    "\n",
    "    test_dataset = SPairDataset(\n",
    "        f'{base}/PairAnnotation',\n",
    "        f'{base}/Layout',\n",
    "        f'{base}/JPEGImages',\n",
    "        'large',\n",
    "        0.1,\n",
    "        datatype='test'\n",
    "    )\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx in range(n_test_samples):\n",
    "            sample = test_dataset[idx]\n",
    "\n",
    "            src_tensor = sample['src_img'].unsqueeze(0).to(device)\n",
    "            tgt_tensor = sample['trg_img'].unsqueeze(0).to(device)\n",
    "\n",
    "            src_tensor = F.interpolate(src_tensor, size=(518, 518), mode='bilinear', align_corners=False)\n",
    "            tgt_tensor = F.interpolate(tgt_tensor, size=(518, 518), mode='bilinear', align_corners=False)\n",
    "\n",
    "            src_features = extract_dense_features(model, src_tensor, training=False)\n",
    "            tgt_features = extract_dense_features(model, tgt_tensor, training=False)\n",
    "\n",
    "            print(f\"  Sample {idx+1}/{n_test_samples}: Features shape = {src_features.shape}\")\n",
    "\n",
    "    print(\"✓ Evaluation completed successfully!\")\n",
    "\n",
    "    # Verify gradients\n",
    "    print(\"\\n[VERIFICATION] Checking gradient flow...\")\n",
    "    has_grads = False\n",
    "    grad_layers = []\n",
    "\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad and param.grad is not None:\n",
    "            has_grads = True\n",
    "            grad_layers.append(name)\n",
    "\n",
    "    if has_grads:\n",
    "        print(f\"✓ Gradients detected in {len(grad_layers)} layers\")\n",
    "        print(\"  Sample layers with gradients:\")\n",
    "        for layer in grad_layers[:3]:  # Show first 3\n",
    "            print(f\"    - {layer}\")\n",
    "    else:\n",
    "        print(\"✗ WARNING: No gradients detected!\")\n",
    "\n",
    "    # Summary\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"TEST SUMMARY\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"✓ Model loading: OK\")\n",
    "    print(\"✓ Freeze/Unfreeze: OK\")\n",
    "    print(\"✓ Training loop: OK\")\n",
    "    print(\"✓ Backward pass: OK\")\n",
    "    print(\"✓ Evaluation: OK\")\n",
    "    print(f\"✓ Gradient flow: {'OK' if has_grads else 'FAILED'}\")\n",
    "    print(\"\\n✓ All tests passed! Ready for full training.\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        quick_test()\n",
    "    except Exception as e:\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"✗ TEST FAILED\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"Error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-mTGc6VfALcw",
    "outputId": "3337ac76-4a8b-4706-b26d-e61bf4f7787c"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "============================================================\n",
      "QUICK FINETUNING TEST\n",
      "============================================================\n",
      "\n",
      "✓ Using device: cuda\n",
      "\n",
      "[1/5] Loading DINOv2 model...\n",
      "✓ Model loaded\n",
      "\n",
      "[2/5] Freezing model and unfreezing last 2 blocks...\n",
      "✓ Unfrozen last 2 blocks + norm layer\n",
      "✓ Trainable: 14,180,352 / 86,580,480 (16.38%)\n",
      "\n",
      "[3/5] Loading 5 training samples...\n",
      "✓ Dataset loaded (total: 53340 samples)\n",
      "\n",
      "[4/5] Testing training loop with 5 samples...\n",
      "  Sample 1/5: Loss = 4.5495\n",
      "  Sample 2/5: Loss = 3.7546\n",
      "  Sample 3/5: Loss = 3.0361\n",
      "  Sample 4/5: Loss = 2.6244\n",
      "  Sample 5/5: Loss = 1.9461\n",
      "✓ Training loop completed successfully!\n",
      "\n",
      "[5/5] Testing evaluation with 3 samples...\n",
      "  Sample 1/3: Features shape = torch.Size([1, 37, 37, 768])\n",
      "  Sample 2/3: Features shape = torch.Size([1, 37, 37, 768])\n",
      "  Sample 3/3: Features shape = torch.Size([1, 37, 37, 768])\n",
      "✓ Evaluation completed successfully!\n",
      "\n",
      "[VERIFICATION] Checking gradient flow...\n",
      "✓ Gradients detected in 30 layers\n",
      "  Sample layers with gradients:\n",
      "    - blocks.10.norm1.weight\n",
      "    - blocks.10.norm1.bias\n",
      "    - blocks.10.attn.qkv.weight\n",
      "\n",
      "============================================================\n",
      "TEST SUMMARY\n",
      "============================================================\n",
      "✓ Model loading: OK\n",
      "✓ Freeze/Unfreeze: OK\n",
      "✓ Training loop: OK\n",
      "✓ Backward pass: OK\n",
      "✓ Evaluation: OK\n",
      "✓ Gradient flow: OK\n",
      "\n",
      "✓ All tests passed! Ready for full training.\n",
      "============================================================\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "#argmax function to find best matching patch\n",
    "def find_best_match_argmax(s, width):\n",
    "    best_match_idx = s.argmax().item()#argmax over the similarities\n",
    "    y = best_match_idx // width\n",
    "    x = best_match_idx % width\n",
    "    return x, y"
   ],
   "metadata": {
    "id": "CK_34apiDfkt"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "def compute_pck(pred_points, gt_points, img_size, threshold):\n",
    "    \"\"\"\n",
    "    Compute PCK@threshold\n",
    "\n",
    "    Args:\n",
    "        pred_points: list of [x, y] predictions\n",
    "        gt_points: list of [x, y] ground truth\n",
    "        img_size: (width, height) of the image\n",
    "        threshold: normalized threshold (e.g., 0.05, 0.1, 0.2)\n",
    "\n",
    "    Returns:\n",
    "        pck: percentage of correct keypoints\n",
    "        correct_mask: boolean array indicating which keypoints are correct\n",
    "    \"\"\"\n",
    "    pred_points = np.array(pred_points)\n",
    "    gt_points = np.array(gt_points)\n",
    "\n",
    "    #compute Euclidean distance\n",
    "    distances = np.sqrt(np.sum((pred_points - gt_points) ** 2, axis=1))\n",
    "\n",
    "    #normalize by image diagonal (standard protocol)\n",
    "    img_diagonal = np.sqrt(img_size[0] ** 2 + img_size[1] ** 2)\n",
    "    normalized_distances = distances / img_diagonal\n",
    "\n",
    "    #check which keypoints are within threshold\n",
    "    correct_mask = normalized_distances <= threshold\n",
    "    pck = np.mean(correct_mask) * 100  # percentage\n",
    "\n",
    "    return pck, correct_mask, normalized_distances\n",
    "\n",
    "def compute_pck_spair71k(pred_points, gt_points, bbox, threshold):\n",
    "    \"\"\"\n",
    "    Compute PCK@threshold\n",
    "\n",
    "    Args:\n",
    "        pred_points: list of [x, y] predictions\n",
    "        gt_points: list of [x, y] ground truth\n",
    "        bbox: [xmin, ymin, xmax, ymax]\n",
    "        threshold: normalized threshold (e.g., 0.05, 0.1, 0.2)\n",
    "\n",
    "    Returns:\n",
    "        pck: percentage of correct keypoints\n",
    "        correct_mask: boolean array indicating which keypoints are correct\n",
    "    \"\"\"\n",
    "    pred_points = np.array(pred_points)\n",
    "    gt_points = np.array(gt_points)\n",
    "\n",
    "    #compute Euclidean distance\n",
    "    distances = np.sqrt(np.sum((pred_points - gt_points) ** 2, axis=1))\n",
    "\n",
    "    # Normalize by max(bbox_width, bbox_height) - STANDARD SPAIR-71K\n",
    "    bbox_width = bbox[2] - bbox[0]\n",
    "    bbox_height = bbox[3] - bbox[1]\n",
    "    normalization_factor = max(bbox_width, bbox_height)\n",
    "    normalized_distances = distances / normalization_factor\n",
    "\n",
    "    #check which keypoints are within threshold\n",
    "    correct_mask = normalized_distances <= threshold\n",
    "    pck = np.mean(correct_mask) * 100  # percentage\n",
    "\n",
    "    return pck, correct_mask, normalized_distances\n"
   ],
   "metadata": {
    "id": "xf6FhjynDjjn"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import json\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "from datetime import datetime\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#from SPair71k.devkit.SPairDataset import SPairDataset\n",
    "#from dinov2 import extract_dense_features, pixel_to_patch_coord, patch_to_pixel_coord\n",
    "#from matching_strategies import find_best_match_argmax\n",
    "#from pck import compute_pck_spair71k\n",
    "#from models.dinov2.dinov2.models.vision_transformer import vit_base\n",
    "\n",
    "\n",
    "def freeze_model(model):\n",
    "    \"\"\"Freeze all model parameters\"\"\"\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "\n",
    "def unfreeze_last_n_blocks(model, n_blocks):\n",
    "    \"\"\"\n",
    "    Unfreeze the last n_blocks transformer blocks + final norm layer\n",
    "\n",
    "    Args:\n",
    "        model: DINOv2 model\n",
    "        n_blocks: number of blocks to unfreeze (counting from the end)\n",
    "    \"\"\"\n",
    "    total_blocks = len(model.blocks)\n",
    "\n",
    "    # Unfreeze last n blocks\n",
    "    for i in range(total_blocks - n_blocks, total_blocks):\n",
    "        for param in model.blocks[i].parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "    # Also unfreeze the final normalization layer\n",
    "    for param in model.norm.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "    print(f\"Unfrozen last {n_blocks} blocks + norm layer\")\n",
    "\n",
    "\n",
    "def compute_cross_entropy_loss(src_features, tgt_features, src_kps, trg_kps,\n",
    "                               src_original_size, tgt_original_size, temperature=10.0):\n",
    "    \"\"\"\n",
    "    Compute cross-entropy loss for semantic correspondence.\n",
    "    Treats correspondence as a classification problem where each target patch is a class.\n",
    "\n",
    "    Args:\n",
    "        src_features: [1, H, W, D] source dense features\n",
    "        tgt_features: [1, H, W, D] target dense features\n",
    "        src_kps: [N, 2] source keypoints in pixel coordinates\n",
    "        trg_kps: [N, 2] target keypoints in pixel coordinates\n",
    "        src_original_size: (width, height) of original source image\n",
    "        tgt_original_size: (width, height) of original target image\n",
    "        temperature: softmax temperature (higher = more peaked distribution)\n",
    "\n",
    "    Returns:\n",
    "        loss: mean cross-entropy loss across all keypoints\n",
    "    \"\"\"\n",
    "    _, H, W, D = tgt_features.shape\n",
    "    tgt_flat = tgt_features.reshape(H * W, D)  # [H*W, D]\n",
    "\n",
    "    losses = []\n",
    "\n",
    "    for i in range(src_kps.shape[0]):\n",
    "        src_x, src_y = src_kps[i]\n",
    "        tgt_x, tgt_y = trg_kps[i]\n",
    "\n",
    "        # Get source feature at keypoint location\n",
    "        src_patch_x, src_patch_y = pixel_to_patch_coord(src_x, src_y, src_original_size)\n",
    "        src_feature = src_features[0, src_patch_y, src_patch_x, :]  # [D]\n",
    "\n",
    "        # Get ground truth target patch coordinates\n",
    "        tgt_patch_x, tgt_patch_y = pixel_to_patch_coord(tgt_x, tgt_y, tgt_original_size)\n",
    "\n",
    "        # Compute cosine similarities with all target patches\n",
    "        similarities = F.cosine_similarity(\n",
    "            src_feature.unsqueeze(0),  # [1, D]\n",
    "            tgt_flat,  # [H*W, D]\n",
    "            dim=1\n",
    "        )  # [H*W]\n",
    "\n",
    "        # Convert similarities to log-probabilities\n",
    "        log_probs = F.log_softmax(similarities * temperature, dim=0)\n",
    "\n",
    "        # Ground truth index (flatten 2D coordinates to 1D)\n",
    "        gt_idx = tgt_patch_y * W + tgt_patch_x\n",
    "\n",
    "        # Negative log-likelihood loss\n",
    "        loss = -log_probs[gt_idx]\n",
    "        losses.append(loss)\n",
    "\n",
    "    return torch.stack(losses).mean()\n",
    "\n",
    "def evaluate(model, dataset, device, thresholds=[0.05, 0.1, 0.2]):\n",
    "    \"\"\"\n",
    "    Evaluate model on test set using PCK metric\n",
    "\n",
    "    Args:\n",
    "        model: DINOv2 model\n",
    "        dataset: test dataset\n",
    "        device: 'cuda' or 'cpu'\n",
    "        thresholds: list of PCK thresholds to evaluate\n",
    "\n",
    "    Returns:\n",
    "        results_SPair71K: dictionary with PCK scores at different thresholds\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    per_image_metrics = []\n",
    "\n",
    "    print(f\"Evaluating on {len(dataset)} image pairs...\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, sample in enumerate(dataset):\n",
    "            # Prepare images\n",
    "            src_tensor = sample['src_img'].unsqueeze(0).to(device)\n",
    "            tgt_tensor = sample['trg_img'].unsqueeze(0).to(device)\n",
    "\n",
    "            src_tensor = F.interpolate(src_tensor, size=(518, 518), mode='bilinear', align_corners=False)\n",
    "            tgt_tensor = F.interpolate(tgt_tensor, size=(518, 518), mode='bilinear', align_corners=False)\n",
    "\n",
    "            src_original_size = (sample['src_imsize'][2], sample['src_imsize'][1])\n",
    "            tgt_original_size = (sample['trg_imsize'][2], sample['trg_imsize'][1])\n",
    "\n",
    "            # Extract features\n",
    "            src_features = extract_dense_features(model, src_tensor)\n",
    "            tgt_features = extract_dense_features(model, tgt_tensor)\n",
    "\n",
    "            _, H, W, D = tgt_features.shape\n",
    "            tgt_flat = tgt_features.reshape(H * W, D)\n",
    "\n",
    "            # Get keypoints and bbox\n",
    "            src_kps = sample['src_kps'].numpy()\n",
    "            trg_kps = sample['trg_kps'].numpy()\n",
    "            trg_bbox = sample['trg_bbox']\n",
    "\n",
    "            # Predict matches for all keypoints\n",
    "            pred_matches = []\n",
    "\n",
    "            for i in range(src_kps.shape[0]):\n",
    "                src_x, src_y = src_kps[i]\n",
    "\n",
    "                # Get source feature\n",
    "                patch_x, patch_y = pixel_to_patch_coord(src_x, src_y, src_original_size)\n",
    "                src_feature = src_features[0, patch_y, patch_x, :]\n",
    "\n",
    "                # Compute similarities\n",
    "                similarities = F.cosine_similarity(\n",
    "                    src_feature.unsqueeze(0),\n",
    "                    tgt_flat,\n",
    "                    dim=1\n",
    "                )\n",
    "\n",
    "                # Find best match using argmax\n",
    "                match_patch_x, match_patch_y = find_best_match_argmax(similarities, W)\n",
    "                match_x, match_y = patch_to_pixel_coord(\n",
    "                    match_patch_x, match_patch_y, tgt_original_size\n",
    "                )\n",
    "\n",
    "                pred_matches.append([match_x, match_y])\n",
    "\n",
    "            # Compute PCK for different thresholds\n",
    "            image_pcks = {}\n",
    "            for threshold in thresholds:\n",
    "                pck, _, _ = compute_pck_spair71k(\n",
    "                    pred_matches,\n",
    "                    trg_kps.tolist(),\n",
    "                    trg_bbox,\n",
    "                    threshold\n",
    "                )\n",
    "                image_pcks[threshold] = pck\n",
    "\n",
    "            per_image_metrics.append({\n",
    "                'category': sample['category'],\n",
    "                'pck_scores': image_pcks,\n",
    "            })\n",
    "\n",
    "            if (idx + 1) % 100 == 0:\n",
    "                print(f\"Evaluated {idx + 1}/{len(dataset)} pairs...\")\n",
    "\n",
    "    # Compute overall statistics\n",
    "    results = {}\n",
    "    for threshold in thresholds:\n",
    "        all_pcks = [img['pck_scores'][threshold] for img in per_image_metrics]\n",
    "        results[f'pck@{threshold:.2f}'] = {\n",
    "            'mean': float(np.mean(all_pcks)),\n",
    "            'std': float(np.std(all_pcks)),\n",
    "            'median': float(np.median(all_pcks)),\n",
    "        }\n",
    "\n",
    "    return results, per_image_metrics\n",
    "\n",
    "\n",
    "def train_epoch(model, dataloader, optimizer, device, epoch, temperature=10.0):\n",
    "    \"\"\"\n",
    "    Train for one epoch\n",
    "\n",
    "    Args:\n",
    "        model: DINOv2 model\n",
    "        dataloader: training data loader\n",
    "        optimizer: optimizer\n",
    "        device: 'cuda' or 'cpu'\n",
    "        epoch: current epoch number\n",
    "        temperature: softmax temperature for loss\n",
    "\n",
    "    Returns:\n",
    "        avg_loss: average loss over the epoch\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    num_batches = 0\n",
    "\n",
    "    for idx, sample in enumerate(dataloader):\n",
    "        # Prepare data\n",
    "        src_tensor = sample['src_img'].to(device)  # [1, 3, H, W]\n",
    "        tgt_tensor = sample['trg_img'].to(device)  # [1, 3, H, W]\n",
    "\n",
    "        # Resize to 518x518 (DINOv2 expects this size)\n",
    "        src_tensor = F.interpolate(src_tensor, size=(518, 518), mode='bilinear', align_corners=False)\n",
    "        tgt_tensor = F.interpolate(tgt_tensor, size=(518, 518), mode='bilinear', align_corners=False)\n",
    "\n",
    "        # Store original sizes for coordinate conversion\n",
    "        src_original_size = (sample['src_imsize'][2], sample['src_imsize'][1])\n",
    "        tgt_original_size = (sample['trg_imsize'][2], sample['trg_imsize'][1])\n",
    "\n",
    "        # Get keypoints\n",
    "        src_kps = sample['src_kps'].numpy()[0]  # [N, 2]\n",
    "        trg_kps = sample['trg_kps'].numpy()[0]  # [N, 2]\n",
    "\n",
    "\n",
    "        # Extract dense features\n",
    "        src_features = extract_dense_features(model, src_tensor, training=True)\n",
    "        tgt_features = extract_dense_features(model, tgt_tensor, training=True)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = compute_cross_entropy_loss(\n",
    "            src_features, tgt_features,\n",
    "            src_kps, trg_kps,\n",
    "            src_original_size, tgt_original_size,\n",
    "            temperature=temperature\n",
    "        )\n",
    "\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        num_batches += 1\n",
    "\n",
    "        # Print progress\n",
    "        if (idx + 1) % 50 == 0:\n",
    "            print(f\"Epoch {epoch}, Batch {idx + 1}/{len(dataloader)}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "    avg_loss = total_loss / num_batches\n",
    "    return avg_loss\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main training and evaluation pipeline\"\"\"\n",
    "\n",
    "    # ========== CONFIGURATION ==========\n",
    "    n_blocks_to_unfreeze = 2  # Try: 1, 2, 3, 4, 6, 12\n",
    "    num_epochs = 3\n",
    "    learning_rate = 1e-4\n",
    "    batch_size = 1  # SPair-71k has variable-sized images\n",
    "    temperature = 10.0  # Softmax temperature for cross-entropy loss\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Create results_SPair71K directory with timestamp\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    results_dir = f'results_SPair71K/finetuned_{n_blocks_to_unfreeze}blocks_{timestamp}'\n",
    "    os.makedirs(results_dir, exist_ok=True)\n",
    "    print(f\"Results will be saved to: {results_dir}\")\n",
    "\n",
    "    # ========== LOAD MODEL ==========\n",
    "    print(\"\\nLoading DINOv2-base model...\")\n",
    "    model = vit_base(\n",
    "        img_size=(518, 518),\n",
    "        patch_size=14,\n",
    "        num_register_tokens=0,\n",
    "        block_chunks=0,\n",
    "        init_values=1.0,\n",
    "    )\n",
    "\n",
    "    # Load pretrained weights\n",
    "    ckpt = torch.load(\"models/dinov2/dinov2_vitb14_pretrain.pth\", map_location=device)\n",
    "    model.load_state_dict(ckpt, strict=True)\n",
    "    model.to(device)\n",
    "\n",
    "    # Freeze entire model, then unfreeze last N blocks\n",
    "    freeze_model(model)\n",
    "    unfreeze_last_n_blocks(model, n_blocks_to_unfreeze)\n",
    "\n",
    "    # Count trainable parameters\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"\\nTrainable parameters: {trainable_params:,} / {total_params:,} \"\n",
    "          f\"({100 * trainable_params / total_params:.2f}%)\")\n",
    "\n",
    "    # ========== LOAD DATASETS ==========\n",
    "    print(\"\\nLoading SPair-71k dataset...\")\n",
    "    base = '/content/semantic_correspondence/SPair71k'\n",
    "\n",
    "    train_dataset = SPairDataset(\n",
    "        f'{base}/PairAnnotation',\n",
    "        f'{base}/Layout',\n",
    "        f'{base}/JPEGImages',\n",
    "        'large',\n",
    "        0.1,  # dummy pck_alpha, not used during training\n",
    "        datatype='trn'  # training split\n",
    "    )\n",
    "\n",
    "    val_dataset = SPairDataset(\n",
    "        f'{base}/PairAnnotation',\n",
    "        f'{base}/Layout',\n",
    "        f'{base}/JPEGImages',\n",
    "        'large',\n",
    "        0.1,\n",
    "        datatype='val'\n",
    "    )\n",
    "\n",
    "    print(f\"Training samples: {len(train_dataset)}\")\n",
    "    print(f\"Test samples: {len(val_dataset)}\")\n",
    "\n",
    "    # Create data loader\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=4,\n",
    "        pin_memory=True if device == 'cuda' else False\n",
    "    )\n",
    "\n",
    "    # ========== OPTIMIZER ==========\n",
    "    optimizer = optim.AdamW(\n",
    "        filter(lambda p: p.requires_grad, model.parameters()),\n",
    "        lr=learning_rate,\n",
    "        weight_decay=0.01\n",
    "    )\n",
    "\n",
    "    # Optional: Learning rate scheduler\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
    "\n",
    "    # ========== TRAINING LOOP ==========\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"STARTING TRAINING\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    best_pck = 0\n",
    "    best_epoch = 0\n",
    "    training_history = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"\\n{'=' * 60}\")\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "        print('=' * 60)\n",
    "\n",
    "        # Train for one epoch\n",
    "        train_loss = train_epoch(\n",
    "            model, train_loader, optimizer, device, epoch + 1, temperature=temperature\n",
    "        )\n",
    "        print(f\"\\nAverage training loss: {train_loss:.4f}\")\n",
    "\n",
    "        # Update learning rate\n",
    "        scheduler.step()\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        print(f\"Learning rate: {current_lr:.6f}\")\n",
    "\n",
    "        # Evaluate on val set\n",
    "        print(\"\\nEvaluating on val set...\")\n",
    "        results, per_image_metrics = evaluate(model, val_dataset, device)\n",
    "\n",
    "        print(\"\\nTest Results:\")\n",
    "        for key, value in results.items():\n",
    "            print(f\"  {key}: {value['mean']:.2f}% ± {value['std']:.2f}% \"\n",
    "                  f\"(median: {value['median']:.2f}%)\")\n",
    "\n",
    "        # Save best model\n",
    "        current_pck = results['pck@0.10']['mean']\n",
    "        if current_pck > best_pck:\n",
    "            best_pck = current_pck\n",
    "            best_epoch = epoch + 1\n",
    "\n",
    "            # Save model checkpoint\n",
    "            torch.save({\n",
    "                'epoch': epoch + 1,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'pck': best_pck,\n",
    "                'n_blocks': n_blocks_to_unfreeze,\n",
    "            }, f'{results_dir}/best_model.pth')\n",
    "\n",
    "            print(f\"\\n✓ New best model saved! PCK@0.1: {best_pck:.2f}%\")\n",
    "\n",
    "        # Store training history\n",
    "        training_history.append({\n",
    "            'epoch': epoch + 1,\n",
    "            'train_loss': train_loss,\n",
    "            'learning_rate': current_lr,\n",
    "            'test_results': results\n",
    "        })\n",
    "\n",
    "        # Save intermediate results_SPair71K\n",
    "        with open(f'{results_dir}/training_history.json', 'w') as f:\n",
    "            json.dump(training_history, f, indent=2)\n",
    "\n",
    "    # ========== FINAL RESULTS ==========\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"TRAINING COMPLETED\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Best PCK@0.1: {best_pck:.2f}% (Epoch {best_epoch})\")\n",
    "    print(f\"Results saved to: {results_dir}\")\n",
    "\n",
    "    # Load best model and evaluate on full test set\n",
    "    print(\"\\nLoading best model for final evaluation...\")\n",
    "    checkpoint = torch.load(f'{results_dir}/best_model.pth')\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "    final_results, final_per_image = evaluate(model, test_dataset, device)\n",
    "\n",
    "    # Save final detailed results_SPair71K\n",
    "    with open(f'{results_dir}/final_results.json', 'w') as f:\n",
    "        json.dump({\n",
    "            'best_epoch': best_epoch,\n",
    "            'n_blocks_unfrozen': n_blocks_to_unfreeze,\n",
    "            'temperature': temperature,\n",
    "            'learning_rate': learning_rate,\n",
    "            'num_epochs': num_epochs,\n",
    "            'results_SPair71K': final_results\n",
    "        }, f, indent=2)\n",
    "\n",
    "    # Save per-category analysis\n",
    "    category_results = defaultdict(lambda: defaultdict(list))\n",
    "    for img_metric in final_per_image:\n",
    "        category = img_metric['category']\n",
    "        for threshold, pck in img_metric['pck_scores'].items():\n",
    "            category_results[category][threshold].append(pck)\n",
    "\n",
    "    # Compute per-category statistics\n",
    "    category_stats = {}\n",
    "    for category, thresholds_dict in category_results.items():\n",
    "        category_stats[category] = {}\n",
    "        for threshold, pcks in thresholds_dict.items():\n",
    "            category_stats[category][f'pck@{threshold:.2f}'] = {\n",
    "                'mean': float(np.mean(pcks)),\n",
    "                'std': float(np.std(pcks)),\n",
    "                'n_samples': len(pcks)\n",
    "            }\n",
    "\n",
    "    with open(f'{results_dir}/per_category_results.json', 'w') as f:\n",
    "        json.dump(category_stats, f, indent=2)\n",
    "\n",
    "    print(\"\\nPer-category results_SPair71K:\")\n",
    "    for category, stats in sorted(category_stats.items()):\n",
    "        pck_01 = stats['pck@0.10']['mean']\n",
    "        n_samples = stats['pck@0.10']['n_samples']\n",
    "        print(f\"  {category:20s}: {pck_01:.2f}% (n={n_samples})\")\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "inaivTBiDXBu",
    "outputId": "ffe70c1d-1680-47dc-ff45-d051aa881c62"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Using device: cuda\n",
      "Results will be saved to: results/finetuned_2blocks_20251221_182948\n",
      "\n",
      "Loading DINOv2-base model...\n",
      "Unfrozen last 2 blocks + norm layer\n",
      "\n",
      "Trainable parameters: 14,180,352 / 86,580,480 (16.38%)\n",
      "\n",
      "Loading SPair-71k dataset...\n",
      "Training samples: 53340\n",
      "Test samples: 5384\n",
      "\n",
      "============================================================\n",
      "STARTING TRAINING\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "Epoch 1/3\n",
      "============================================================\n",
      "Epoch 1, Batch 50/53340, Loss: 5.2169\n",
      "Epoch 1, Batch 100/53340, Loss: 2.2020\n",
      "Epoch 1, Batch 150/53340, Loss: 2.9084\n",
      "Epoch 1, Batch 200/53340, Loss: 3.6872\n",
      "Epoch 1, Batch 250/53340, Loss: 4.0350\n",
      "Epoch 1, Batch 300/53340, Loss: 2.9882\n",
      "Epoch 1, Batch 350/53340, Loss: 4.0329\n",
      "Epoch 1, Batch 400/53340, Loss: 2.6079\n",
      "Epoch 1, Batch 450/53340, Loss: 3.6571\n",
      "Epoch 1, Batch 500/53340, Loss: 1.9698\n",
      "Epoch 1, Batch 550/53340, Loss: 2.6591\n",
      "Epoch 1, Batch 600/53340, Loss: 3.5188\n",
      "Epoch 1, Batch 650/53340, Loss: 2.7611\n",
      "Epoch 1, Batch 700/53340, Loss: 1.4283\n",
      "Epoch 1, Batch 750/53340, Loss: 1.5851\n",
      "Epoch 1, Batch 800/53340, Loss: 3.8043\n",
      "Epoch 1, Batch 850/53340, Loss: 3.7350\n",
      "Epoch 1, Batch 900/53340, Loss: 4.6240\n",
      "Epoch 1, Batch 950/53340, Loss: 1.6796\n",
      "Epoch 1, Batch 1000/53340, Loss: 1.9512\n",
      "Epoch 1, Batch 1050/53340, Loss: 3.1117\n",
      "Epoch 1, Batch 1100/53340, Loss: 2.6350\n",
      "Epoch 1, Batch 1150/53340, Loss: 1.6793\n",
      "Epoch 1, Batch 1200/53340, Loss: 2.7677\n",
      "Epoch 1, Batch 1250/53340, Loss: 4.1652\n",
      "Epoch 1, Batch 1300/53340, Loss: 2.5762\n",
      "Epoch 1, Batch 1350/53340, Loss: 1.5226\n",
      "Epoch 1, Batch 1400/53340, Loss: 2.5535\n",
      "Epoch 1, Batch 1450/53340, Loss: 1.9359\n",
      "Epoch 1, Batch 1500/53340, Loss: 2.9430\n",
      "Epoch 1, Batch 1550/53340, Loss: 2.9674\n",
      "Epoch 1, Batch 1600/53340, Loss: 1.9278\n",
      "Epoch 1, Batch 1650/53340, Loss: 2.4179\n",
      "Epoch 1, Batch 1700/53340, Loss: 2.9265\n",
      "Epoch 1, Batch 1750/53340, Loss: 2.3415\n",
      "Epoch 1, Batch 1800/53340, Loss: 1.6025\n",
      "Epoch 1, Batch 1850/53340, Loss: 2.9034\n",
      "Epoch 1, Batch 1900/53340, Loss: 2.4630\n",
      "Epoch 1, Batch 1950/53340, Loss: 1.6777\n",
      "Epoch 1, Batch 2000/53340, Loss: 4.2362\n",
      "Epoch 1, Batch 2050/53340, Loss: 2.8294\n",
      "Epoch 1, Batch 2100/53340, Loss: 1.7070\n",
      "Epoch 1, Batch 2150/53340, Loss: 1.6025\n",
      "Epoch 1, Batch 2200/53340, Loss: 2.5420\n",
      "Epoch 1, Batch 2250/53340, Loss: 2.0361\n",
      "Epoch 1, Batch 2300/53340, Loss: 2.5034\n",
      "Epoch 1, Batch 2350/53340, Loss: 3.5496\n",
      "Epoch 1, Batch 2400/53340, Loss: 2.2958\n",
      "Epoch 1, Batch 2450/53340, Loss: 1.2548\n",
      "Epoch 1, Batch 2500/53340, Loss: 2.9036\n",
      "Epoch 1, Batch 2550/53340, Loss: 4.5305\n",
      "Epoch 1, Batch 2600/53340, Loss: 2.0163\n",
      "Epoch 1, Batch 2650/53340, Loss: 1.4625\n",
      "Epoch 1, Batch 2700/53340, Loss: 1.5782\n",
      "Epoch 1, Batch 2750/53340, Loss: 3.4097\n",
      "Epoch 1, Batch 2800/53340, Loss: 1.2263\n",
      "Epoch 1, Batch 2850/53340, Loss: 0.7949\n",
      "Epoch 1, Batch 2900/53340, Loss: 2.7153\n",
      "Epoch 1, Batch 2950/53340, Loss: 1.3601\n",
      "Epoch 1, Batch 3000/53340, Loss: 0.9757\n",
      "Epoch 1, Batch 3050/53340, Loss: 1.5599\n",
      "Epoch 1, Batch 3100/53340, Loss: 2.6446\n",
      "Epoch 1, Batch 3150/53340, Loss: 1.3542\n",
      "Epoch 1, Batch 3200/53340, Loss: 1.4449\n",
      "Epoch 1, Batch 3250/53340, Loss: 2.2145\n",
      "Epoch 1, Batch 3300/53340, Loss: 0.6877\n",
      "Epoch 1, Batch 3350/53340, Loss: 1.0835\n",
      "Epoch 1, Batch 3400/53340, Loss: 1.1353\n",
      "Epoch 1, Batch 3450/53340, Loss: 2.5129\n",
      "Epoch 1, Batch 3500/53340, Loss: 0.7236\n",
      "Epoch 1, Batch 3550/53340, Loss: 2.2077\n",
      "Epoch 1, Batch 3600/53340, Loss: 2.6613\n",
      "Epoch 1, Batch 3650/53340, Loss: 1.1775\n",
      "Epoch 1, Batch 3700/53340, Loss: 3.1215\n",
      "Epoch 1, Batch 3750/53340, Loss: 1.2655\n",
      "Epoch 1, Batch 3800/53340, Loss: 2.0246\n",
      "Epoch 1, Batch 3850/53340, Loss: 1.7009\n",
      "Epoch 1, Batch 3900/53340, Loss: 0.0716\n",
      "Epoch 1, Batch 3950/53340, Loss: 1.2244\n",
      "Epoch 1, Batch 4000/53340, Loss: 2.4882\n",
      "Epoch 1, Batch 4050/53340, Loss: 1.3278\n",
      "Epoch 1, Batch 4100/53340, Loss: 0.4966\n",
      "Epoch 1, Batch 4150/53340, Loss: 2.2306\n",
      "Epoch 1, Batch 4200/53340, Loss: 0.4556\n",
      "Epoch 1, Batch 4250/53340, Loss: 1.3275\n",
      "Epoch 1, Batch 4300/53340, Loss: 2.3059\n",
      "Epoch 1, Batch 4350/53340, Loss: 1.5001\n",
      "Epoch 1, Batch 4400/53340, Loss: 2.2191\n",
      "Epoch 1, Batch 4450/53340, Loss: 2.1433\n",
      "Epoch 1, Batch 4500/53340, Loss: 1.5357\n",
      "Epoch 1, Batch 4550/53340, Loss: 1.1547\n",
      "Epoch 1, Batch 4600/53340, Loss: 1.8472\n",
      "Epoch 1, Batch 4650/53340, Loss: 1.8262\n",
      "Epoch 1, Batch 4700/53340, Loss: 1.1675\n",
      "Epoch 1, Batch 4750/53340, Loss: 1.5818\n",
      "Epoch 1, Batch 4800/53340, Loss: 0.7990\n",
      "Epoch 1, Batch 4850/53340, Loss: 1.1527\n",
      "Epoch 1, Batch 4900/53340, Loss: 1.4195\n",
      "Epoch 1, Batch 4950/53340, Loss: 0.5007\n",
      "Epoch 1, Batch 5000/53340, Loss: 2.3238\n",
      "Epoch 1, Batch 5050/53340, Loss: 1.1357\n",
      "Epoch 1, Batch 5100/53340, Loss: 0.6070\n",
      "Epoch 1, Batch 5150/53340, Loss: 1.4888\n",
      "Epoch 1, Batch 5200/53340, Loss: 0.7003\n",
      "Epoch 1, Batch 5250/53340, Loss: 1.1698\n",
      "Epoch 1, Batch 5300/53340, Loss: 1.9846\n",
      "Epoch 1, Batch 5350/53340, Loss: 2.2642\n",
      "Epoch 1, Batch 5400/53340, Loss: 2.1551\n",
      "Epoch 1, Batch 5450/53340, Loss: 1.3287\n",
      "Epoch 1, Batch 5500/53340, Loss: 1.9190\n",
      "Epoch 1, Batch 5550/53340, Loss: 1.2034\n",
      "Epoch 1, Batch 5600/53340, Loss: 2.2771\n",
      "Epoch 1, Batch 5650/53340, Loss: 0.5617\n",
      "Epoch 1, Batch 5700/53340, Loss: 2.1435\n",
      "Epoch 1, Batch 5750/53340, Loss: 1.2787\n",
      "Epoch 1, Batch 5800/53340, Loss: 0.5701\n",
      "Epoch 1, Batch 5850/53340, Loss: 1.3739\n",
      "Epoch 1, Batch 5900/53340, Loss: 0.6982\n",
      "Epoch 1, Batch 5950/53340, Loss: 0.8458\n",
      "Epoch 1, Batch 6000/53340, Loss: 0.3321\n",
      "Epoch 1, Batch 6050/53340, Loss: 1.2827\n",
      "Epoch 1, Batch 6100/53340, Loss: 4.5815\n",
      "Epoch 1, Batch 6150/53340, Loss: 1.2587\n",
      "Epoch 1, Batch 6200/53340, Loss: 1.1308\n",
      "Epoch 1, Batch 6250/53340, Loss: 1.2115\n",
      "Epoch 1, Batch 6300/53340, Loss: 0.5881\n",
      "Epoch 1, Batch 6350/53340, Loss: 0.5179\n",
      "Epoch 1, Batch 6400/53340, Loss: 0.6202\n",
      "Epoch 1, Batch 6450/53340, Loss: 1.3102\n",
      "Epoch 1, Batch 6500/53340, Loss: 1.6259\n",
      "Epoch 1, Batch 6550/53340, Loss: 1.4848\n",
      "Epoch 1, Batch 6600/53340, Loss: 1.3205\n",
      "Epoch 1, Batch 6650/53340, Loss: 1.0636\n",
      "Epoch 1, Batch 6700/53340, Loss: 0.8404\n",
      "Epoch 1, Batch 6750/53340, Loss: 1.2545\n",
      "Epoch 1, Batch 6800/53340, Loss: 2.1887\n",
      "Epoch 1, Batch 6850/53340, Loss: 1.3602\n",
      "Epoch 1, Batch 6900/53340, Loss: 2.1351\n",
      "Epoch 1, Batch 6950/53340, Loss: 1.0350\n",
      "Epoch 1, Batch 7000/53340, Loss: 2.9046\n",
      "Epoch 1, Batch 7050/53340, Loss: 1.4512\n",
      "Epoch 1, Batch 7100/53340, Loss: 1.0120\n",
      "Epoch 1, Batch 7150/53340, Loss: 0.8651\n",
      "Epoch 1, Batch 7200/53340, Loss: 3.6569\n",
      "Epoch 1, Batch 7250/53340, Loss: 0.4991\n",
      "Epoch 1, Batch 7300/53340, Loss: 1.6924\n",
      "Epoch 1, Batch 7350/53340, Loss: 0.6601\n",
      "Epoch 1, Batch 7400/53340, Loss: 0.8837\n",
      "Epoch 1, Batch 7450/53340, Loss: 1.3534\n",
      "Epoch 1, Batch 7500/53340, Loss: 1.6060\n",
      "Epoch 1, Batch 7550/53340, Loss: 0.5787\n",
      "Epoch 1, Batch 7600/53340, Loss: 1.0360\n",
      "Epoch 1, Batch 7650/53340, Loss: 0.9543\n",
      "Epoch 1, Batch 7700/53340, Loss: 1.4129\n",
      "Epoch 1, Batch 7750/53340, Loss: 2.3227\n",
      "Epoch 1, Batch 7800/53340, Loss: 0.9991\n",
      "Epoch 1, Batch 7850/53340, Loss: 1.4699\n",
      "Epoch 1, Batch 7900/53340, Loss: 2.4734\n",
      "Epoch 1, Batch 7950/53340, Loss: 2.0343\n",
      "Epoch 1, Batch 8000/53340, Loss: 1.5833\n",
      "Epoch 1, Batch 8050/53340, Loss: 0.0367\n",
      "Epoch 1, Batch 8100/53340, Loss: 1.2091\n",
      "Epoch 1, Batch 8150/53340, Loss: 0.8700\n",
      "Epoch 1, Batch 8200/53340, Loss: 1.2075\n",
      "Epoch 1, Batch 8250/53340, Loss: 1.2679\n",
      "Epoch 1, Batch 8300/53340, Loss: 1.8978\n",
      "Epoch 1, Batch 8350/53340, Loss: 2.9176\n",
      "Epoch 1, Batch 8400/53340, Loss: 0.3554\n",
      "Epoch 1, Batch 8450/53340, Loss: 0.0277\n",
      "Epoch 1, Batch 8500/53340, Loss: 1.3348\n",
      "Epoch 1, Batch 8550/53340, Loss: 0.5567\n",
      "Epoch 1, Batch 8600/53340, Loss: 0.1034\n",
      "Epoch 1, Batch 8650/53340, Loss: 1.1107\n",
      "Epoch 1, Batch 8700/53340, Loss: 0.9464\n",
      "Epoch 1, Batch 8750/53340, Loss: 0.9548\n",
      "Epoch 1, Batch 8800/53340, Loss: 0.5502\n",
      "Epoch 1, Batch 8850/53340, Loss: 0.4644\n",
      "Epoch 1, Batch 8900/53340, Loss: 0.6458\n",
      "Epoch 1, Batch 8950/53340, Loss: 0.3261\n",
      "Epoch 1, Batch 9000/53340, Loss: 0.6272\n",
      "Epoch 1, Batch 9050/53340, Loss: 0.9521\n",
      "Epoch 1, Batch 9100/53340, Loss: 0.9872\n",
      "Epoch 1, Batch 9150/53340, Loss: 0.3891\n",
      "Epoch 1, Batch 9200/53340, Loss: 1.8016\n",
      "Epoch 1, Batch 9250/53340, Loss: 0.7657\n",
      "Epoch 1, Batch 9300/53340, Loss: 1.1179\n",
      "Epoch 1, Batch 9350/53340, Loss: 0.9324\n",
      "Epoch 1, Batch 9400/53340, Loss: 1.6771\n",
      "Epoch 1, Batch 9450/53340, Loss: 0.7805\n",
      "Epoch 1, Batch 9500/53340, Loss: 1.1223\n",
      "Epoch 1, Batch 9550/53340, Loss: 0.4177\n",
      "Epoch 1, Batch 9600/53340, Loss: 0.3145\n",
      "Epoch 1, Batch 9650/53340, Loss: 2.3902\n",
      "Epoch 1, Batch 9700/53340, Loss: 1.2563\n",
      "Epoch 1, Batch 9750/53340, Loss: 0.9116\n",
      "Epoch 1, Batch 9800/53340, Loss: 1.1745\n",
      "Epoch 1, Batch 9850/53340, Loss: 0.4199\n",
      "Epoch 1, Batch 9900/53340, Loss: 0.8165\n",
      "Epoch 1, Batch 9950/53340, Loss: 0.6828\n",
      "Epoch 1, Batch 10000/53340, Loss: 1.4342\n",
      "Epoch 1, Batch 10050/53340, Loss: 0.7850\n",
      "Epoch 1, Batch 10100/53340, Loss: 0.1471\n",
      "Epoch 1, Batch 10150/53340, Loss: 0.4744\n",
      "Epoch 1, Batch 10200/53340, Loss: 0.3413\n",
      "Epoch 1, Batch 10250/53340, Loss: 0.5461\n",
      "Epoch 1, Batch 10300/53340, Loss: 1.2749\n",
      "Epoch 1, Batch 10350/53340, Loss: 0.6602\n",
      "Epoch 1, Batch 10400/53340, Loss: 1.5901\n",
      "Epoch 1, Batch 10450/53340, Loss: 0.5696\n",
      "Epoch 1, Batch 10500/53340, Loss: 0.1184\n",
      "Epoch 1, Batch 10550/53340, Loss: 1.3814\n",
      "Epoch 1, Batch 10600/53340, Loss: 0.4428\n",
      "Epoch 1, Batch 10650/53340, Loss: 1.3689\n",
      "Epoch 1, Batch 10700/53340, Loss: 1.2502\n",
      "Epoch 1, Batch 10750/53340, Loss: 1.0463\n",
      "Epoch 1, Batch 10800/53340, Loss: 0.6702\n",
      "Epoch 1, Batch 10850/53340, Loss: 0.4867\n",
      "Epoch 1, Batch 10900/53340, Loss: 2.0893\n",
      "Epoch 1, Batch 10950/53340, Loss: 0.5947\n",
      "Epoch 1, Batch 11000/53340, Loss: 1.7497\n",
      "Epoch 1, Batch 11050/53340, Loss: 0.6424\n",
      "Epoch 1, Batch 11100/53340, Loss: 0.4881\n",
      "Epoch 1, Batch 11150/53340, Loss: 0.9871\n",
      "Epoch 1, Batch 11200/53340, Loss: 0.9855\n",
      "Epoch 1, Batch 11250/53340, Loss: 0.5059\n",
      "Epoch 1, Batch 11300/53340, Loss: 1.6204\n",
      "Epoch 1, Batch 11350/53340, Loss: 1.0875\n",
      "Epoch 1, Batch 11400/53340, Loss: 0.8763\n",
      "Epoch 1, Batch 11450/53340, Loss: 0.9062\n",
      "Epoch 1, Batch 11500/53340, Loss: 0.9317\n",
      "Epoch 1, Batch 11550/53340, Loss: 0.9158\n",
      "Epoch 1, Batch 11600/53340, Loss: 0.6073\n",
      "Epoch 1, Batch 11650/53340, Loss: 0.7228\n",
      "Epoch 1, Batch 11700/53340, Loss: 1.4553\n",
      "Epoch 1, Batch 11750/53340, Loss: 1.0778\n",
      "Epoch 1, Batch 11800/53340, Loss: 1.2975\n",
      "Epoch 1, Batch 11850/53340, Loss: 0.1876\n",
      "Epoch 1, Batch 11900/53340, Loss: 1.6047\n",
      "Epoch 1, Batch 11950/53340, Loss: 0.8061\n",
      "Epoch 1, Batch 12000/53340, Loss: 0.6341\n",
      "Epoch 1, Batch 12050/53340, Loss: 0.5152\n",
      "Epoch 1, Batch 12100/53340, Loss: 1.8405\n",
      "Epoch 1, Batch 12150/53340, Loss: 0.1211\n",
      "Epoch 1, Batch 12200/53340, Loss: 0.2913\n",
      "Epoch 1, Batch 12250/53340, Loss: 1.2268\n",
      "Epoch 1, Batch 12300/53340, Loss: 1.4824\n",
      "Epoch 1, Batch 12350/53340, Loss: 0.8932\n",
      "Epoch 1, Batch 12400/53340, Loss: 2.0484\n",
      "Epoch 1, Batch 12450/53340, Loss: 1.5461\n",
      "Epoch 1, Batch 12500/53340, Loss: 0.2282\n",
      "Epoch 1, Batch 12550/53340, Loss: 0.4255\n",
      "Epoch 1, Batch 12600/53340, Loss: 0.5266\n",
      "Epoch 1, Batch 12650/53340, Loss: 0.6227\n",
      "Epoch 1, Batch 12700/53340, Loss: 0.4438\n",
      "Epoch 1, Batch 12750/53340, Loss: 0.0484\n",
      "Epoch 1, Batch 12800/53340, Loss: 0.8350\n",
      "Epoch 1, Batch 12850/53340, Loss: 0.4122\n",
      "Epoch 1, Batch 12900/53340, Loss: 0.2404\n",
      "Epoch 1, Batch 12950/53340, Loss: 1.2033\n",
      "Epoch 1, Batch 13000/53340, Loss: 1.4789\n",
      "Epoch 1, Batch 13050/53340, Loss: 1.6406\n",
      "Epoch 1, Batch 13100/53340, Loss: 0.4088\n",
      "Epoch 1, Batch 13150/53340, Loss: 1.2439\n",
      "Epoch 1, Batch 13200/53340, Loss: 0.9485\n",
      "Epoch 1, Batch 13250/53340, Loss: 1.4715\n",
      "Epoch 1, Batch 13300/53340, Loss: 0.0897\n",
      "Epoch 1, Batch 13350/53340, Loss: 0.2053\n",
      "Epoch 1, Batch 13400/53340, Loss: 0.9929\n",
      "Epoch 1, Batch 13450/53340, Loss: 1.0165\n",
      "Epoch 1, Batch 13500/53340, Loss: 0.3466\n",
      "Epoch 1, Batch 13550/53340, Loss: 1.5998\n",
      "Epoch 1, Batch 13600/53340, Loss: 0.3508\n",
      "Epoch 1, Batch 13650/53340, Loss: 0.2693\n",
      "Epoch 1, Batch 13700/53340, Loss: 0.3582\n",
      "Epoch 1, Batch 13750/53340, Loss: 1.5677\n",
      "Epoch 1, Batch 13800/53340, Loss: 0.3836\n",
      "Epoch 1, Batch 13850/53340, Loss: 0.9631\n",
      "Epoch 1, Batch 13900/53340, Loss: 1.0768\n",
      "Epoch 1, Batch 13950/53340, Loss: 1.7340\n",
      "Epoch 1, Batch 14000/53340, Loss: 1.1718\n",
      "Epoch 1, Batch 14050/53340, Loss: 4.0119\n",
      "Epoch 1, Batch 14100/53340, Loss: 0.1903\n",
      "Epoch 1, Batch 14150/53340, Loss: 1.5390\n",
      "Epoch 1, Batch 14200/53340, Loss: 0.9878\n",
      "Epoch 1, Batch 14250/53340, Loss: 0.2538\n",
      "Epoch 1, Batch 14300/53340, Loss: 0.9270\n",
      "Epoch 1, Batch 14350/53340, Loss: 0.9740\n",
      "Epoch 1, Batch 14400/53340, Loss: 0.7133\n",
      "Epoch 1, Batch 14450/53340, Loss: 0.5152\n",
      "Epoch 1, Batch 14500/53340, Loss: 0.1452\n",
      "Epoch 1, Batch 14550/53340, Loss: 1.1872\n",
      "Epoch 1, Batch 14600/53340, Loss: 0.5300\n",
      "Epoch 1, Batch 14650/53340, Loss: 0.6765\n",
      "Epoch 1, Batch 14700/53340, Loss: 1.1060\n",
      "Epoch 1, Batch 14750/53340, Loss: 1.1992\n",
      "Epoch 1, Batch 14800/53340, Loss: 0.6866\n",
      "Epoch 1, Batch 14850/53340, Loss: 1.7905\n",
      "Epoch 1, Batch 14900/53340, Loss: 2.5678\n",
      "Epoch 1, Batch 14950/53340, Loss: 1.0976\n",
      "Epoch 1, Batch 15000/53340, Loss: 0.5751\n",
      "Epoch 1, Batch 15050/53340, Loss: 0.5379\n",
      "Epoch 1, Batch 15100/53340, Loss: 0.4995\n",
      "Epoch 1, Batch 15150/53340, Loss: 2.4725\n",
      "Epoch 1, Batch 15200/53340, Loss: 1.3562\n",
      "Epoch 1, Batch 15250/53340, Loss: 0.5891\n",
      "Epoch 1, Batch 15300/53340, Loss: 1.5075\n",
      "Epoch 1, Batch 15350/53340, Loss: 0.6517\n",
      "Epoch 1, Batch 15400/53340, Loss: 1.1417\n",
      "Epoch 1, Batch 15450/53340, Loss: 0.2633\n",
      "Epoch 1, Batch 15500/53340, Loss: 1.1129\n",
      "Epoch 1, Batch 15550/53340, Loss: 0.3920\n",
      "Epoch 1, Batch 15600/53340, Loss: 0.7219\n",
      "Epoch 1, Batch 15650/53340, Loss: 1.3853\n",
      "Epoch 1, Batch 15700/53340, Loss: 1.0284\n",
      "Epoch 1, Batch 15750/53340, Loss: 0.9885\n",
      "Epoch 1, Batch 15800/53340, Loss: 1.5438\n",
      "Epoch 1, Batch 15850/53340, Loss: 0.6841\n",
      "Epoch 1, Batch 15900/53340, Loss: 0.7801\n",
      "Epoch 1, Batch 15950/53340, Loss: 1.4416\n",
      "Epoch 1, Batch 16000/53340, Loss: 2.6384\n",
      "Epoch 1, Batch 16050/53340, Loss: 1.5868\n",
      "Epoch 1, Batch 16100/53340, Loss: 1.3041\n",
      "Epoch 1, Batch 16150/53340, Loss: 0.0708\n",
      "Epoch 1, Batch 16200/53340, Loss: 0.3777\n",
      "Epoch 1, Batch 16250/53340, Loss: 0.8570\n",
      "Epoch 1, Batch 16300/53340, Loss: 0.0730\n",
      "Epoch 1, Batch 16350/53340, Loss: 1.0573\n",
      "Epoch 1, Batch 16400/53340, Loss: 0.0313\n",
      "Epoch 1, Batch 16450/53340, Loss: 0.4501\n",
      "Epoch 1, Batch 16500/53340, Loss: 0.7440\n",
      "Epoch 1, Batch 16550/53340, Loss: 0.8031\n",
      "Epoch 1, Batch 16600/53340, Loss: 0.8681\n",
      "Epoch 1, Batch 16650/53340, Loss: 0.5004\n",
      "Epoch 1, Batch 16700/53340, Loss: 0.9095\n",
      "Epoch 1, Batch 16750/53340, Loss: 0.2983\n",
      "Epoch 1, Batch 16800/53340, Loss: 0.6041\n",
      "Epoch 1, Batch 16850/53340, Loss: 0.8135\n",
      "Epoch 1, Batch 16900/53340, Loss: 0.4491\n",
      "Epoch 1, Batch 16950/53340, Loss: 0.8669\n",
      "Epoch 1, Batch 17000/53340, Loss: 1.8144\n",
      "Epoch 1, Batch 17050/53340, Loss: 1.2044\n",
      "Epoch 1, Batch 17100/53340, Loss: 0.0184\n",
      "Epoch 1, Batch 17150/53340, Loss: 1.8196\n",
      "Epoch 1, Batch 17200/53340, Loss: 1.2020\n",
      "Epoch 1, Batch 17250/53340, Loss: 0.1308\n",
      "Epoch 1, Batch 17300/53340, Loss: 0.3670\n",
      "Epoch 1, Batch 17350/53340, Loss: 0.6866\n",
      "Epoch 1, Batch 17400/53340, Loss: 1.5235\n",
      "Epoch 1, Batch 17450/53340, Loss: 0.2195\n",
      "Epoch 1, Batch 17500/53340, Loss: 0.5291\n",
      "Epoch 1, Batch 17550/53340, Loss: 1.2003\n",
      "Epoch 1, Batch 17600/53340, Loss: 0.6097\n",
      "Epoch 1, Batch 17650/53340, Loss: 0.4311\n",
      "Epoch 1, Batch 17700/53340, Loss: 0.4158\n",
      "Epoch 1, Batch 17750/53340, Loss: 1.0194\n",
      "Epoch 1, Batch 17800/53340, Loss: 0.0562\n",
      "Epoch 1, Batch 17850/53340, Loss: 0.5727\n",
      "Epoch 1, Batch 17900/53340, Loss: 0.0874\n",
      "Epoch 1, Batch 17950/53340, Loss: 1.4699\n",
      "Epoch 1, Batch 18000/53340, Loss: 0.3158\n",
      "Epoch 1, Batch 18050/53340, Loss: 0.0131\n",
      "Epoch 1, Batch 18100/53340, Loss: 0.5937\n",
      "Epoch 1, Batch 18150/53340, Loss: 0.2347\n",
      "Epoch 1, Batch 18200/53340, Loss: 0.6154\n",
      "Epoch 1, Batch 18250/53340, Loss: 0.3974\n",
      "Epoch 1, Batch 18300/53340, Loss: 0.3760\n",
      "Epoch 1, Batch 18350/53340, Loss: 0.9854\n",
      "Epoch 1, Batch 18400/53340, Loss: 0.2463\n",
      "Epoch 1, Batch 18450/53340, Loss: 0.3542\n",
      "Epoch 1, Batch 18500/53340, Loss: 1.1854\n",
      "Epoch 1, Batch 18550/53340, Loss: 0.3414\n",
      "Epoch 1, Batch 18600/53340, Loss: 1.6013\n",
      "Epoch 1, Batch 18650/53340, Loss: 0.9213\n",
      "Epoch 1, Batch 18700/53340, Loss: 1.4731\n",
      "Epoch 1, Batch 18750/53340, Loss: 0.2633\n",
      "Epoch 1, Batch 18800/53340, Loss: 0.4467\n",
      "Epoch 1, Batch 18850/53340, Loss: 0.5352\n",
      "Epoch 1, Batch 18900/53340, Loss: 1.3611\n",
      "Epoch 1, Batch 18950/53340, Loss: 0.9098\n",
      "Epoch 1, Batch 19000/53340, Loss: 0.2112\n",
      "Epoch 1, Batch 19050/53340, Loss: 0.5507\n",
      "Epoch 1, Batch 19100/53340, Loss: 0.5356\n",
      "Epoch 1, Batch 19150/53340, Loss: 0.1980\n",
      "Epoch 1, Batch 19200/53340, Loss: 0.5317\n",
      "Epoch 1, Batch 19250/53340, Loss: 0.6208\n",
      "Epoch 1, Batch 19300/53340, Loss: 1.8385\n",
      "Epoch 1, Batch 19350/53340, Loss: 0.5616\n",
      "Epoch 1, Batch 19400/53340, Loss: 0.2179\n",
      "Epoch 1, Batch 19450/53340, Loss: 0.2373\n",
      "Epoch 1, Batch 19500/53340, Loss: 0.2360\n",
      "Epoch 1, Batch 19550/53340, Loss: 1.5512\n",
      "Epoch 1, Batch 19600/53340, Loss: 0.4030\n",
      "Epoch 1, Batch 19650/53340, Loss: 0.1019\n",
      "Epoch 1, Batch 19700/53340, Loss: 2.3856\n",
      "Epoch 1, Batch 19750/53340, Loss: 0.3788\n",
      "Epoch 1, Batch 19800/53340, Loss: 0.3741\n",
      "Epoch 1, Batch 19850/53340, Loss: 0.2808\n",
      "Epoch 1, Batch 19900/53340, Loss: 0.2338\n",
      "Epoch 1, Batch 19950/53340, Loss: 1.1350\n",
      "Epoch 1, Batch 20000/53340, Loss: 0.6189\n",
      "Epoch 1, Batch 20050/53340, Loss: 0.1971\n",
      "Epoch 1, Batch 20100/53340, Loss: 0.3541\n",
      "Epoch 1, Batch 20150/53340, Loss: 1.7384\n",
      "Epoch 1, Batch 20200/53340, Loss: 1.1632\n",
      "Epoch 1, Batch 20250/53340, Loss: 0.2843\n",
      "Epoch 1, Batch 20300/53340, Loss: 0.2582\n",
      "Epoch 1, Batch 20350/53340, Loss: 0.6953\n",
      "Epoch 1, Batch 20400/53340, Loss: 0.6468\n",
      "Epoch 1, Batch 20450/53340, Loss: 0.7525\n",
      "Epoch 1, Batch 20500/53340, Loss: 0.3211\n",
      "Epoch 1, Batch 20550/53340, Loss: 0.4399\n",
      "Epoch 1, Batch 20600/53340, Loss: 0.4859\n",
      "Epoch 1, Batch 20650/53340, Loss: 0.8332\n",
      "Epoch 1, Batch 20700/53340, Loss: 0.7700\n",
      "Epoch 1, Batch 20750/53340, Loss: 0.2869\n",
      "Epoch 1, Batch 20800/53340, Loss: 0.0875\n",
      "Epoch 1, Batch 20850/53340, Loss: 1.6620\n",
      "Epoch 1, Batch 20900/53340, Loss: 0.4653\n",
      "Epoch 1, Batch 20950/53340, Loss: 0.7290\n",
      "Epoch 1, Batch 21000/53340, Loss: 0.4622\n",
      "Epoch 1, Batch 21050/53340, Loss: 0.3720\n",
      "Epoch 1, Batch 21100/53340, Loss: 0.4829\n",
      "Epoch 1, Batch 21150/53340, Loss: 0.6879\n",
      "Epoch 1, Batch 21200/53340, Loss: 0.1538\n",
      "Epoch 1, Batch 21250/53340, Loss: 1.1961\n",
      "Epoch 1, Batch 21300/53340, Loss: 0.1208\n",
      "Epoch 1, Batch 21350/53340, Loss: 0.5927\n",
      "Epoch 1, Batch 21400/53340, Loss: 3.5301\n",
      "Epoch 1, Batch 21450/53340, Loss: 0.3944\n",
      "Epoch 1, Batch 21500/53340, Loss: 0.3886\n",
      "Epoch 1, Batch 21550/53340, Loss: 0.8142\n",
      "Epoch 1, Batch 21600/53340, Loss: 0.8052\n",
      "Epoch 1, Batch 21650/53340, Loss: 0.0516\n",
      "Epoch 1, Batch 21700/53340, Loss: 0.1978\n",
      "Epoch 1, Batch 21750/53340, Loss: 0.0629\n",
      "Epoch 1, Batch 21800/53340, Loss: 0.8278\n",
      "Epoch 1, Batch 21850/53340, Loss: 5.0472\n",
      "Epoch 1, Batch 21900/53340, Loss: 0.2394\n",
      "Epoch 1, Batch 21950/53340, Loss: 0.7366\n",
      "Epoch 1, Batch 22000/53340, Loss: 0.3866\n",
      "Epoch 1, Batch 22050/53340, Loss: 0.5089\n",
      "Epoch 1, Batch 22100/53340, Loss: 0.4378\n",
      "Epoch 1, Batch 22150/53340, Loss: 0.0312\n",
      "Epoch 1, Batch 22200/53340, Loss: 0.9296\n",
      "Epoch 1, Batch 22250/53340, Loss: 0.5900\n",
      "Epoch 1, Batch 22300/53340, Loss: 0.1683\n",
      "Epoch 1, Batch 22350/53340, Loss: 1.1280\n",
      "Epoch 1, Batch 22400/53340, Loss: 0.1753\n",
      "Epoch 1, Batch 22450/53340, Loss: 0.1353\n",
      "Epoch 1, Batch 22500/53340, Loss: 1.2053\n",
      "Epoch 1, Batch 22550/53340, Loss: 0.0848\n",
      "Epoch 1, Batch 22600/53340, Loss: 0.2659\n",
      "Epoch 1, Batch 22650/53340, Loss: 1.8131\n",
      "Epoch 1, Batch 22700/53340, Loss: 0.2464\n",
      "Epoch 1, Batch 22750/53340, Loss: 0.6428\n",
      "Epoch 1, Batch 22800/53340, Loss: 0.4423\n",
      "Epoch 1, Batch 22850/53340, Loss: 0.4807\n",
      "Epoch 1, Batch 22900/53340, Loss: 1.4491\n",
      "Epoch 1, Batch 22950/53340, Loss: 0.6090\n",
      "Epoch 1, Batch 23000/53340, Loss: 0.7958\n",
      "Epoch 1, Batch 23050/53340, Loss: 0.2546\n",
      "Epoch 1, Batch 23100/53340, Loss: 0.3955\n",
      "Epoch 1, Batch 23150/53340, Loss: 0.2527\n",
      "Epoch 1, Batch 23200/53340, Loss: 0.1191\n",
      "Epoch 1, Batch 23250/53340, Loss: 0.0801\n",
      "Epoch 1, Batch 23300/53340, Loss: 0.6086\n",
      "Epoch 1, Batch 23350/53340, Loss: 0.1836\n",
      "Epoch 1, Batch 23400/53340, Loss: 0.3981\n",
      "Epoch 1, Batch 23450/53340, Loss: 0.9155\n",
      "Epoch 1, Batch 23500/53340, Loss: 0.1837\n",
      "Epoch 1, Batch 23550/53340, Loss: 0.4142\n",
      "Epoch 1, Batch 23600/53340, Loss: 0.1469\n",
      "Epoch 1, Batch 23650/53340, Loss: 0.2978\n",
      "Epoch 1, Batch 23700/53340, Loss: 0.0366\n",
      "Epoch 1, Batch 23750/53340, Loss: 0.1307\n",
      "Epoch 1, Batch 23800/53340, Loss: 0.0150\n",
      "Epoch 1, Batch 23850/53340, Loss: 0.5787\n",
      "Epoch 1, Batch 23900/53340, Loss: 0.6429\n",
      "Epoch 1, Batch 23950/53340, Loss: 0.4032\n",
      "Epoch 1, Batch 24000/53340, Loss: 0.5549\n",
      "Epoch 1, Batch 24050/53340, Loss: 0.4174\n",
      "Epoch 1, Batch 24100/53340, Loss: 0.4725\n",
      "Epoch 1, Batch 24150/53340, Loss: 0.1744\n",
      "Epoch 1, Batch 24200/53340, Loss: 0.2604\n",
      "Epoch 1, Batch 24250/53340, Loss: 1.0166\n",
      "Epoch 1, Batch 24300/53340, Loss: 0.5103\n",
      "Epoch 1, Batch 24350/53340, Loss: 0.7042\n",
      "Epoch 1, Batch 24400/53340, Loss: 0.8114\n",
      "Epoch 1, Batch 24450/53340, Loss: 0.2541\n",
      "Epoch 1, Batch 24500/53340, Loss: 0.1236\n",
      "Epoch 1, Batch 24550/53340, Loss: 0.0421\n",
      "Epoch 1, Batch 24600/53340, Loss: 0.4082\n",
      "Epoch 1, Batch 24650/53340, Loss: 0.3616\n",
      "Epoch 1, Batch 24700/53340, Loss: 0.2569\n",
      "Epoch 1, Batch 24750/53340, Loss: 0.3843\n",
      "Epoch 1, Batch 24800/53340, Loss: 0.9396\n",
      "Epoch 1, Batch 24850/53340, Loss: 0.0919\n",
      "Epoch 1, Batch 24900/53340, Loss: 0.4514\n",
      "Epoch 1, Batch 24950/53340, Loss: 0.2791\n",
      "Epoch 1, Batch 25000/53340, Loss: 0.4393\n",
      "Epoch 1, Batch 25050/53340, Loss: 0.3756\n",
      "Epoch 1, Batch 25100/53340, Loss: 1.0431\n",
      "Epoch 1, Batch 25150/53340, Loss: 0.6684\n",
      "Epoch 1, Batch 25200/53340, Loss: 1.0779\n",
      "Epoch 1, Batch 25250/53340, Loss: 0.0883\n",
      "Epoch 1, Batch 25300/53340, Loss: 0.3978\n",
      "Epoch 1, Batch 25350/53340, Loss: 0.5846\n",
      "Epoch 1, Batch 25400/53340, Loss: 0.3659\n",
      "Epoch 1, Batch 25450/53340, Loss: 0.0678\n",
      "Epoch 1, Batch 25500/53340, Loss: 0.9037\n",
      "Epoch 1, Batch 25550/53340, Loss: 0.0161\n",
      "Epoch 1, Batch 25600/53340, Loss: 1.2561\n",
      "Epoch 1, Batch 25650/53340, Loss: 0.4812\n",
      "Epoch 1, Batch 25700/53340, Loss: 0.6760\n",
      "Epoch 1, Batch 25750/53340, Loss: 0.5665\n",
      "Epoch 1, Batch 25800/53340, Loss: 0.0931\n",
      "Epoch 1, Batch 25850/53340, Loss: 0.4112\n",
      "Epoch 1, Batch 25900/53340, Loss: 1.0293\n",
      "Epoch 1, Batch 25950/53340, Loss: 0.3752\n",
      "Epoch 1, Batch 26000/53340, Loss: 1.6581\n",
      "Epoch 1, Batch 26050/53340, Loss: 0.1496\n",
      "Epoch 1, Batch 26100/53340, Loss: 0.1063\n",
      "Epoch 1, Batch 26150/53340, Loss: 0.7233\n",
      "Epoch 1, Batch 26200/53340, Loss: 0.3127\n",
      "Epoch 1, Batch 26250/53340, Loss: 0.4273\n",
      "Epoch 1, Batch 26300/53340, Loss: 0.5999\n",
      "Epoch 1, Batch 26350/53340, Loss: 0.7144\n",
      "Epoch 1, Batch 26400/53340, Loss: 0.4293\n",
      "Epoch 1, Batch 26450/53340, Loss: 0.3322\n",
      "Epoch 1, Batch 26500/53340, Loss: 0.3205\n",
      "Epoch 1, Batch 26550/53340, Loss: 0.0979\n",
      "Epoch 1, Batch 26600/53340, Loss: 0.4559\n",
      "Epoch 1, Batch 26650/53340, Loss: 0.2499\n",
      "Epoch 1, Batch 26700/53340, Loss: 0.2292\n",
      "Epoch 1, Batch 26750/53340, Loss: 1.2133\n",
      "Epoch 1, Batch 26800/53340, Loss: 0.3533\n",
      "Epoch 1, Batch 26850/53340, Loss: 0.0252\n",
      "Epoch 1, Batch 26900/53340, Loss: 0.0914\n",
      "Epoch 1, Batch 26950/53340, Loss: 0.3865\n",
      "Epoch 1, Batch 27000/53340, Loss: 0.8053\n",
      "Epoch 1, Batch 27050/53340, Loss: 0.0886\n",
      "Epoch 1, Batch 27100/53340, Loss: 0.3915\n",
      "Epoch 1, Batch 27150/53340, Loss: 0.2672\n",
      "Epoch 1, Batch 27200/53340, Loss: 0.0130\n",
      "Epoch 1, Batch 27250/53340, Loss: 0.1967\n",
      "Epoch 1, Batch 27300/53340, Loss: 0.1675\n",
      "Epoch 1, Batch 27350/53340, Loss: 0.1375\n",
      "Epoch 1, Batch 27400/53340, Loss: 0.2691\n",
      "Epoch 1, Batch 27450/53340, Loss: 0.0439\n",
      "Epoch 1, Batch 27500/53340, Loss: 0.3726\n",
      "Epoch 1, Batch 27550/53340, Loss: 0.1507\n",
      "Epoch 1, Batch 27600/53340, Loss: 0.4833\n",
      "Epoch 1, Batch 27650/53340, Loss: 0.3530\n",
      "Epoch 1, Batch 27700/53340, Loss: 0.1768\n",
      "Epoch 1, Batch 27750/53340, Loss: 0.2393\n",
      "Epoch 1, Batch 27800/53340, Loss: 0.5132\n",
      "Epoch 1, Batch 27850/53340, Loss: 0.1833\n",
      "Epoch 1, Batch 27900/53340, Loss: 0.4444\n",
      "Epoch 1, Batch 27950/53340, Loss: 0.0173\n",
      "Epoch 1, Batch 28000/53340, Loss: 0.5324\n",
      "Epoch 1, Batch 28050/53340, Loss: 0.8032\n",
      "Epoch 1, Batch 28100/53340, Loss: 0.1947\n",
      "Epoch 1, Batch 28150/53340, Loss: 1.0001\n",
      "Epoch 1, Batch 28200/53340, Loss: 1.1124\n",
      "Epoch 1, Batch 28250/53340, Loss: 0.2880\n",
      "Epoch 1, Batch 28300/53340, Loss: 0.2020\n",
      "Epoch 1, Batch 28350/53340, Loss: 0.3311\n",
      "Epoch 1, Batch 28400/53340, Loss: 0.5132\n",
      "Epoch 1, Batch 28450/53340, Loss: 0.3668\n",
      "Epoch 1, Batch 28500/53340, Loss: 0.5448\n",
      "Epoch 1, Batch 28550/53340, Loss: 0.1013\n",
      "Epoch 1, Batch 28600/53340, Loss: 0.7156\n",
      "Epoch 1, Batch 28650/53340, Loss: 0.0934\n",
      "Epoch 1, Batch 28700/53340, Loss: 0.4069\n",
      "Epoch 1, Batch 28750/53340, Loss: 0.5627\n",
      "Epoch 1, Batch 28800/53340, Loss: 0.1231\n",
      "Epoch 1, Batch 28850/53340, Loss: 1.4131\n",
      "Epoch 1, Batch 28900/53340, Loss: 0.3419\n",
      "Epoch 1, Batch 28950/53340, Loss: 0.1245\n",
      "Epoch 1, Batch 29000/53340, Loss: 0.1847\n",
      "Epoch 1, Batch 29050/53340, Loss: 1.0023\n",
      "Epoch 1, Batch 29100/53340, Loss: 0.3695\n",
      "Epoch 1, Batch 29150/53340, Loss: 0.1922\n",
      "Epoch 1, Batch 29200/53340, Loss: 0.5739\n",
      "Epoch 1, Batch 29250/53340, Loss: 0.0499\n",
      "Epoch 1, Batch 29300/53340, Loss: 0.0455\n",
      "Epoch 1, Batch 29350/53340, Loss: 0.8863\n",
      "Epoch 1, Batch 29400/53340, Loss: 0.0229\n",
      "Epoch 1, Batch 29450/53340, Loss: 0.2074\n",
      "Epoch 1, Batch 29500/53340, Loss: 0.2773\n",
      "Epoch 1, Batch 29550/53340, Loss: 0.2391\n",
      "Epoch 1, Batch 29600/53340, Loss: 0.0983\n",
      "Epoch 1, Batch 29650/53340, Loss: 0.0401\n",
      "Epoch 1, Batch 29700/53340, Loss: 0.4968\n",
      "Epoch 1, Batch 29750/53340, Loss: 0.1005\n",
      "Epoch 1, Batch 29800/53340, Loss: 0.4152\n",
      "Epoch 1, Batch 29850/53340, Loss: 0.5064\n",
      "Epoch 1, Batch 29900/53340, Loss: 0.4213\n",
      "Epoch 1, Batch 29950/53340, Loss: 1.0494\n",
      "Epoch 1, Batch 30000/53340, Loss: 0.8645\n",
      "Epoch 1, Batch 30050/53340, Loss: 0.0169\n",
      "Epoch 1, Batch 30100/53340, Loss: 0.3306\n",
      "Epoch 1, Batch 30150/53340, Loss: 0.4655\n",
      "Epoch 1, Batch 30200/53340, Loss: 0.3496\n",
      "Epoch 1, Batch 30250/53340, Loss: 0.0279\n",
      "Epoch 1, Batch 30300/53340, Loss: 0.0522\n",
      "Epoch 1, Batch 30350/53340, Loss: 0.0090\n",
      "Epoch 1, Batch 30400/53340, Loss: 0.2040\n",
      "Epoch 1, Batch 30450/53340, Loss: 0.2178\n",
      "Epoch 1, Batch 30500/53340, Loss: 0.2548\n",
      "Epoch 1, Batch 30550/53340, Loss: 0.2100\n",
      "Epoch 1, Batch 30600/53340, Loss: 0.3380\n",
      "Epoch 1, Batch 30650/53340, Loss: 0.6501\n",
      "Epoch 1, Batch 30700/53340, Loss: 0.3114\n",
      "Epoch 1, Batch 30750/53340, Loss: 0.6401\n",
      "Epoch 1, Batch 30800/53340, Loss: 1.9382\n",
      "Epoch 1, Batch 30850/53340, Loss: 0.0471\n",
      "Epoch 1, Batch 30900/53340, Loss: 0.3683\n",
      "Epoch 1, Batch 30950/53340, Loss: 0.1770\n",
      "Epoch 1, Batch 31000/53340, Loss: 2.3783\n",
      "Epoch 1, Batch 31050/53340, Loss: 0.4136\n",
      "Epoch 1, Batch 31100/53340, Loss: 0.0233\n",
      "Epoch 1, Batch 31150/53340, Loss: 0.2131\n",
      "Epoch 1, Batch 31200/53340, Loss: 1.0836\n",
      "Epoch 1, Batch 31250/53340, Loss: 0.1935\n",
      "Epoch 1, Batch 31300/53340, Loss: 0.8092\n",
      "Epoch 1, Batch 31350/53340, Loss: 2.6636\n",
      "Epoch 1, Batch 31400/53340, Loss: 0.0325\n",
      "Epoch 1, Batch 31450/53340, Loss: 0.6170\n",
      "Epoch 1, Batch 31500/53340, Loss: 0.0161\n",
      "Epoch 1, Batch 31550/53340, Loss: 0.7854\n",
      "Epoch 1, Batch 31600/53340, Loss: 0.0321\n",
      "Epoch 1, Batch 31650/53340, Loss: 0.6062\n",
      "Epoch 1, Batch 31700/53340, Loss: 0.6166\n",
      "Epoch 1, Batch 31750/53340, Loss: 0.1794\n",
      "Epoch 1, Batch 31800/53340, Loss: 0.0671\n",
      "Epoch 1, Batch 31850/53340, Loss: 0.2280\n",
      "Epoch 1, Batch 31900/53340, Loss: 0.1596\n",
      "Epoch 1, Batch 31950/53340, Loss: 0.2287\n",
      "Epoch 1, Batch 32000/53340, Loss: 0.1877\n",
      "Epoch 1, Batch 32050/53340, Loss: 0.3317\n",
      "Epoch 1, Batch 32100/53340, Loss: 2.0008\n",
      "Epoch 1, Batch 32150/53340, Loss: 0.0489\n",
      "Epoch 1, Batch 32200/53340, Loss: 0.3727\n",
      "Epoch 1, Batch 32250/53340, Loss: 0.7007\n",
      "Epoch 1, Batch 32300/53340, Loss: 0.5838\n",
      "Epoch 1, Batch 32350/53340, Loss: 0.4799\n",
      "Epoch 1, Batch 32400/53340, Loss: 0.2332\n",
      "Epoch 1, Batch 32450/53340, Loss: 0.4651\n",
      "Epoch 1, Batch 32500/53340, Loss: 0.6815\n",
      "Epoch 1, Batch 32550/53340, Loss: 0.3146\n",
      "Epoch 1, Batch 32600/53340, Loss: 1.4381\n",
      "Epoch 1, Batch 32650/53340, Loss: 0.3854\n",
      "Epoch 1, Batch 32700/53340, Loss: 0.0605\n",
      "Epoch 1, Batch 32750/53340, Loss: 0.2377\n",
      "Epoch 1, Batch 32800/53340, Loss: 0.3794\n",
      "Epoch 1, Batch 32850/53340, Loss: 0.1805\n",
      "Epoch 1, Batch 32900/53340, Loss: 0.2413\n",
      "Epoch 1, Batch 32950/53340, Loss: 1.4747\n",
      "Epoch 1, Batch 33000/53340, Loss: 0.3657\n",
      "Epoch 1, Batch 33050/53340, Loss: 0.0859\n",
      "Epoch 1, Batch 33100/53340, Loss: 0.4066\n",
      "Epoch 1, Batch 33150/53340, Loss: 0.6009\n",
      "Epoch 1, Batch 33200/53340, Loss: 0.1211\n",
      "Epoch 1, Batch 33250/53340, Loss: 0.4769\n",
      "Epoch 1, Batch 33300/53340, Loss: 0.0116\n",
      "Epoch 1, Batch 33350/53340, Loss: 0.3024\n",
      "Epoch 1, Batch 33400/53340, Loss: 0.2544\n",
      "Epoch 1, Batch 33450/53340, Loss: 0.6733\n",
      "Epoch 1, Batch 33500/53340, Loss: 0.0741\n",
      "Epoch 1, Batch 33550/53340, Loss: 0.6041\n",
      "Epoch 1, Batch 33600/53340, Loss: 0.1981\n",
      "Epoch 1, Batch 33650/53340, Loss: 1.1051\n",
      "Epoch 1, Batch 33700/53340, Loss: 0.1118\n",
      "Epoch 1, Batch 33750/53340, Loss: 0.0342\n",
      "Epoch 1, Batch 33800/53340, Loss: 0.0303\n",
      "Epoch 1, Batch 33850/53340, Loss: 0.2105\n",
      "Epoch 1, Batch 33900/53340, Loss: 0.3933\n",
      "Epoch 1, Batch 33950/53340, Loss: 1.4853\n",
      "Epoch 1, Batch 34000/53340, Loss: 0.0853\n",
      "Epoch 1, Batch 34050/53340, Loss: 0.5721\n",
      "Epoch 1, Batch 34100/53340, Loss: 0.4194\n",
      "Epoch 1, Batch 34150/53340, Loss: 0.1005\n",
      "Epoch 1, Batch 34200/53340, Loss: 0.0499\n",
      "Epoch 1, Batch 34250/53340, Loss: 0.3843\n",
      "Epoch 1, Batch 34300/53340, Loss: 0.1822\n",
      "Epoch 1, Batch 34350/53340, Loss: 0.0126\n",
      "Epoch 1, Batch 34400/53340, Loss: 0.3862\n",
      "Epoch 1, Batch 34450/53340, Loss: 0.1820\n",
      "Epoch 1, Batch 34500/53340, Loss: 0.7698\n",
      "Epoch 1, Batch 34550/53340, Loss: 0.0961\n",
      "Epoch 1, Batch 34600/53340, Loss: 0.6468\n",
      "Epoch 1, Batch 34650/53340, Loss: 2.5039\n",
      "Epoch 1, Batch 34700/53340, Loss: 0.4765\n",
      "Epoch 1, Batch 34750/53340, Loss: 0.1809\n",
      "Epoch 1, Batch 34800/53340, Loss: 0.5249\n",
      "Epoch 1, Batch 34850/53340, Loss: 0.2162\n",
      "Epoch 1, Batch 34900/53340, Loss: 0.7754\n",
      "Epoch 1, Batch 34950/53340, Loss: 0.7402\n",
      "Epoch 1, Batch 35000/53340, Loss: 1.7111\n",
      "Epoch 1, Batch 35050/53340, Loss: 0.4587\n",
      "Epoch 1, Batch 35100/53340, Loss: 0.2298\n",
      "Epoch 1, Batch 35150/53340, Loss: 1.1558\n",
      "Epoch 1, Batch 35200/53340, Loss: 0.0345\n",
      "Epoch 1, Batch 35250/53340, Loss: 0.0388\n",
      "Epoch 1, Batch 35300/53340, Loss: 0.7998\n",
      "Epoch 1, Batch 35350/53340, Loss: 0.1208\n",
      "Epoch 1, Batch 35400/53340, Loss: 0.1549\n",
      "Epoch 1, Batch 35450/53340, Loss: 0.0718\n",
      "Epoch 1, Batch 35500/53340, Loss: 1.8578\n",
      "Epoch 1, Batch 35550/53340, Loss: 0.0508\n",
      "Epoch 1, Batch 35600/53340, Loss: 0.1742\n",
      "Epoch 1, Batch 35650/53340, Loss: 1.1920\n",
      "Epoch 1, Batch 35700/53340, Loss: 0.1770\n",
      "Epoch 1, Batch 35750/53340, Loss: 0.4762\n",
      "Epoch 1, Batch 35800/53340, Loss: 0.2684\n",
      "Epoch 1, Batch 35850/53340, Loss: 0.1853\n",
      "Epoch 1, Batch 35900/53340, Loss: 0.2995\n",
      "Epoch 1, Batch 35950/53340, Loss: 0.0447\n",
      "Epoch 1, Batch 36000/53340, Loss: 0.0220\n",
      "Epoch 1, Batch 36050/53340, Loss: 0.1841\n",
      "Epoch 1, Batch 36100/53340, Loss: 0.1963\n",
      "Epoch 1, Batch 36150/53340, Loss: 0.7852\n",
      "Epoch 1, Batch 36200/53340, Loss: 0.4621\n",
      "Epoch 1, Batch 36250/53340, Loss: 0.6074\n",
      "Epoch 1, Batch 36300/53340, Loss: 0.1249\n",
      "Epoch 1, Batch 36350/53340, Loss: 0.2968\n",
      "Epoch 1, Batch 36400/53340, Loss: 0.7921\n",
      "Epoch 1, Batch 36450/53340, Loss: 0.6395\n",
      "Epoch 1, Batch 36500/53340, Loss: 0.2485\n",
      "Epoch 1, Batch 36550/53340, Loss: 0.0357\n",
      "Epoch 1, Batch 36600/53340, Loss: 0.1167\n",
      "Epoch 1, Batch 36650/53340, Loss: 0.5389\n",
      "Epoch 1, Batch 36700/53340, Loss: 0.2532\n",
      "Epoch 1, Batch 36750/53340, Loss: 0.7987\n",
      "Epoch 1, Batch 36800/53340, Loss: 0.8654\n",
      "Epoch 1, Batch 36850/53340, Loss: 0.1121\n",
      "Epoch 1, Batch 36900/53340, Loss: 0.6237\n",
      "Epoch 1, Batch 36950/53340, Loss: 0.2264\n",
      "Epoch 1, Batch 37000/53340, Loss: 0.2706\n",
      "Epoch 1, Batch 37050/53340, Loss: 0.0883\n",
      "Epoch 1, Batch 37100/53340, Loss: 0.1429\n",
      "Epoch 1, Batch 37150/53340, Loss: 0.0968\n",
      "Epoch 1, Batch 37200/53340, Loss: 1.0803\n",
      "Epoch 1, Batch 37250/53340, Loss: 0.0698\n",
      "Epoch 1, Batch 37300/53340, Loss: 0.2506\n",
      "Epoch 1, Batch 37350/53340, Loss: 0.1298\n",
      "Epoch 1, Batch 37400/53340, Loss: 0.4214\n",
      "Epoch 1, Batch 37450/53340, Loss: 0.1108\n",
      "Epoch 1, Batch 37500/53340, Loss: 0.2130\n",
      "Epoch 1, Batch 37550/53340, Loss: 0.0167\n",
      "Epoch 1, Batch 37600/53340, Loss: 0.9227\n",
      "Epoch 1, Batch 37650/53340, Loss: 0.0363\n",
      "Epoch 1, Batch 37700/53340, Loss: 1.7730\n",
      "Epoch 1, Batch 37750/53340, Loss: 0.4013\n",
      "Epoch 1, Batch 37800/53340, Loss: 0.6174\n",
      "Epoch 1, Batch 37850/53340, Loss: 0.0295\n",
      "Epoch 1, Batch 37900/53340, Loss: 0.0554\n",
      "Epoch 1, Batch 37950/53340, Loss: 0.1769\n",
      "Epoch 1, Batch 38000/53340, Loss: 0.1407\n",
      "Epoch 1, Batch 38050/53340, Loss: 1.7845\n",
      "Epoch 1, Batch 38100/53340, Loss: 0.2285\n",
      "Epoch 1, Batch 38150/53340, Loss: 0.2714\n",
      "Epoch 1, Batch 38200/53340, Loss: 0.1154\n",
      "Epoch 1, Batch 38250/53340, Loss: 0.0871\n",
      "Epoch 1, Batch 38300/53340, Loss: 0.2676\n",
      "Epoch 1, Batch 38350/53340, Loss: 0.0143\n",
      "Epoch 1, Batch 38400/53340, Loss: 0.1636\n",
      "Epoch 1, Batch 38450/53340, Loss: 0.0966\n",
      "Epoch 1, Batch 38500/53340, Loss: 0.8245\n",
      "Epoch 1, Batch 38550/53340, Loss: 0.5234\n",
      "Epoch 1, Batch 38600/53340, Loss: 0.1361\n",
      "Epoch 1, Batch 38650/53340, Loss: 1.0294\n",
      "Epoch 1, Batch 38700/53340, Loss: 0.0922\n",
      "Epoch 1, Batch 38750/53340, Loss: 0.5165\n",
      "Epoch 1, Batch 38800/53340, Loss: 0.5993\n",
      "Epoch 1, Batch 38850/53340, Loss: 0.0314\n",
      "Epoch 1, Batch 38900/53340, Loss: 0.1705\n",
      "Epoch 1, Batch 38950/53340, Loss: 0.0553\n",
      "Epoch 1, Batch 39000/53340, Loss: 0.2897\n",
      "Epoch 1, Batch 39050/53340, Loss: 0.0282\n",
      "Epoch 1, Batch 39100/53340, Loss: 0.0490\n",
      "Epoch 1, Batch 39150/53340, Loss: 0.3883\n",
      "Epoch 1, Batch 39200/53340, Loss: 0.1793\n",
      "Epoch 1, Batch 39250/53340, Loss: 0.0829\n",
      "Epoch 1, Batch 39300/53340, Loss: 0.0702\n",
      "Epoch 1, Batch 39350/53340, Loss: 0.6543\n",
      "Epoch 1, Batch 39400/53340, Loss: 0.6398\n",
      "Epoch 1, Batch 39450/53340, Loss: 0.2331\n",
      "Epoch 1, Batch 39500/53340, Loss: 0.3503\n",
      "Epoch 1, Batch 39550/53340, Loss: 0.1664\n",
      "Epoch 1, Batch 39600/53340, Loss: 0.1391\n",
      "Epoch 1, Batch 39650/53340, Loss: 0.0615\n",
      "Epoch 1, Batch 39700/53340, Loss: 0.0109\n",
      "Epoch 1, Batch 39750/53340, Loss: 0.9223\n",
      "Epoch 1, Batch 39800/53340, Loss: 0.1582\n",
      "Epoch 1, Batch 39850/53340, Loss: 1.4503\n",
      "Epoch 1, Batch 39900/53340, Loss: 0.7904\n",
      "Epoch 1, Batch 39950/53340, Loss: 0.5340\n",
      "Epoch 1, Batch 40000/53340, Loss: 0.2753\n",
      "Epoch 1, Batch 40050/53340, Loss: 0.1529\n",
      "Epoch 1, Batch 40100/53340, Loss: 0.2054\n",
      "Epoch 1, Batch 40150/53340, Loss: 0.0150\n",
      "Epoch 1, Batch 40200/53340, Loss: 0.0828\n",
      "Epoch 1, Batch 40250/53340, Loss: 0.0261\n",
      "Epoch 1, Batch 40300/53340, Loss: 0.3537\n",
      "Epoch 1, Batch 40350/53340, Loss: 0.3886\n",
      "Epoch 1, Batch 40400/53340, Loss: 0.4478\n",
      "Epoch 1, Batch 40450/53340, Loss: 0.0166\n",
      "Epoch 1, Batch 40500/53340, Loss: 0.0239\n",
      "Epoch 1, Batch 40550/53340, Loss: 1.9804\n",
      "Epoch 1, Batch 40600/53340, Loss: 0.1604\n",
      "Epoch 1, Batch 40650/53340, Loss: 0.0712\n",
      "Epoch 1, Batch 40700/53340, Loss: 0.3459\n",
      "Epoch 1, Batch 40750/53340, Loss: 0.7513\n",
      "Epoch 1, Batch 40800/53340, Loss: 0.2354\n",
      "Epoch 1, Batch 40850/53340, Loss: 0.3158\n",
      "Epoch 1, Batch 40900/53340, Loss: 0.0275\n",
      "Epoch 1, Batch 40950/53340, Loss: 0.1564\n",
      "Epoch 1, Batch 41000/53340, Loss: 0.0490\n",
      "Epoch 1, Batch 41050/53340, Loss: 0.1488\n",
      "Epoch 1, Batch 41100/53340, Loss: 0.1226\n",
      "Epoch 1, Batch 41150/53340, Loss: 0.1576\n",
      "Epoch 1, Batch 41200/53340, Loss: 0.0090\n",
      "Epoch 1, Batch 41250/53340, Loss: 0.2239\n",
      "Epoch 1, Batch 41300/53340, Loss: 0.2948\n",
      "Epoch 1, Batch 41350/53340, Loss: 0.4722\n",
      "Epoch 1, Batch 41400/53340, Loss: 0.0904\n",
      "Epoch 1, Batch 41450/53340, Loss: 0.0564\n",
      "Epoch 1, Batch 41500/53340, Loss: 0.0465\n",
      "Epoch 1, Batch 41550/53340, Loss: 0.1706\n",
      "Epoch 1, Batch 41600/53340, Loss: 0.1363\n",
      "Epoch 1, Batch 41650/53340, Loss: 1.6124\n",
      "Epoch 1, Batch 41700/53340, Loss: 0.0433\n",
      "Epoch 1, Batch 41750/53340, Loss: 1.1563\n",
      "Epoch 1, Batch 41800/53340, Loss: 0.0428\n",
      "Epoch 1, Batch 41850/53340, Loss: 0.3509\n",
      "Epoch 1, Batch 41900/53340, Loss: 0.0231\n",
      "Epoch 1, Batch 41950/53340, Loss: 0.0905\n",
      "Epoch 1, Batch 42000/53340, Loss: 0.3994\n",
      "Epoch 1, Batch 42050/53340, Loss: 0.0495\n",
      "Epoch 1, Batch 42100/53340, Loss: 0.0250\n",
      "Epoch 1, Batch 42150/53340, Loss: 0.1899\n",
      "Epoch 1, Batch 42200/53340, Loss: 0.0690\n",
      "Epoch 1, Batch 42250/53340, Loss: 1.0684\n",
      "Epoch 1, Batch 42300/53340, Loss: 0.5128\n",
      "Epoch 1, Batch 42350/53340, Loss: 0.1439\n",
      "Epoch 1, Batch 42400/53340, Loss: 0.3643\n",
      "Epoch 1, Batch 42450/53340, Loss: 0.0073\n",
      "Epoch 1, Batch 42500/53340, Loss: 0.3702\n",
      "Epoch 1, Batch 42550/53340, Loss: 0.0590\n",
      "Epoch 1, Batch 42600/53340, Loss: 0.0305\n",
      "Epoch 1, Batch 42650/53340, Loss: 0.1400\n",
      "Epoch 1, Batch 42700/53340, Loss: 0.0140\n",
      "Epoch 1, Batch 42750/53340, Loss: 0.1709\n",
      "Epoch 1, Batch 42800/53340, Loss: 0.1663\n",
      "Epoch 1, Batch 42850/53340, Loss: 0.2592\n",
      "Epoch 1, Batch 42900/53340, Loss: 0.1616\n",
      "Epoch 1, Batch 42950/53340, Loss: 0.0154\n",
      "Epoch 1, Batch 43000/53340, Loss: 0.1941\n",
      "Epoch 1, Batch 43050/53340, Loss: 0.1559\n",
      "Epoch 1, Batch 43100/53340, Loss: 0.4530\n",
      "Epoch 1, Batch 43150/53340, Loss: 0.0226\n",
      "Epoch 1, Batch 43200/53340, Loss: 0.3683\n",
      "Epoch 1, Batch 43250/53340, Loss: 1.0635\n",
      "Epoch 1, Batch 43300/53340, Loss: 0.0306\n",
      "Epoch 1, Batch 43350/53340, Loss: 0.7680\n",
      "Epoch 1, Batch 43400/53340, Loss: 0.0260\n",
      "Epoch 1, Batch 43450/53340, Loss: 0.0278\n",
      "Epoch 1, Batch 43500/53340, Loss: 0.7389\n",
      "Epoch 1, Batch 43550/53340, Loss: 0.0346\n",
      "Epoch 1, Batch 43600/53340, Loss: 0.5704\n",
      "Epoch 1, Batch 43650/53340, Loss: 0.1037\n",
      "Epoch 1, Batch 43700/53340, Loss: 0.0416\n",
      "Epoch 1, Batch 43750/53340, Loss: 0.3293\n",
      "Epoch 1, Batch 43800/53340, Loss: 0.0540\n",
      "Epoch 1, Batch 43850/53340, Loss: 0.0330\n",
      "Epoch 1, Batch 43900/53340, Loss: 0.1117\n",
      "Epoch 1, Batch 43950/53340, Loss: 0.9512\n",
      "Epoch 1, Batch 44000/53340, Loss: 0.1996\n",
      "Epoch 1, Batch 44050/53340, Loss: 0.6628\n",
      "Epoch 1, Batch 44100/53340, Loss: 0.1328\n",
      "Epoch 1, Batch 44150/53340, Loss: 0.0732\n",
      "Epoch 1, Batch 44200/53340, Loss: 0.0956\n",
      "Epoch 1, Batch 44250/53340, Loss: 0.2144\n",
      "Epoch 1, Batch 44300/53340, Loss: 0.0197\n",
      "Epoch 1, Batch 44350/53340, Loss: 0.4770\n",
      "Epoch 1, Batch 44400/53340, Loss: 0.6603\n",
      "Epoch 1, Batch 44450/53340, Loss: 0.1825\n",
      "Epoch 1, Batch 44500/53340, Loss: 0.0866\n",
      "Epoch 1, Batch 44550/53340, Loss: 0.0068\n",
      "Epoch 1, Batch 44600/53340, Loss: 0.4902\n",
      "Epoch 1, Batch 44650/53340, Loss: 1.2382\n",
      "Epoch 1, Batch 44700/53340, Loss: 0.0139\n",
      "Epoch 1, Batch 44750/53340, Loss: 0.6178\n",
      "Epoch 1, Batch 44800/53340, Loss: 0.0063\n",
      "Epoch 1, Batch 44850/53340, Loss: 0.9528\n",
      "Epoch 1, Batch 44900/53340, Loss: 0.2391\n",
      "Epoch 1, Batch 44950/53340, Loss: 0.2859\n",
      "Epoch 1, Batch 45000/53340, Loss: 1.3024\n",
      "Epoch 1, Batch 45050/53340, Loss: 0.5639\n",
      "Epoch 1, Batch 45100/53340, Loss: 0.1427\n",
      "Epoch 1, Batch 45150/53340, Loss: 0.7842\n",
      "Epoch 1, Batch 45200/53340, Loss: 0.2879\n",
      "Epoch 1, Batch 45250/53340, Loss: 0.1595\n",
      "Epoch 1, Batch 45300/53340, Loss: 0.4283\n",
      "Epoch 1, Batch 45350/53340, Loss: 0.6864\n",
      "Epoch 1, Batch 45400/53340, Loss: 0.0297\n",
      "Epoch 1, Batch 45450/53340, Loss: 0.1514\n",
      "Epoch 1, Batch 45500/53340, Loss: 0.0105\n",
      "Epoch 1, Batch 45550/53340, Loss: 0.7679\n",
      "Epoch 1, Batch 45600/53340, Loss: 0.0069\n",
      "Epoch 1, Batch 45650/53340, Loss: 0.4987\n",
      "Epoch 1, Batch 45700/53340, Loss: 0.0949\n",
      "Epoch 1, Batch 45750/53340, Loss: 0.0484\n",
      "Epoch 1, Batch 45800/53340, Loss: 0.3636\n",
      "Epoch 1, Batch 45850/53340, Loss: 0.0253\n",
      "Epoch 1, Batch 45900/53340, Loss: 0.1658\n",
      "Epoch 1, Batch 45950/53340, Loss: 0.0340\n",
      "Epoch 1, Batch 46000/53340, Loss: 0.1962\n",
      "Epoch 1, Batch 46050/53340, Loss: 0.0608\n",
      "Epoch 1, Batch 46100/53340, Loss: 1.0663\n",
      "Epoch 1, Batch 46150/53340, Loss: 1.0029\n",
      "Epoch 1, Batch 46200/53340, Loss: 0.0113\n",
      "Epoch 1, Batch 46250/53340, Loss: 0.0329\n",
      "Epoch 1, Batch 46300/53340, Loss: 0.2452\n",
      "Epoch 1, Batch 46350/53340, Loss: 0.1325\n",
      "Epoch 1, Batch 46400/53340, Loss: 0.0443\n",
      "Epoch 1, Batch 46450/53340, Loss: 0.8342\n",
      "Epoch 1, Batch 46500/53340, Loss: 0.0197\n",
      "Epoch 1, Batch 46550/53340, Loss: 0.3291\n",
      "Epoch 1, Batch 46600/53340, Loss: 0.0972\n",
      "Epoch 1, Batch 46650/53340, Loss: 0.4988\n",
      "Epoch 1, Batch 46700/53340, Loss: 0.0732\n",
      "Epoch 1, Batch 46750/53340, Loss: 0.0226\n",
      "Epoch 1, Batch 46800/53340, Loss: 0.0338\n",
      "Epoch 1, Batch 46850/53340, Loss: 0.0101\n",
      "Epoch 1, Batch 46900/53340, Loss: 0.2414\n",
      "Epoch 1, Batch 46950/53340, Loss: 0.1658\n",
      "Epoch 1, Batch 47000/53340, Loss: 0.2564\n",
      "Epoch 1, Batch 47050/53340, Loss: 1.0110\n",
      "Epoch 1, Batch 47100/53340, Loss: 0.0144\n",
      "Epoch 1, Batch 47150/53340, Loss: 0.1050\n",
      "Epoch 1, Batch 47200/53340, Loss: 0.3154\n",
      "Epoch 1, Batch 47250/53340, Loss: 0.3904\n",
      "Epoch 1, Batch 47300/53340, Loss: 0.0264\n",
      "Epoch 1, Batch 47350/53340, Loss: 2.1720\n",
      "Epoch 1, Batch 47400/53340, Loss: 0.6081\n",
      "Epoch 1, Batch 47450/53340, Loss: 0.7420\n",
      "Epoch 1, Batch 47500/53340, Loss: 0.4604\n",
      "Epoch 1, Batch 47550/53340, Loss: 0.5364\n",
      "Epoch 1, Batch 47600/53340, Loss: 0.6847\n",
      "Epoch 1, Batch 47650/53340, Loss: 0.0427\n",
      "Epoch 1, Batch 47700/53340, Loss: 0.0331\n",
      "Epoch 1, Batch 47750/53340, Loss: 0.0178\n",
      "Epoch 1, Batch 47800/53340, Loss: 0.0599\n",
      "Epoch 1, Batch 47850/53340, Loss: 0.2304\n",
      "Epoch 1, Batch 47900/53340, Loss: 0.0265\n",
      "Epoch 1, Batch 47950/53340, Loss: 0.0218\n",
      "Epoch 1, Batch 48000/53340, Loss: 0.0072\n",
      "Epoch 1, Batch 48050/53340, Loss: 0.1060\n",
      "Epoch 1, Batch 48100/53340, Loss: 0.5269\n",
      "Epoch 1, Batch 48150/53340, Loss: 0.1762\n",
      "Epoch 1, Batch 48200/53340, Loss: 0.0055\n",
      "Epoch 1, Batch 48250/53340, Loss: 1.2145\n",
      "Epoch 1, Batch 48300/53340, Loss: 0.0337\n",
      "Epoch 1, Batch 48350/53340, Loss: 0.2431\n",
      "Epoch 1, Batch 48400/53340, Loss: 0.4350\n",
      "Epoch 1, Batch 48450/53340, Loss: 1.2364\n",
      "Epoch 1, Batch 48500/53340, Loss: 0.0228\n",
      "Epoch 1, Batch 48550/53340, Loss: 0.2850\n",
      "Epoch 1, Batch 48600/53340, Loss: 0.3236\n",
      "Epoch 1, Batch 48650/53340, Loss: 0.0882\n",
      "Epoch 1, Batch 48700/53340, Loss: 0.0298\n",
      "Epoch 1, Batch 48750/53340, Loss: 0.1143\n",
      "Epoch 1, Batch 48800/53340, Loss: 0.6518\n",
      "Epoch 1, Batch 48850/53340, Loss: 0.9813\n",
      "Epoch 1, Batch 48900/53340, Loss: 0.0847\n",
      "Epoch 1, Batch 48950/53340, Loss: 0.1014\n",
      "Epoch 1, Batch 49000/53340, Loss: 0.4802\n",
      "Epoch 1, Batch 49050/53340, Loss: 0.0645\n",
      "Epoch 1, Batch 49100/53340, Loss: 0.1989\n",
      "Epoch 1, Batch 49150/53340, Loss: 0.0361\n",
      "Epoch 1, Batch 49200/53340, Loss: 0.0455\n",
      "Epoch 1, Batch 49250/53340, Loss: 0.0997\n",
      "Epoch 1, Batch 49300/53340, Loss: 0.1936\n",
      "Epoch 1, Batch 49350/53340, Loss: 0.0258\n",
      "Epoch 1, Batch 49400/53340, Loss: 0.1089\n",
      "Epoch 1, Batch 49450/53340, Loss: 0.0785\n",
      "Epoch 1, Batch 49500/53340, Loss: 0.5271\n",
      "Epoch 1, Batch 49550/53340, Loss: 0.3304\n",
      "Epoch 1, Batch 49600/53340, Loss: 0.3892\n",
      "Epoch 1, Batch 49650/53340, Loss: 0.1942\n",
      "Epoch 1, Batch 49700/53340, Loss: 0.0690\n",
      "Epoch 1, Batch 49750/53340, Loss: 0.0554\n",
      "Epoch 1, Batch 49800/53340, Loss: 0.1804\n",
      "Epoch 1, Batch 49850/53340, Loss: 0.0256\n",
      "Epoch 1, Batch 49900/53340, Loss: 0.0241\n",
      "Epoch 1, Batch 49950/53340, Loss: 0.1687\n",
      "Epoch 1, Batch 50000/53340, Loss: 0.7927\n",
      "Epoch 1, Batch 50050/53340, Loss: 0.5503\n",
      "Epoch 1, Batch 50100/53340, Loss: 0.0385\n",
      "Epoch 1, Batch 50150/53340, Loss: 0.0267\n",
      "Epoch 1, Batch 50200/53340, Loss: 0.1354\n",
      "Epoch 1, Batch 50250/53340, Loss: 0.0641\n",
      "Epoch 1, Batch 50300/53340, Loss: 0.2470\n",
      "Epoch 1, Batch 50350/53340, Loss: 0.0839\n",
      "Epoch 1, Batch 50400/53340, Loss: 1.6931\n",
      "Epoch 1, Batch 50450/53340, Loss: 0.0482\n",
      "Epoch 1, Batch 50500/53340, Loss: 0.0810\n",
      "Epoch 1, Batch 50550/53340, Loss: 1.5630\n",
      "Epoch 1, Batch 50600/53340, Loss: 0.0101\n",
      "Epoch 1, Batch 50650/53340, Loss: 0.3030\n",
      "Epoch 1, Batch 50700/53340, Loss: 0.1104\n",
      "Epoch 1, Batch 50750/53340, Loss: 0.1274\n",
      "Epoch 1, Batch 50800/53340, Loss: 0.0384\n",
      "Epoch 1, Batch 50850/53340, Loss: 0.0784\n",
      "Epoch 1, Batch 50900/53340, Loss: 0.0487\n",
      "Epoch 1, Batch 50950/53340, Loss: 0.7043\n",
      "Epoch 1, Batch 51000/53340, Loss: 0.0250\n",
      "Epoch 1, Batch 51050/53340, Loss: 0.3540\n",
      "Epoch 1, Batch 51100/53340, Loss: 0.2424\n",
      "Epoch 1, Batch 51150/53340, Loss: 0.0081\n",
      "Epoch 1, Batch 51200/53340, Loss: 0.2347\n",
      "Epoch 1, Batch 51250/53340, Loss: 0.0086\n",
      "Epoch 1, Batch 51300/53340, Loss: 0.5551\n",
      "Epoch 1, Batch 51350/53340, Loss: 0.0441\n",
      "Epoch 1, Batch 51400/53340, Loss: 0.1729\n",
      "Epoch 1, Batch 51450/53340, Loss: 0.2816\n",
      "Epoch 1, Batch 51500/53340, Loss: 0.8154\n",
      "Epoch 1, Batch 51550/53340, Loss: 0.0266\n",
      "Epoch 1, Batch 51600/53340, Loss: 0.0548\n",
      "Epoch 1, Batch 51650/53340, Loss: 0.0792\n",
      "Epoch 1, Batch 51700/53340, Loss: 6.2687\n",
      "Epoch 1, Batch 51750/53340, Loss: 0.4054\n",
      "Epoch 1, Batch 51800/53340, Loss: 0.2775\n",
      "Epoch 1, Batch 51850/53340, Loss: 0.0291\n",
      "Epoch 1, Batch 51900/53340, Loss: 0.3897\n",
      "Epoch 1, Batch 51950/53340, Loss: 0.0459\n",
      "Epoch 1, Batch 52000/53340, Loss: 0.2689\n",
      "Epoch 1, Batch 52050/53340, Loss: 0.1994\n",
      "Epoch 1, Batch 52100/53340, Loss: 0.2934\n",
      "Epoch 1, Batch 52150/53340, Loss: 0.2992\n",
      "Epoch 1, Batch 52200/53340, Loss: 0.0360\n",
      "Epoch 1, Batch 52250/53340, Loss: 0.1103\n",
      "Epoch 1, Batch 52300/53340, Loss: 0.1543\n",
      "Epoch 1, Batch 52350/53340, Loss: 0.0136\n",
      "Epoch 1, Batch 52400/53340, Loss: 0.3814\n",
      "Epoch 1, Batch 52450/53340, Loss: 0.0510\n",
      "Epoch 1, Batch 52500/53340, Loss: 0.2388\n",
      "Epoch 1, Batch 52550/53340, Loss: 0.0881\n",
      "Epoch 1, Batch 52600/53340, Loss: 0.3204\n",
      "Epoch 1, Batch 52650/53340, Loss: 0.0549\n",
      "Epoch 1, Batch 52700/53340, Loss: 0.7681\n",
      "Epoch 1, Batch 52750/53340, Loss: 0.5990\n",
      "Epoch 1, Batch 52800/53340, Loss: 0.0224\n",
      "Epoch 1, Batch 52850/53340, Loss: 0.2126\n",
      "Epoch 1, Batch 52900/53340, Loss: 0.0910\n",
      "Epoch 1, Batch 52950/53340, Loss: 0.3827\n",
      "Epoch 1, Batch 53000/53340, Loss: 0.3329\n",
      "Epoch 1, Batch 53050/53340, Loss: 0.1838\n",
      "Epoch 1, Batch 53100/53340, Loss: 0.0768\n",
      "Epoch 1, Batch 53150/53340, Loss: 0.0461\n",
      "Epoch 1, Batch 53200/53340, Loss: 0.0309\n",
      "Epoch 1, Batch 53250/53340, Loss: 0.0955\n",
      "Epoch 1, Batch 53300/53340, Loss: 0.1472\n",
      "\n",
      "Average training loss: 0.7439\n",
      "Learning rate: 0.000075\n",
      "\n",
      "Evaluating on val set...\n",
      "Evaluating on 5384 image pairs...\n",
      "Evaluated 100/5384 pairs...\n",
      "Evaluated 200/5384 pairs...\n",
      "Evaluated 300/5384 pairs...\n",
      "Evaluated 400/5384 pairs...\n",
      "Evaluated 500/5384 pairs...\n",
      "Evaluated 600/5384 pairs...\n",
      "Evaluated 700/5384 pairs...\n",
      "Evaluated 800/5384 pairs...\n",
      "Evaluated 900/5384 pairs...\n",
      "Evaluated 1000/5384 pairs...\n",
      "Evaluated 1100/5384 pairs...\n",
      "Evaluated 1200/5384 pairs...\n",
      "Evaluated 1300/5384 pairs...\n",
      "Evaluated 1400/5384 pairs...\n",
      "Evaluated 1500/5384 pairs...\n",
      "Evaluated 1600/5384 pairs...\n",
      "Evaluated 1700/5384 pairs...\n",
      "Evaluated 1800/5384 pairs...\n",
      "Evaluated 1900/5384 pairs...\n",
      "Evaluated 2000/5384 pairs...\n",
      "Evaluated 2100/5384 pairs...\n",
      "Evaluated 2200/5384 pairs...\n",
      "Evaluated 2300/5384 pairs...\n",
      "Evaluated 2400/5384 pairs...\n",
      "Evaluated 2500/5384 pairs...\n",
      "Evaluated 2600/5384 pairs...\n",
      "Evaluated 2700/5384 pairs...\n",
      "Evaluated 2800/5384 pairs...\n",
      "Evaluated 2900/5384 pairs...\n",
      "Evaluated 3000/5384 pairs...\n",
      "Evaluated 3100/5384 pairs...\n",
      "Evaluated 3200/5384 pairs...\n",
      "Evaluated 3300/5384 pairs...\n",
      "Evaluated 3400/5384 pairs...\n",
      "Evaluated 3500/5384 pairs...\n",
      "Evaluated 3600/5384 pairs...\n",
      "Evaluated 3700/5384 pairs...\n",
      "Evaluated 3800/5384 pairs...\n",
      "Evaluated 3900/5384 pairs...\n",
      "Evaluated 4000/5384 pairs...\n",
      "Evaluated 4100/5384 pairs...\n",
      "Evaluated 4200/5384 pairs...\n",
      "Evaluated 4300/5384 pairs...\n",
      "Evaluated 4400/5384 pairs...\n",
      "Evaluated 4500/5384 pairs...\n",
      "Evaluated 4600/5384 pairs...\n",
      "Evaluated 4700/5384 pairs...\n",
      "Evaluated 4800/5384 pairs...\n",
      "Evaluated 4900/5384 pairs...\n",
      "Evaluated 5000/5384 pairs...\n",
      "Evaluated 5100/5384 pairs...\n",
      "Evaluated 5200/5384 pairs...\n",
      "Evaluated 5300/5384 pairs...\n",
      "\n",
      "Test Results:\n",
      "  pck@0.05: 47.58% ± 28.91% (median: 50.00%)\n",
      "  pck@0.10: 60.01% ± 29.60% (median: 66.67%)\n",
      "  pck@0.20: 70.88% ± 28.27% (median: 77.78%)\n",
      "\n",
      "✓ New best model saved! PCK@0.1: 60.01%\n",
      "\n",
      "============================================================\n",
      "Epoch 2/3\n",
      "============================================================\n",
      "Epoch 2, Batch 50/53340, Loss: 0.0462\n",
      "Epoch 2, Batch 100/53340, Loss: 0.0160\n",
      "Epoch 2, Batch 150/53340, Loss: 0.2131\n",
      "Epoch 2, Batch 200/53340, Loss: 0.2317\n",
      "Epoch 2, Batch 250/53340, Loss: 0.1859\n",
      "Epoch 2, Batch 300/53340, Loss: 0.1477\n",
      "Epoch 2, Batch 350/53340, Loss: 0.0089\n",
      "Epoch 2, Batch 400/53340, Loss: 0.0849\n",
      "Epoch 2, Batch 450/53340, Loss: 0.0350\n",
      "Epoch 2, Batch 500/53340, Loss: 0.4081\n",
      "Epoch 2, Batch 550/53340, Loss: 0.0268\n",
      "Epoch 2, Batch 600/53340, Loss: 0.0522\n",
      "Epoch 2, Batch 650/53340, Loss: 0.0611\n",
      "Epoch 2, Batch 700/53340, Loss: 0.0456\n",
      "Epoch 2, Batch 750/53340, Loss: 0.0075\n",
      "Epoch 2, Batch 800/53340, Loss: 0.2901\n",
      "Epoch 2, Batch 850/53340, Loss: 0.0957\n",
      "Epoch 2, Batch 900/53340, Loss: 0.0785\n",
      "Epoch 2, Batch 950/53340, Loss: 0.0374\n",
      "Epoch 2, Batch 1000/53340, Loss: 0.0174\n",
      "Epoch 2, Batch 1050/53340, Loss: 0.0621\n",
      "Epoch 2, Batch 1100/53340, Loss: 0.3026\n",
      "Epoch 2, Batch 1150/53340, Loss: 0.4121\n",
      "Epoch 2, Batch 1200/53340, Loss: 0.4269\n",
      "Epoch 2, Batch 1250/53340, Loss: 0.0048\n",
      "Epoch 2, Batch 1300/53340, Loss: 0.0233\n",
      "Epoch 2, Batch 1350/53340, Loss: 0.2937\n",
      "Epoch 2, Batch 1400/53340, Loss: 0.2615\n",
      "Epoch 2, Batch 1450/53340, Loss: 0.1254\n",
      "Epoch 2, Batch 1500/53340, Loss: 0.0207\n",
      "Epoch 2, Batch 1550/53340, Loss: 0.0222\n",
      "Epoch 2, Batch 1600/53340, Loss: 1.1049\n",
      "Epoch 2, Batch 1650/53340, Loss: 0.1936\n",
      "Epoch 2, Batch 1700/53340, Loss: 0.0213\n",
      "Epoch 2, Batch 1750/53340, Loss: 0.0123\n",
      "Epoch 2, Batch 1800/53340, Loss: 0.0590\n",
      "Epoch 2, Batch 1850/53340, Loss: 0.1224\n",
      "Epoch 2, Batch 1900/53340, Loss: 0.0233\n",
      "Epoch 2, Batch 1950/53340, Loss: 0.0355\n",
      "Epoch 2, Batch 2000/53340, Loss: 0.0283\n",
      "Epoch 2, Batch 2050/53340, Loss: 0.0593\n",
      "Epoch 2, Batch 2100/53340, Loss: 0.0084\n",
      "Epoch 2, Batch 2150/53340, Loss: 0.0626\n",
      "Epoch 2, Batch 2200/53340, Loss: 0.1679\n",
      "Epoch 2, Batch 2250/53340, Loss: 0.1090\n",
      "Epoch 2, Batch 2300/53340, Loss: 0.0168\n",
      "Epoch 2, Batch 2350/53340, Loss: 0.0837\n",
      "Epoch 2, Batch 2400/53340, Loss: 0.1332\n",
      "Epoch 2, Batch 2450/53340, Loss: 0.2966\n",
      "Epoch 2, Batch 2500/53340, Loss: 1.5386\n",
      "Epoch 2, Batch 2550/53340, Loss: 0.0647\n",
      "Epoch 2, Batch 2600/53340, Loss: 0.0087\n",
      "Epoch 2, Batch 2650/53340, Loss: 0.1307\n",
      "Epoch 2, Batch 2700/53340, Loss: 0.1701\n",
      "Epoch 2, Batch 2750/53340, Loss: 0.0807\n",
      "Epoch 2, Batch 2800/53340, Loss: 0.0113\n",
      "Epoch 2, Batch 2850/53340, Loss: 0.0071\n",
      "Epoch 2, Batch 2900/53340, Loss: 0.0128\n",
      "Epoch 2, Batch 2950/53340, Loss: 0.9134\n",
      "Epoch 2, Batch 3000/53340, Loss: 0.0410\n",
      "Epoch 2, Batch 3050/53340, Loss: 0.6301\n",
      "Epoch 2, Batch 3100/53340, Loss: 0.0270\n",
      "Epoch 2, Batch 3150/53340, Loss: 2.7127\n",
      "Epoch 2, Batch 3200/53340, Loss: 0.6083\n",
      "Epoch 2, Batch 3250/53340, Loss: 0.0347\n",
      "Epoch 2, Batch 3300/53340, Loss: 0.0401\n",
      "Epoch 2, Batch 3350/53340, Loss: 0.1066\n",
      "Epoch 2, Batch 3400/53340, Loss: 0.1019\n",
      "Epoch 2, Batch 3450/53340, Loss: 0.2281\n",
      "Epoch 2, Batch 3500/53340, Loss: 0.3832\n",
      "Epoch 2, Batch 3550/53340, Loss: 0.0046\n",
      "Epoch 2, Batch 3600/53340, Loss: 0.0625\n",
      "Epoch 2, Batch 3650/53340, Loss: 0.1951\n",
      "Epoch 2, Batch 3700/53340, Loss: 0.2132\n",
      "Epoch 2, Batch 3750/53340, Loss: 0.0649\n",
      "Epoch 2, Batch 3800/53340, Loss: 0.1025\n",
      "Epoch 2, Batch 3850/53340, Loss: 0.4005\n",
      "Epoch 2, Batch 3900/53340, Loss: 0.2709\n",
      "Epoch 2, Batch 3950/53340, Loss: 0.0135\n",
      "Epoch 2, Batch 4000/53340, Loss: 0.0633\n",
      "Epoch 2, Batch 4050/53340, Loss: 0.0340\n",
      "Epoch 2, Batch 4100/53340, Loss: 0.0045\n",
      "Epoch 2, Batch 4150/53340, Loss: 0.0062\n",
      "Epoch 2, Batch 4200/53340, Loss: 0.0538\n",
      "Epoch 2, Batch 4250/53340, Loss: 0.0132\n",
      "Epoch 2, Batch 4300/53340, Loss: 0.1748\n",
      "Epoch 2, Batch 4350/53340, Loss: 0.0168\n",
      "Epoch 2, Batch 4400/53340, Loss: 0.0073\n",
      "Epoch 2, Batch 4450/53340, Loss: 0.0514\n",
      "Epoch 2, Batch 4500/53340, Loss: 0.0099\n",
      "Epoch 2, Batch 4550/53340, Loss: 0.0304\n",
      "Epoch 2, Batch 4600/53340, Loss: 0.0132\n",
      "Epoch 2, Batch 4650/53340, Loss: 0.0081\n",
      "Epoch 2, Batch 4700/53340, Loss: 0.0261\n",
      "Epoch 2, Batch 4750/53340, Loss: 0.0416\n",
      "Epoch 2, Batch 4800/53340, Loss: 0.1784\n",
      "Epoch 2, Batch 4850/53340, Loss: 0.0089\n",
      "Epoch 2, Batch 4900/53340, Loss: 0.0045\n",
      "Epoch 2, Batch 4950/53340, Loss: 0.0101\n",
      "Epoch 2, Batch 5000/53340, Loss: 0.0204\n",
      "Epoch 2, Batch 5050/53340, Loss: 0.0160\n",
      "Epoch 2, Batch 5100/53340, Loss: 0.0056\n",
      "Epoch 2, Batch 5150/53340, Loss: 0.3042\n",
      "Epoch 2, Batch 5200/53340, Loss: 0.0716\n",
      "Epoch 2, Batch 5250/53340, Loss: 0.0365\n",
      "Epoch 2, Batch 5300/53340, Loss: 0.0201\n",
      "Epoch 2, Batch 5350/53340, Loss: 0.3881\n",
      "Epoch 2, Batch 5400/53340, Loss: 0.1878\n",
      "Epoch 2, Batch 5450/53340, Loss: 0.0784\n",
      "Epoch 2, Batch 5500/53340, Loss: 0.1184\n",
      "Epoch 2, Batch 5550/53340, Loss: 0.0166\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "yKKlI7n1E0e1"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
