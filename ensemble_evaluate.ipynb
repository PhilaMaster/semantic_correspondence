{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RZX8dGTK_xIO",
        "outputId": "c59f1aab-e61b-4753-cc88-f8b2c92487ea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Monta Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "wdRMIwKP_4VQ",
        "outputId": "80961ecc-6191-4f55-d297-2dc38ea35aa7"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/semantic_correspondence.zip'"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Percorsi\n",
        "zip_su_drive = '/content/drive/MyDrive/semantic_correspondence.zip'\n",
        "zip_locale = '/content/semantic_correspondence.zip'\n",
        "cartella_destinazione = '/content/'\n",
        "\n",
        "# Copia lo zip in locale\n",
        "import shutil\n",
        "shutil.copy(zip_su_drive, zip_locale)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "4spGp1KM_75c"
      },
      "outputs": [],
      "source": [
        "# Estrai lo zip\n",
        "import zipfile, os\n",
        "os.makedirs(cartella_destinazione, exist_ok=True)\n",
        "with zipfile.ZipFile(zip_locale, 'r') as z:\n",
        "    z.extractall(cartella_destinazione)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "emjurSt8AE5P",
        "outputId": "3602f334-7c3e-4554-c077-1a6718366938"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "✓ GPU: Tesla T4\n"
          ]
        }
      ],
      "source": [
        "# 5. Verify GPU\n",
        "import torch\n",
        "print(f\"\\n✓ GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'No GPU'}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "hcZlHAWdVsBY"
      },
      "outputs": [],
      "source": [
        "\n",
        "base = '/content/semantic_correspondence/SPair71k'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 772
        },
        "id": "KBO4_UOXHnXk",
        "outputId": "17c88022-8c69-43dc-8334-a350a9665abe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "GRID SEARCH FOR WINDOWED SOFTARGMAX HYPERPARAMETERS\n",
            "================================================================================\n",
            "Temperature values: [0.05, 0.1, 0.2, 0.5, 1.0, 2.0]\n",
            "Total combinations: 30\n",
            "Validation set size: 5384\n",
            "================================================================================\n",
            "\n",
            "[1/30] Testing K=3, temperature=0.05\n",
            "  Processed 1/5384 images\n",
            "  Processed 101/5384 images\n",
            "  Processed 1001/5384 images\n",
            "  Processed 2001/5384 images\n",
            "  Processed 3001/5384 images\n",
            "  Processed 4001/5384 images\n",
            "  Processed 5001/5384 images\n",
            "  PCK@0.05: mean=56.75%, median=60.00%\n",
            "  PCK@0.10: mean=68.98%, median=75.00%\n",
            "  PCK@0.20: mean=78.86%, median=87.50%\n",
            "  Time: 927.29s\n",
            "\n",
            "[2/30] Testing K=3, temperature=0.1\n",
            "  Processed 1/5384 images\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-224652174.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    252\u001b[0m     \u001b[0mval_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSPairDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpair_ann_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayout_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpck_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatatype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'val'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 254\u001b[0;31m     \u001b[0mdf_results\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_grid_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'grid_search_results/dinov3/dinov3_finetuned'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-224652174.py\u001b[0m in \u001b[0;36mrun_grid_search\u001b[0;34m(model, val_dataset, device, results_dir)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m             \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m             per_image_metrics = evaluate_with_params(\n\u001b[0m\u001b[1;32m    135\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthresholds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m             )\n",
            "\u001b[0;32m/tmp/ipython-input-224652174.py\u001b[0m in \u001b[0;36mevaluate_with_params\u001b[0;34m(model, dataset, device, K, temperature, img_size, patch_size, thresholds)\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0;31m# extract dense features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0msrc_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_dense_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m             \u001b[0mtgt_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_dense_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0;31m# reshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/semantic_correspondence/helper_functions.py\u001b[0m in \u001b[0;36mextract_dense_features\u001b[0;34m(model, img_tensor, training)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;31m#get tokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mfeatures_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0mpatch_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeatures_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'x_norm_patchtokens'\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# [B, N_patches, D]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/semantic_correspondence/models/dinov3/dinov3/models/vision_transformer.py\u001b[0m in \u001b[0;36mforward_features\u001b[0;34m(self, x, masks)\u001b[0m\n\u001b[1;32m    263\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasks\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_features_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mmasks\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_features_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/semantic_correspondence/models/dinov3/dinov3/models/vision_transformer.py\u001b[0m in \u001b[0;36mforward_features_list\u001b[0;34m(self, x_list, masks_list)\u001b[0m\n\u001b[1;32m    232\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m                 \u001b[0mrope_sincos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrope\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mblk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrope_sincos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m         \u001b[0mall_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/semantic_correspondence/models/dinov3/dinov3/layers/block.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x_or_x_list, rope_or_rope_list)\u001b[0m\n\u001b[1;32m    208\u001b[0m                 \u001b[0mrope_or_rope_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx_or_x_list\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m             \u001b[0;31m# return [self._forward(x, rope=rope) for x, rope in zip(x_or_x_list, rope_or_rope_list)]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_or_x_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrope_list\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrope_or_rope_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mAssertionError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/semantic_correspondence/models/dinov3/dinov3/layers/block.py\u001b[0m in \u001b[0;36m_forward_list\u001b[0;34m(self, x_list, rope_list)\u001b[0m\n\u001b[1;32m    191\u001b[0m             \u001b[0mx_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrope\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrope_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m                 \u001b[0mx_attn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mls1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrope\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m                 \u001b[0mx_ffn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_attn\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mls2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_attn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m                 \u001b[0mx_out\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_ffn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/semantic_correspondence/models/dinov3/dinov3/layers/attention.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, attn_bias, rope)\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_bias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrope\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0mqkv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqkv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         \u001b[0mattn_v\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_attention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqkv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mqkv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_bias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattn_bias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrope\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattn_v\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproj_drop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/semantic_correspondence/models/dinov3/dinov3/layers/attention.py\u001b[0m in \u001b[0;36mcompute_attention\u001b[0;34m(self, qkv, attn_bias, rope)\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrope\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             \u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_rope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscaled_dot_product_attention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/semantic_correspondence/models/dinov3/dinov3/layers/attention.py\u001b[0m in \u001b[0;36mapply_rope\u001b[0;34m(self, q, k, rope)\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0mq_prefix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mprefix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0mq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrope_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprefix\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcos\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# [B, head, hw, D//head]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m         \u001b[0mq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_prefix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# [B, head, N, D//head]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m         \u001b[0mk_prefix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mprefix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrope_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprefix\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcos\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# [B, head, hw, D//head]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import json\n",
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import time\n",
        "import os\n",
        "from datetime import datetime\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "import shutil\n",
        "import sys\n",
        "# Add the extracted directory to the Python path\n",
        "sys.path.insert(0, '/content/semantic_correspondence')\n",
        "research_path = \"/content/semantic_correspondence/models/segment_anything\"\n",
        "if research_path not in sys.path:\n",
        "    sys.path.insert(0, research_path)\n",
        "\n",
        "from SPair71k.devkit.SPairDataset import SPairDataset\n",
        "from helper_functions import extract_dense_features, extract_dense_features_SAM, pixel_to_patch_coord, patch_to_pixel_coord\n",
        "from matching_strategies import find_best_match_argmax, find_best_match_window_softargmax\n",
        "from pck import compute_pck_spair71k\n",
        "from models.dinov3.dinov3.models.vision_transformer import vit_base as dinov3_vit_base\n",
        "from models.dinov2.dinov2.models.vision_transformer import vit_base as dinov2_vit_base\n",
        "from models.segment_anything.segment_anything import sam_model_registry\n",
        "\n",
        "# ==================== CONFIG ====================\n",
        "IMG_SIZE_DINOV2 = 518\n",
        "PATCH_SIZE_DINOV2 = 14\n",
        "IMG_SIZE_DINOV3 = 512\n",
        "PATCH_SIZE_DINOV3 = 16\n",
        "IMG_SIZE_SAM = 512\n",
        "PATCH_SIZE_SAM = 16\n",
        "\n",
        "BASE = 'semantic_correspondence/SPair71k'\n",
        "PAIR_ANN_PATH = f'{BASE}/PairAnnotation'\n",
        "LAYOUT_PATH = f'{BASE}/Layout'\n",
        "IMAGE_PATH = f'{BASE}/JPEGImages'\n",
        "DATASET_SIZE = 'large'\n",
        "PCK_ALPHA = 0.1\n",
        "THRESHOLDS = [0.05, 0.1, 0.2]\n",
        "\n",
        "print(os.getcwd())\n",
        "\n",
        "b = \"semantic_correspondence/models\"\n",
        "\n",
        "CHECKPOINT_PATHS = {\n",
        "    \"DINOv2\": f\"{b}/dinov2/weights/dinov2_vitb14_finetuned_only_model_10temp.pth\",\n",
        "    \"DINOv3\": f\"{b}/dinov3/weights/finetuned/dinov3_vitb16_finetuned_3bl_0.0001lr_15t.pth\",\n",
        "    \"SAM\": f\"{b}/segment_anything/weights/finetuned/SAM_finetuned_4bl_15t_0.0001lr.pth\"\n",
        "}\n",
        "\n",
        "\n",
        "# ==================== HELPER FUNCTIONS ====================\n",
        "\n",
        "def load_models(device):\n",
        "    \"\"\"Load all three finetuned models.\"\"\"\n",
        "    print(\"Loading models...\")\n",
        "\n",
        "    # DINOv2\n",
        "    dinov2 = dinov2_vit_base(\n",
        "        img_size=(IMG_SIZE_DINOV2, IMG_SIZE_DINOV2),\n",
        "        patch_size=PATCH_SIZE_DINOV2,\n",
        "        num_register_tokens=0,\n",
        "        block_chunks=0,\n",
        "        init_values=1.0,\n",
        "    )\n",
        "    ckpt_dinov2 = torch.load(CHECKPOINT_PATHS[\"DINOv2\"], map_location=device)\n",
        "    dinov2.load_state_dict(ckpt_dinov2, strict=True)\n",
        "    dinov2.to(device)\n",
        "    dinov2.eval()\n",
        "\n",
        "    # DINOv3\n",
        "    dinov3 = dinov3_vit_base(\n",
        "        img_size=(IMG_SIZE_DINOV3, IMG_SIZE_DINOV3),\n",
        "        patch_size=PATCH_SIZE_DINOV3,\n",
        "        n_storage_tokens=4,\n",
        "        layerscale_init=1.0,\n",
        "        mask_k_bias=True,\n",
        "    )\n",
        "    ckpt_dinov3 = torch.load(CHECKPOINT_PATHS[\"DINOv3\"], map_location=device)\n",
        "    dinov3.load_state_dict(ckpt_dinov3[\"model_state_dict\"], strict=True)\n",
        "    dinov3.to(device)\n",
        "    dinov3.eval()\n",
        "\n",
        "    # SAM\n",
        "    # Initialize the SAM model without loading checkpoint yet\n",
        "    sam = sam_model_registry[\"vit_b\"](checkpoint=None) # Pass None to initialize without loading\n",
        "    sam.to(device)\n",
        "\n",
        "    # Load the custom finetuned checkpoint\n",
        "    print(f\"Loading finetuned SAM checkpoint from {CHECKPOINT_PATHS['SAM']}\")\n",
        "    checkpoint = torch.load(CHECKPOINT_PATHS[\"SAM\"], map_location=device)\n",
        "\n",
        "    # The finetuned checkpoint likely contains more than just the model state_dict.\n",
        "    # Extract the actual model_state_dict and load it\n",
        "    if 'model_state_dict' in checkpoint:\n",
        "        sam.load_state_dict(checkpoint['model_state_dict'])\n",
        "        print(\"Successfully loaded 'model_state_dict' from checkpoint.\")\n",
        "    else:\n",
        "        # If the checkpoint itself is just the state_dict, try loading it directly\n",
        "        sam.load_state_dict(checkpoint)\n",
        "        print(\"Successfully loaded checkpoint directly as state_dict.\")\n",
        "    sam.eval()\n",
        "\n",
        "    print(\"✓ All models loaded successfully\")\n",
        "    return dinov2, dinov3, sam\n",
        "\n",
        "\n",
        "def normalize_features(features):\n",
        "    \"\"\"L2 normalize features along the feature dimension.\"\"\"\n",
        "    # features: [B, H, W, D] or [H*W, D]\n",
        "    if len(features.shape) == 4:\n",
        "        # [B, H, W, D] -> normalize over D\n",
        "        return F.normalize(features, p=2, dim=-1)\n",
        "    else:\n",
        "        # [H*W, D] -> normalize over D\n",
        "        return F.normalize(features, p=2, dim=1)\n",
        "\n",
        "\n",
        "def evaluate_ensemble_with_params(\n",
        "    models_dict,\n",
        "    dataset,\n",
        "    device,\n",
        "    K,\n",
        "    temperature,\n",
        "    weights,\n",
        "    thresholds=None\n",
        "):\n",
        "    \"\"\"\n",
        "    Evaluate ensemble with weighted_avg fusion.\n",
        "\n",
        "    Args:\n",
        "        models_dict: dict with 'dinov2', 'dinov3', 'sam' models\n",
        "        dataset: evaluation dataset\n",
        "        device: torch device\n",
        "        K: window size for softargmax\n",
        "        temperature: softmax temperature\n",
        "        weights: [w_dinov2, w_dinov3, w_sam] for weighted fusion (must sum to 1)\n",
        "        thresholds: PCK thresholds\n",
        "\n",
        "    Returns:\n",
        "        per_image_metrics: list of dicts with PCK scores\n",
        "    \"\"\"\n",
        "    if thresholds is None:\n",
        "        thresholds = THRESHOLDS\n",
        "\n",
        "    per_image_metrics = []\n",
        "    dinov2, dinov3, sam = models_dict['dinov2'], models_dict['dinov3'], models_dict['sam']\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for idx, sample in enumerate(dataset):\n",
        "            # Load and resize images\n",
        "            src_tensor = sample['src_img'].unsqueeze(0).to(device)\n",
        "            tgt_tensor = sample['trg_img'].unsqueeze(0).to(device)\n",
        "\n",
        "            # Resize for DINOv2\n",
        "            src_dinov2 = F.interpolate(src_tensor, size=(IMG_SIZE_DINOV2, IMG_SIZE_DINOV2),\n",
        "                                       mode='bilinear', align_corners=False)\n",
        "            tgt_dinov2 = F.interpolate(tgt_tensor, size=(IMG_SIZE_DINOV2, IMG_SIZE_DINOV2),\n",
        "                                       mode='bilinear', align_corners=False)\n",
        "\n",
        "            # Resize for DINOv3\n",
        "            src_dinov3 = F.interpolate(src_tensor, size=(IMG_SIZE_DINOV3, IMG_SIZE_DINOV3),\n",
        "                                       mode='bilinear', align_corners=False)\n",
        "            tgt_dinov3 = F.interpolate(tgt_tensor, size=(IMG_SIZE_DINOV3, IMG_SIZE_DINOV3),\n",
        "                                       mode='bilinear', align_corners=False)\n",
        "\n",
        "            # Resize for SAM\n",
        "            src_sam = F.interpolate(src_tensor, size=(IMG_SIZE_SAM, IMG_SIZE_SAM),\n",
        "                                    mode='bilinear', align_corners=False)\n",
        "            tgt_sam = F.interpolate(tgt_tensor, size=(IMG_SIZE_SAM, IMG_SIZE_SAM),\n",
        "                                    mode='bilinear', align_corners=False)\n",
        "\n",
        "            # Extract features from all models\n",
        "            src_feat_dinov2 = extract_dense_features(dinov2, src_dinov2)\n",
        "            tgt_feat_dinov2 = extract_dense_features(dinov2, tgt_dinov2)\n",
        "\n",
        "            src_feat_dinov3 = extract_dense_features(dinov3, src_dinov3)\n",
        "            tgt_feat_dinov3 = extract_dense_features(dinov3, tgt_dinov3)\n",
        "\n",
        "            src_feat_sam = extract_dense_features_SAM(sam, src_sam, image_size=IMG_SIZE_SAM)\n",
        "            tgt_feat_sam = extract_dense_features_SAM(sam, tgt_sam, image_size=IMG_SIZE_SAM)\n",
        "\n",
        "            # Get original sizes\n",
        "            src_original_size = (sample['src_imsize'][2], sample['src_imsize'][1])\n",
        "            tgt_original_size = (sample['trg_imsize'][2], sample['trg_imsize'][1])\n",
        "\n",
        "            # Get keypoints\n",
        "            src_kps = sample['src_kps'].numpy()\n",
        "            trg_kps = sample['trg_kps'].numpy()\n",
        "            kps_ids = sample['kps_ids']\n",
        "            trg_bbox = sample['trg_bbox']\n",
        "            category = sample['category']\n",
        "\n",
        "            # Prepare target features for score-level fusion\n",
        "            tgt_feat_dinov2_squeezed = tgt_feat_dinov2.squeeze(0)  # [H2, W2, D2]\n",
        "            tgt_feat_dinov3_squeezed = tgt_feat_dinov3.squeeze(0)  # [H3, W3, D3]\n",
        "            tgt_feat_sam_squeezed    = tgt_feat_sam.squeeze(0)     # [Hs, Ws, Ds]\n",
        "\n",
        "            # Use SAM grid as the reference grid\n",
        "            ref_shape = tgt_feat_sam_squeezed.shape  # (Hs, Ws, Ds)\n",
        "            H_ref, W_ref = ref_shape[0], ref_shape[1]\n",
        "\n",
        "            # Precompute normalized target flats for score-level fusion\n",
        "            H2, W2, D2 = tgt_feat_dinov2_squeezed.shape\n",
        "            H3, W3, D3 = tgt_feat_dinov3_squeezed.shape\n",
        "            Hs, Ws, Ds = tgt_feat_sam_squeezed.shape\n",
        "\n",
        "            tgt_v2_flat = F.normalize(tgt_feat_dinov2_squeezed.reshape(H2 * W2, D2), dim=1)\n",
        "            tgt_v3_flat = F.normalize(tgt_feat_dinov3_squeezed.reshape(H3 * W3, D3), dim=1)\n",
        "            tgt_s_flat  = F.normalize(tgt_feat_sam_squeezed.reshape(Hs * Ws, Ds),    dim=1)\n",
        "\n",
        "            pred_matches = []\n",
        "\n",
        "            # Process each keypoint\n",
        "            for i in range(src_kps.shape[0]):\n",
        "                src_x, src_y = src_kps[i]\n",
        "\n",
        "                # Source features per model\n",
        "                px2, py2 = pixel_to_patch_coord(src_x, src_y, src_original_size,\n",
        "                                                patch_size=PATCH_SIZE_DINOV2, resized_size=IMG_SIZE_DINOV2)\n",
        "                src_v2 = F.normalize(src_feat_dinov2[0, py2, px2, :], dim=0)\n",
        "\n",
        "                px3, py3 = pixel_to_patch_coord(src_x, src_y, src_original_size,\n",
        "                                                patch_size=PATCH_SIZE_DINOV3, resized_size=IMG_SIZE_DINOV3)\n",
        "                src_v3 = F.normalize(src_feat_dinov3[0, py3, px3, :], dim=0)\n",
        "\n",
        "                pxs, pys = pixel_to_patch_coord(src_x, src_y, src_original_size,\n",
        "                                                patch_size=PATCH_SIZE_SAM, resized_size=IMG_SIZE_SAM)\n",
        "                src_vs = F.normalize(src_feat_sam[0, pys, pxs, :], dim=0)\n",
        "\n",
        "                # Score-level fusion: build per-model sim maps, upsample to ref grid, then weight-sum\n",
        "                sim2 = F.cosine_similarity(src_v2.unsqueeze(0), tgt_v2_flat, dim=1).view(H2, W2)\n",
        "                sim3 = F.cosine_similarity(src_v3.unsqueeze(0), tgt_v3_flat, dim=1).view(H3, W3)\n",
        "                sims = F.cosine_similarity(src_vs.unsqueeze(0),  tgt_s_flat,  dim=1).view(Hs, Ws)\n",
        "\n",
        "                def resize_map(m, H_t, W_t):\n",
        "                    if (H_t, W_t) == (H_ref, W_ref):\n",
        "                        return m\n",
        "                    return F.interpolate(m.unsqueeze(0).unsqueeze(0), size=(H_ref, W_ref),\n",
        "                                         mode='bilinear', align_corners=False).squeeze(0).squeeze(0)\n",
        "\n",
        "                sim2_r = resize_map(sim2, H2, W2)\n",
        "                sim3_r = resize_map(sim3, H3, W3)\n",
        "                sims_r = resize_map(sims, Hs, Ws)\n",
        "\n",
        "                similarities = (weights[0] * sim2_r + weights[1] * sim3_r + weights[2] * sims_r).reshape(-1)\n",
        "\n",
        "                # Find best match on ensemble similarity map\n",
        "                match_patch_x, match_patch_y = find_best_match_window_softargmax(\n",
        "                    similarities, W_ref, H_ref, K=K, temperature=temperature\n",
        "                )\n",
        "\n",
        "                # Convert to original image coords (ref grid = SAM)\n",
        "                match_x, match_y = patch_to_pixel_coord(\n",
        "                    match_patch_x, match_patch_y, tgt_original_size,\n",
        "                    patch_size=PATCH_SIZE_SAM, resized_size=IMG_SIZE_SAM\n",
        "                )\n",
        "                pred_matches.append([match_x, match_y])\n",
        "\n",
        "            # Compute PCK\n",
        "            image_pcks = {}\n",
        "            for threshold in thresholds:\n",
        "                pck, _, _ = compute_pck_spair71k(\n",
        "                    pred_matches,\n",
        "                    trg_kps.tolist(),\n",
        "                    trg_bbox,\n",
        "                    threshold\n",
        "                )\n",
        "                image_pcks[threshold] = pck\n",
        "\n",
        "            per_image_metrics.append({\n",
        "                'category': category,\n",
        "                'pck_scores': image_pcks,\n",
        "            })\n",
        "\n",
        "            if (idx + 1) % 100 == 0:\n",
        "                print(f\"  Processed {idx + 1}/{len(dataset)} images\")\n",
        "\n",
        "    return per_image_metrics\n",
        "\n",
        "\n",
        "# ==================== MAIN ====================\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # Load models\n",
        "    dinov2, dinov3, sam = load_models(device)\n",
        "    models_dict = {'dinov2': dinov2, 'dinov3': dinov3, 'sam': sam}\n",
        "\n",
        "    # Fixed window soft-argmax params\n",
        "    K = 5\n",
        "    temperature = 0.2\n",
        "\n",
        "    # Weighted-average fusion weights: [DINOv2, DINOv3, SAM]\n",
        "    weights = [0.25, 0.65, 0.10]\n",
        "\n",
        "    # Load test dataset\n",
        "    print(\"\\nLoading test dataset...\")\n",
        "    val_dataset = SPairDataset(PAIR_ANN_PATH, LAYOUT_PATH, IMAGE_PATH, DATASET_SIZE,\n",
        "                                PCK_ALPHA, datatype='val')\n",
        "    print(f\"✓ Test set loaded: {len(val_dataset)} pairs\")\n",
        "\n",
        "    # Results dir\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    wtag = f\"{weights[0]:.2f}-{weights[1]:.2f}-{weights[2]:.2f}\"\n",
        "    results_dir = f'results_SPair71K/ensemble/weighted_avg/K{K}_T{temperature}_w{wtag}_{timestamp}'\n",
        "    os.makedirs(results_dir, exist_ok=True)\n",
        "    print(f\"Results will be saved to: {results_dir}\")\n",
        "\n",
        "    # Evaluate with weighted_avg fusion\n",
        "    start = time.time()\n",
        "    per_image_metrics = evaluate_ensemble_with_params(\n",
        "        models_dict=models_dict,\n",
        "        dataset=val_dataset,\n",
        "        device=device,\n",
        "        K=K,\n",
        "        temperature=temperature,\n",
        "        weights=weights,\n",
        "        thresholds=THRESHOLDS\n",
        "    )\n",
        "    elapsed = time.time() - start\n",
        "    print(f\"Total inference time: {elapsed:.2f} seconds\")\n",
        "\n",
        "    # Aggregate overall stats\n",
        "    overall_stats = {\"inference_time_sec\": elapsed}\n",
        "    for threshold in THRESHOLDS:\n",
        "        all_pcks = np.array([img['pck_scores'][threshold] for img in per_image_metrics])\n",
        "        overall_stats[f\"pck@{threshold:.2f}\"] = {\n",
        "            \"mean\": float(np.mean(all_pcks)),\n",
        "            \"std\": float(np.std(all_pcks)),\n",
        "            \"median\": float(np.median(all_pcks)),\n",
        "            \"p25\": float(np.percentile(all_pcks, 25)),\n",
        "            \"p75\": float(np.percentile(all_pcks, 75)),\n",
        "        }\n",
        "        print(f\"PCK@{threshold:.2f}: mean={overall_stats[f'pck@{threshold:.2f}']['mean']:.2f}% \"\n",
        "              f\"std={overall_stats[f'pck@{threshold:.2f}']['std']:.2f}% \"\n",
        "              f\"median={overall_stats[f'pck@{threshold:.2f}']['median']:.2f}% \"\n",
        "              f\"p25={overall_stats[f'pck@{threshold:.2f}']['p25']:.2f}% \"\n",
        "              f\"p75={overall_stats[f'pck@{threshold:.2f}']['p75']:.2f}%\")\n",
        "\n",
        "    # Save outputs\n",
        "    with open(f'{results_dir}/overall_stats.json', 'w') as f:\n",
        "        json.dump(overall_stats, f, indent=2)\n",
        "    df_all = pd.DataFrame([\n",
        "        {\"category\": m[\"category\"], **{f\"pck@{t:.2f}\": m[\"pck_scores\"][t] for t in THRESHOLDS}}\n",
        "        for m in per_image_metrics\n",
        "    ])\n",
        "    df_all.to_csv(f'{results_dir}/per_image_metrics.csv', index=False)\n",
        "    print(f\"Saved overall_stats.json and per_image_metrics.csv to {results_dir}\")\n",
        "\n",
        "    drive_results_base_path = '/content/drive/MyDrive/Colab_ensmble_validation_results/'\n",
        "    drive_destination_path = os.path.join(drive_results_base_path, os.path.basename(results_dir))\n",
        "\n",
        "    try:\n",
        "        if not os.path.exists(drive_results_base_path):\n",
        "            os.makedirs(drive_results_base_path, exist_ok=True)\n",
        "        shutil.copytree(results_dir, drive_destination_path)\n",
        "        print(f\"\\n✓ Successfully copied results to Google Drive: {drive_destination_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"\\n✗ Error copying results to Google Drive: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Smonta il Drive\n",
        "drive.flush_and_unmount()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
